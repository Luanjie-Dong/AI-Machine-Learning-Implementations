{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a904cf",
   "metadata": {},
   "source": [
    "## Implement Seq2Seq Model from scratch\n",
    "- maps an input to an output sequence of different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f47308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\codes\\AI-ML-Implementations\\ml-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b33bb2",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b529dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\codes\\AI-ML-Implementations\\ml-env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--FBK-MT--MCIF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 100%|██████████| 362/362 [00:00<00:00, 39574.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"FBK-MT/MCIF\", \"long_fixedprompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9476d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'prompt_en', 'prompt_de', 'prompt_it', 'prompt_zh', 'metadata', 'audio', 'video', 'text'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71b5a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'prompt_en': 'Answer the following question concisely given the English content: What are the main data sources for language models?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind die wichtigsten Datenquellen für Sprachmodelle?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le principali fonti di dati per i modelli linguistici?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 语言模型的主要数据来源是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '1', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '2', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/crgYiwKDfX.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/crgYiwKDfX.mp4', 'text': 'Hello everyone. I am Tu Yi from Ant Group, and here I\\'m going to be presenting our team\\'s paper on document understanding. The co-authors of this paper are all algorithm engineers from Ant Group, and this article is derived from our working practice. In this paper, we focus on the Visually-rich Document Understanding problem. It aims to understand various types of documents, like forms, receipts, and posters. In recent years, pre-training techniques have been introduced into this area and self-supervised pre-training multi-modal models have demonstrated great successes in various VrDU tasks. However, existing document pre-training models suffer from reading order issues. Following the idea of BERT, these methods usually adopt ascending numbers (like 0, 1, 2...) to represent the global reading order of tokens in the document, which are encoded as global 1D positions. We can see the illustration of global 1D position in Figure 1. We propose a novel pre-trained model, LayoutMask, to address these issues. LayoutMask only uses text and layout information as model input, and aims to enhance the text layout interactions and layout representations learned during pre-training. It differs from previous studies in three aspects: choice of 1D position, masking strategy, and pre-training objectives. Instead of global 1D position, LayerMask proposes to use the in-segment token orders as 1D position, which is referred to as “local 1D position”. As local 1D position does not provide cross-segment orders, LayoutMask is supposed to infer global reading order by jointly using 1D position, 2D position, and semantic information, thus bringing in-depth text-layout interactions. To further promote such interactions, we equip the commonly used pre-training objective, Masked Language Modeling, with two novel masking strategies: Whole Word Masking and Layout-Aware Masking. The Masked Language Modeling task is the most essential and commonly used pre-training task in multi-modal pre-training. Following the Whole Word Masking strategy, we set masks at word-level instead of token-level, which is much more challenging. When using Whole Word Masking, the semantic relations between masked and unmasked tokens of the same words are eliminated, so the model has to find more context to predict masked words, which can promote text-layout interactions. In Layout-Aware Masking strategy, the first and last words of each segment has a higher probability to be masked, so the model has to pay more attention to finding their contexts in the preceding or succeeding segment, thus promoting learning cross-segment orders. We also designed a new pre-training objective, Masked Position Modeling. It has a symmetric pre-training objective: recovering randomly masked 2D positions during pre-training. The MPM task is very similar to the cloze test, where a group of randomly selected words is supposed to be refilled at the right positions in the original documents. The model has to find the context for each word based on semantic relations and infer with 2D position clues from a spatial perspective. The joint learning process with both semantic and spatial inference can promote text-layout interactions and help the model learn better layout representations. In our experiments, we compare the performance of LayoutMask using different layout information. For 1D position, Local-1D outperforms Global-1D on both FUNSD and SROIE, and falls a little behind on CORD. The performance gap between local and global mainly comes from the entity \"Total\", while other entities have similar F1 scores. We illustrate two examples of SROIE datasets and their entities annotations are in the figure. The right image, which contains entity “Total”, has both vertical layout and horizontal layout, and has multiple misleading numbers with the same content as the ground truth. So it is hard to recognize entity \"Total\" by using the ordinary reading order implied by Global-1D. Based on this result, we can conclude that using Local-1D can perform better since it is more adaptive to such cases. For more details of the paper, please refer to our paper and posters. Thanks for watching. If you have any question, please send me an email.'}\n",
      "{'id': '4', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '5', 'prompt_en': 'Answer the following question concisely given the English content: Which model did they use to obtain the 82%-87% accuracy?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale modello hanno utilizzato per ottenere l'accuratezza dell'82%-87%?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 他们使用哪种模型获得 82%-87% 的准确率？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '6', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MmiKtcykVs.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MmiKtcykVs.mp4', 'text': 'Hello, everyone, I\\'m Jiaan. I\\'m so excited to present our work, \"Towards Unifying Multi-Lingual and Cross-Lingual Summarization\". This is a joint work with Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. First, let me summarize our contributions in this work. We unify previous multilingual summarization and cross-lingual summarization into a more general setting named many-to-many summarization. Many-to-many summarization aims to build one single summarization model to process a document in any source language and generate its summary in any target language. We also conduct preliminary studies to provide deeper analyses among multilingual summarization, cross-lingual summarization, and our many-to-many summarization. And we find that many-to-many summarization could help the summarization model better transfer task knowledge across different languages than previous multilingual summarization and cross-lingual summarization. Moreover, we proposed PISCES, a pre-trained many-to-many summarization model that learns language modeling, cross-lingual ability, and summarization ability through a carefully designed three-stage pre-training. Next, I would like to show you the difference between previous multilingual summarization, cross-lingual summarization, and our many-to-many summarization. Look at this figure. Given a document in your source language, the multilingual summarization model aims to generate its summary also in the source language, and the cross-lingual summarization model aims to generate its summary in a different target language. In other words, the input language and output language are the same on the settings of multi-Lingual summarization, while they are different on the settings of cross-lingual summarization. And our many-to-many summarization combines two previous tasks into a more general setting that the model can summarize the document in any language into the corresponding summary, also in any language. To compare cross-lingual summarization, multilingual summarization, and our many-to-many summarization, we conduct a preliminary experiment on the widely-used WikiLingua dataset. The samples used in the experiments includes English, French, Hindi, Chinese, Thai, and Turkish. We trained the following four models with the same backbone, that is mBART-50. First, mBART ONE: we separately train several mBART-50 models, each of which is built and evaluated in one single direction. Second, mBART U-CLS: We train a unified model with all cross-lingual samples, and test the model in all directions. Third, mBART MLS: We train one unified model with monolingual samples in all directions, and test the model also in all directions. Lastly, we train mBART Many-to-Many Summarization with the new settings introduced by our work, where the model is both trained and evaluated in all directions. This table shows the result of our preliminary experiments. We can see that the multilingual model trained in the many-to-many summarization setting can better transfer task knowledge across different languages than the settings of multilingual summarization, cross-lingual summarization, and unified cross-lingual summarization. Moreover, since we are the first to use many-to-many summarization, we also propose a pre-trained many-to-many summarization model named PISCES. Our PISCES is trained through a carefully designed three-stage pre-training. Specifically, meta pre-training requires a model to generate original sentences based on the noisy counterparts, and cross-lingual pre-training generates the sentences in the target language based on the noisy parallel sentences in the different source language. And task-specific pre-training utilizes pseudo many-to-many summarization samples to train the model. For more details, please see our paper. The experimental results show that our PISCES outperforms various baselines, including mBART-50 and mT5. We also conduct ablation studies to verify the effectiveness of each training stage and conduct human studies to show the superiority of our PISCES. Thus, please don\\'t forget to check out our paper. Thanks for listening.'}\n",
      "{'id': '7', 'prompt_en': 'Answer the following question concisely given the English content: Do CoNLL-2003 taggers still work?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Funktionieren CoNLL-2003-Tagger noch?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: I tagger CoNLL-2003 funzionano ancora?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： CoNLL-2003 标注器是否仍然有效？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '8', 'prompt_en': 'Answer the following question concisely given the English content: What is the novelty of the proposed human evaluation method?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist neu an der vorgeschlagenen menschlichen Bewertungsmethode?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è la novità del metodo di valutazione umana proposto?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 提出的人工评估方法有何新颖之处？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '9', 'prompt_en': 'Answer the following question concisely given the English content: What does the success of the existing weakly supervised approach heavily rely on?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wovon hängt der Erfolg des bestehenden schwach überwachten Ansatzes ab?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su cosa si basa in larga misura il successo dell'attuale approccio scarsamente supervisionato?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 现有弱监督方法的成功在很大程度上依赖于什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '10', 'prompt_en': 'Answer the following question concisely given the English content: What kind of advances can be done to improve the score?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie kann das Ergebnis noch verbessert werden?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Che tipo di progressi possono essere fatti per migliorare il punteggio?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 可以采取哪些措施来提高分数？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '11', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/PJWMkwXVGI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/PJWMkwXVGI.mp4', 'text': 'Hi everyone. My name is Jack Hessel and I\\'m a research scientist at AI2. I\\'m super excited to be here today to present \"Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest\". This is joint work with a lot of awesome collaborators from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. Have you heard the news? Large language models can now generate and even explain jokes. If you log on to ChatGPT and ask it to tell you a joke, it might generate something like this: Why don\\'t scientists trust atoms? Because they make up everything. Even beyond generating simple jokes like this, some language models have even successfully explained jokes before. Here\\'s an example from Google\\'s 540 billion-parameter PaLM language model. In this case, PaLM is attempting to explain a joke about TPUs, which coincidentally is the type of hardware that the model was trained on. I would argue this is a successful joke explanation. This capability was actually highlighted at Google IO, which is Google\\'s developer conference. Here\\'s a picture of Sundar Pichai touting joke explanation as an interesting capability of PaLM. This public-facing declaration of capacity led to headlines like \"No Joke: Google\\'s AI Is Smart Enough to Understand Your Humor.\" But do large language models really understand humor? If you play around, for example, with ChatGPT, and probe at its humor understanding capacity — for example, by asking it to tell you a knock-knock joke involving a pineapple — you might be a little bit disappointed. Here, ChatGPT appears to paste the word pineapple into some sort of knock-knock joke and then claims that it\\'s made a pun, but it doesn\\'t quite make sense. I don\\'t see the pun here. Now, I\\'m not going to tell you this isn\\'t an objectively funny output. It\\'s a little bit absurd because there isn\\'t a pun, and I am a fan of absurd humor, but the fact that the language model is claiming that there\\'s a pun really calls into question whether or not ChatGPT understands what\\'s going on here. To probe at this in a more structured way, we turn to The New Yorker Caption Contest. The New Yorker is a magazine which has been publishing cartoons since its inception nearly 100 years ago. More recently, starting in the mid 90\\'s, The New Yorker Caption Contest has been run through the magazine as well. The contest works like this: Each week, a captionless cartoon like this cheese sailing one is published in The New Yorker, and readers are invited to submit their best captions for that cartoon. Then the editors select three finalists and a final vote determines the ultimate winner. This is a very popular contest and they get thousands of entries from all across the country and really the globe, including famous celebrity entrants like actors and former presidential candidates. So this is very much on the public consciousness. So what do we do with The New Yorker Caption Contest data? Well, we operationalize it into three different tasks. For the first task, which we call matching, we present models with five choices of captions, only one of which was truly written about a given cartoon. Next, we consider quality ranking. Here we present two captions that were both written about the cartoon, but one was judged by human raters to be much higher quality. These human raters in some cases are The New Yorker editors themselves, and in other cases, this is done by crowd voting. Finally, we have an explanation generation task where we prompt the language model to generate a two to four sentence explanation of why the joke is funny. To support computational experiments on the corpus, we gathered a new set of annotations. In particular, for each of over 700 cartoons representing more than a decade of caption contests, we collect locations, descriptions, uncanny highlights, and entity links for each of the cartoons. Also, we collect a corpus of joke explanations, so we have a set of over 650 2-to-4 sentence explanations of jokes that we use. So, how well do language models do at our various tasks? We\\'ll look first at matching and quality ranking. On the matching task, our best model — which is CLIP fine-tuned on the corpus we annotated — achieves around 62% accuracy on this task. This is relative to a 20% random-guessing baseline. That\\'s one in five. However, humans get around 94% on the same task, representing a big gap in humor understanding. Now you might be wondering, how well do models that don\\'t have to do computer vision do? We wanted to test models like GPT-4 on this corpus, but GPT-4 can\\'t take in pixels directly. So separately, we also consider a setting where we take language models like GPT-4 and condition them to do the exact same tasks, but give them a human-authored description of the image. Even with this additional annotation, there\\'s still quite a big performance gap between GPT-4\\'s five-shot performance and human on the matching and quality ranking tasks. Speaking of GPT-4, let\\'s take a look at some of its joke explanations for our third task of explanation generation. Here we prompted GPT-4 to generate an explanation for this cartoon and caption, \"He\\'ll be back.\" You can see some errors highlighted. So, for example, GPT-4 claims that the customer is the one saying \"He\\'ll be back,\" when I would argue it\\'s pretty clear it\\'s the people who are working at this establishment are saying this, and there are a few other errors as well. This is borne out in human evaluation experiments, where in a blind A/B study, human explanations are preferred to five-shot GPT-4 explanations in more than two-thirds of cases. Overall, we\\'re super excited to see what folks do with our dataset. We have a leaderboard and models available at this URL. Thank you so much for your attention, and I look forward to seeing you at ACL. Thank you.'}\n",
      "{'id': '12', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '13', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/gUAIqKCjIt.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/gUAIqKCjIt.mp4', 'text': 'Hello everybody. My name is Daniel Rotem, and I will be presenting my work, \"Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings\", which was done in Professor Roy Schwartz\\'s lab at the Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models. To use it, we rely on the fact that real-world data varies in complexity. Therefore we can use low-capacity models for easy samples and therefore in that way, reduce the average inference costs, whether it be time or money. The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They are trained separately on the entire training set, and when used for inference, they are run sequentially until a classifier decides to halt the computation. For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They are all trained together, and for inference, a sample is run through the model until a classifier decides to halt, that way saving the computation, which would have been exhausted by the rest of the model. So let us look at the pros and cons of each method. For Multi Model, it is more versatile. It is easily extended. It is expensive to store. And it also suffers from overhead because using the last model for inference means we ran a sample through all previous models without using their output. For Early Exit, we have faster inference, that is, no overhead. It is memory efficient. But model parameters are shared amongst all classifiers, which we will soon see can lead to lower performance. We hypothesize that the last point I emphasized leads to a phenomenon we call conflicting gradients. Each classifier updates model weights, trying to optimize its own goal. Gradient signals from different classifiers may interfere with each other, degrading performance for all classifiers involved. Let us look at this illustration. Classifier M and the update from its loss function, represented here with the color blue, backpropagates through the entire model and updates the first layer. So does the update from Loss function 1 and Loss function 2. All of these updates do different things to the weights of Layer 1, potentially harming the performance of all classifiers in the model. In order to test our hypothesis, we compared individual Early Exit models’ classifiers with separate Multi Model classifiers, which are truncated versions of the BERT pre-trained language model. Keep in mind that training the Multi Model separate classifiers does not suffer from conflicting gradients. As you can see, the Multi Model classifiers outperformed those of Early Exit by an average of 2.3%, both for BERT-base and for BERT-large. Moreover, the gap is largest, for the earliest classifiers, 5.2% on average. We also measured the speed/accuracy trade-off of the models. Notice that for high inference speeds, Multi Model is much better. However, when we use later classifiers to predict, we see that Early Exit outperforms Multi Model because of the overhead suffered by Multi Model predicting with its largest classifiers. Based on these findings, we present SWEET: Separating Weights in Early Exit Transformers. It is a novel fine-tuning method for Early Exit architectures. What we do is we train an Early Exit architecture where each layer receives updates only from the following classifier. That means we avoid the conflicting gradient problem completely. Let\\'s look at this illustration. On the left-hand side, you see the standard Early Exit model. On the right-hand side is our method, SWEET. You can see that each transformer layer only receives updates from its following classifier\\'s loss function, therefore avoiding the conflicting gradient problem. Let us look at the results of the SWEET method. When evaluating the individual layers, we can see that SWEET closes most of the gap between Early Exit and Multi Model. However, in some cases, later classifiers are negatively affected by our method, as you can see here by the negative gap between SWEET and Early Exit. We also ran the test and examined the speed/accuracy trade-off. We can see here that in fast speeds, SWEET outperforms both methods, whereas for BERT-Large, it outperforms both methods throughout the entire speed/accuracy curve. The takeaways from our work are as follows. We show the existence of conflicting gradients in Early Exit training process. To the best of our knowledge, we conduct the first fair comparison of Early Exit and Multi Model adaptive inference methods. We also introduced the SWEET method, the results of which motivate future research and fine-tuning algorithms tailored to the Early Exit architecture. Thank you very much for listening. If you enjoyed the talk and you want to see more, go visit our paper on Archive, \"Finding the SWEET spot\".'}\n",
      "{'id': '15', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '16', 'prompt_en': 'Answer the following question concisely given the English content: Which domains are simplified more?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Domains werden stärker vereinfacht?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali domini risultano più semplificati?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些领域的简化程度更大？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '17', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/eXmqPhcZFN.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/eXmqPhcZFN.mp4', 'text': 'Hi everyone. My name is Shengqiong Wu, a PhD student at NUS. I\\'m glad to introduce our work about multimodal relation extraction. Relation extraction is a widely explored task. It aims to determine the semantic relation between entities in a given text. However, in some realistic scenarios, such as in social media, the data is often in various forms and modalities rather than just pure text. So if you just look at the text, you may find that it lacks sufficient context to understand some ambiguous or multi-context word. So multimodal relation extraction has been introduced recently. Additional visual sources are added to the textual RE. For example, with the support of visual evidence such as \"Bachelor\", \"Gown\", \"Cap\", we can easily infer the relationship between JFK and Harvard is \"graduated at\". However, there are still some problems. One problem is internal-information over-utilization. This is because when inferring the relationship between two entities, only parts of the text are useful. Moreover, not all and always the visual sources play positive roles for the target task. Therefore, we argue that a fine-grained information pruning over two modalities is needed. The other problem is external-information under-exploitation. This is because, although compensating the text inputs with visual sources, there can still be an information deficiency, especially when the visual features serve less or even negative utility. In this way, we think more external information should be considered, such as topic information. To address these two issues, we firstly propose a Graph Information Bottleneck principle-guided feature refinement. And then we consider taking multimodal topic information as additional semantic supplementary to enrich the overall context. This is the whole framework of our proposed method. It consists of five parts. Firstly, we represent the text and image with the corresponding visual scene graph and the textual scene graph. And then we consider merging the visual scene graph and the textual scene graph into one unified backbone cross-modal graph, named CMG. Next, we screen the initial CMG structures by fine-grainedly filtering nodes and adjusting edges in CMG. To ensure the above adjustments of CMG is correct, our graph information bottleneck is leveraged to guide optimization. Then we enrich the compressed CMG features with the multimodal topic features. We retrieve the associated top-L textual and visual topic keywords, and then an attention operation is used to integrate the multimodal topic words to enrich the overall context. To evaluate the effectiveness of the proposed method, we conduct experiments on a widely used MRE dataset. As you see from the table, compared with the text-based method, leveraging visual features can obtain higher performances, and we see that all proposed methods can get the best performance among the multimodal baselines. In the ablation study, we find that the information screening and compensating both contribute to the task performances. And the scene graph is beneficial for structural modeling of the multimodal inputs. When removing the scene graph, the performances get decreased. Next, we want to know, \"Under what circumstances do the internal-information screening and external-information exploiting help?\" So we group the instance by text region relevance scores and make the predictions for different groups. For the input with higher text-vision relevance, the GENE plays a greater role than LAMO, indicating the performance screening plays a more important role because most of the high cross-modal relevance input features come with rich yet even redundant information, where the internal-information screening is needed for denoising. For the inputs with lower text-vision relevance, LAMO contributes more significantly than GENE. That means the external-information exploiting is more useful. In conclusion, we introduce a novel idea of simultaneous information subtraction and addition for multimodal relation extraction. We perform internal-information screening with the guidance of the graph information bottleneck principle. We devise a latent multimodal topic model, and induce latent multimodal topic features to enrich the feature contexts. Our overall system achieves significant improvements over the existing best models on the benchmarks. So thanks a lot. If you\\'re interested in our work, you can scan the QR code for more detailed information. Thank you.'}\n",
      "{'id': '18', 'prompt_en': 'Answer the following question concisely given the English content: What is the example of the preference for shorter left conjuncts?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lautet das Beispiel für die Präferenz für kürzere linke Konjunktionen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è l'esempio della preferenza per i congiunti a sinistra più brevi?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 偏好较短左并列词的示例是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '19', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xUDLtuhJUS.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xUDLtuhJUS.mp4', 'text': 'Hello everyone. My name is Zhang Qin, a master\\'s student from Shenzhen University. I\\'m so glad that our work, \"A Survey for Efficient Open Domain Question Answering\", was accepted by ACL 2023. It\\'s my great honor to present our work. I will introduce our work following the five parts. Our work focuses on open-domain question answering. A mainstream framework is the two-stage model proposed by Danqi Chen in 2017 year. The first stage uses a retrieval to retrieve several evidence contexts from a Wikipedia corpus and the second stage uses a reader to understand the question and retrieve the evidence to reason out the answer. For the retrieval, it is composed of two encoders: question encoder and document encoder. Before we answer the question, the Wikipedia corpus should be preprocessed into an indexed shell by the documenting coder. So when it receives a question, the retrieval only needs to encode the question and search the index file to retrieve evidence context. Here are some challenges of open-domain question answering. We should notice that the Wikipedia corpus is very large. It contains 26 million documents, and storing it requires 20 GB. The second point is that the index file, which is 65 GB, and searching the index file becomes the bottleneck of inference speed. The third point is that there contains multiple language models with millions of parameters. These points make open-domain question answering systems doubly challenging in real-time applications and deployments to resource-constrained devices. Our motivation is to achieve efficient open-domain question answering systems, specifically smaller memory costs, faster inferring, and comparable performance. Next, we will sum up some core techniques to achieve these goals. In addition to retrieval and reader frameworks, some one-stage frameworks have been proposed. For example, the retrieval-only system, as shown in the middle subgraph, and the generator-only system in the right subgraph. The retrieval-only system retrieves the answer from the index directly and the generator-only generates the answer directly. Then we summarize some efficient tactics from several aspects. The first aspect is how to research evidence fast. Compared to the brute search, the approximate nearest neighbor search may be an efficient method. And the second aspect is how to read fast. Some researchers proposed skip reading, such as adaptive computation. It only skips some context reading if these contexts are less likely to contain the answers. The third aspect is how to reduce the index size. Oh, there are two little things. On document filtering before retrieval and embedding, dimension completion, or product quantization. The first aspect is how to reduce the model size. To achieve this goal, you can select lightweight models or parameter sharing, or designing fewer models. Such as using one model to achieve the retrieval and reading. We also compare existing open-domain question answering models from the data aspect, as shown in the left figures. We can observe that retrieval and reader systems perform well-balanced among speed, memory, and performance. The retrieval-only systems create large indexes, but they infer answers quickly. And the generator-only systems create no index, but they are always large models and achieve low performance. Based on this analysis, we come to our conclusions or insights. If one is limited by resources, you can consider reducing index size by generator-only systems or embedding compression. Or maybe reduce the model size by knowledge distillation or designing a one-stage model. For both retrieval and reading, if you pursue real-time feedback, retrieval-only systems should be good choices. If one pursues trade-offs, retrieval and reader systems are relatively more appropriate. Finally, we discuss two future works. The first one is how can the open-domain question answering systems be deployed in low-power devices, and the second one is more evaluation metrics should be considered. That is all for my presentation. Thank you for your attention.'}\n",
      "{'id': '20', 'prompt_en': 'Answer the following question concisely given the English content: Can I use the models for my research?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Kann ich die Modelle für meine Forschung verwenden?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Posso usare i modelli per la mia ricerca?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 我可以将这些模型用于我的研究吗？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '21', 'prompt_en': 'Answer the following question concisely given the English content: DEplain-web contains documents from the web. Which type of content is inside DEplain-apa?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: DEplain-web enthält Dokumente aus dem Internet. Welche Inhalte enthält DEplain-apa?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: DEplain-web contiene documenti del web. Quale tipo di contenuto è presente in DEplain-apa?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： DEplain-web 包含来自网络的文档。DEplain-apa 中包含哪种类型的内容？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '22', 'prompt_en': 'Answer the following question concisely given the English content: Which factors lead to good generalization?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Faktoren führen zu einer guten Generalisierung?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali fattori contribuiscono a una buona generalizzazione?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些因素有助于良好的泛化？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '23', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/yBDqNxQUwV.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/yBDqNxQUwV.mp4', 'text': \"Hi. I'm Dan Garrette, and I'm going to talk about our work on improving the ability for text image models to render visual text. Text image modeling research has made huge strides in the last year, with the ability to generate very high quality, interesting images, but a lot of people have noticed that these models are often very bad at representing text. We specifically look at the Imagen model, which works by taking the input text, encoding it with a T5-XXL encoder, and then using that encoded text representation as input to a diffusion model, which is actually able to generate the image. Here we can see a fairly complex input producing an accurate image output. However, even a much simpler textual input that requires the image to contain a word will often fail. To understand what's going on here, we can dig into the text encoder itself. T5 uses SentencePiece tokenization, which means that instead of the model receiving the individual letters that make up the spelling of the input, it's receiving subword IDs for chunks of the input string. That means if it's asked to render a word, it has to be able to decompose that atomic subword vocabulary item into the individual letters that make it up, so that it can draw each of those letters. So the first set of experiments that we did in our paper was to understand the degree to which these text encoders know how to spell the words. So we looked at T5, and you could see from this graph that at smaller scales, T5 does a very bad job of spelling. So Base and Large get under 20% accuracy on spelling. The larger versions do better, but still even the XXL model, the largest T5 model, gets under 70% accuracy. And, given that these models are able to do fairly complex NLP tasks, it seems surprising that they are so bad at something that should be trivial, such as spelling. We also investigated the PaLM models, which do a much better job of spelling, with the larger PaLM models getting near perfect accuracy. But these models are substantially larger in terms of number of parameters and are trained on far more data, which makes them impractical for a lot of applications. Conversely, we can compare this with ByT5, which is a model that, instead of using subword tokenization, actually receives the individual bytes of the input string, which is an even smaller granularity than the individual characters. So, ByT5 has full access to the spelling information, and all it needs to do is effectively learn to copy characters from the input to the output. And so you can see in this graph here that across all scales, ByT5 does very well at spelling. To understand a bit better what's happening and see how this phenomenon plays out, we can break down the results by the frequency of the word that is being spelled. So for T5, we see that actually the most frequent words — so this is that top 1% category in this graph — are the words that T5 struggles the most on spelling. And that's because the way the SentencePiece algorithm works is that more frequent words tend to be represented by a single vocabulary item or a couple of subwords. And so that means that the model is having to take a smaller number of bigger chunks and decompose it into a larger number of letters. Here again, we can see that ByT5, because it has full access to the character-level information, does very well and isn't affected by the frequency of the word. All it has to do is learn how to copy the input characters to the outputs. So how can we use these intuitions to actually improve the text rendering models? What we did was to augment the Imagen model by adding or concatenating to the existing text representation an additional text representation that came out of the ByT5 small model. And we'll note here that we used ByT5-small because it only increased the parameter count of the text encoder by about 5%. But even that small addition was enough to give the model the ability to spell quite well, and so this improved image generation characteristics, as well as the ability for it to render text. It's not perfect at rendering text, though, because the diffusion model actually can introduce errors, so even though the text encoder itself knows the spelling, the generation might contain mistakes. So the main takeaways from our paper are the WikiSpell benchmark for text-only models, the DrawText benchmark for text-to-image models, and a new efficient strategy for improving model spelling ability, which is to concatenate in a model that's actually aware of the characters in the input.\"}\n",
      "{'id': '24', 'prompt_en': 'Answer the following question concisely given the English content: How was the tendency for left conjuncts to be shorter measured?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurde die Tendenz zu kürzeren linken Konjunktionen gemessen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come è stata misurata la tendenza dei congiunti a sinistra a essere più brevi?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何衡量左并列词是否更短？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '25', 'prompt_en': 'Answer the following question concisely given the English content: How were the experiments designed to study the effect of the governor’s position?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurden die Experimente gestaltet, um die Auswirkungen der Position des Begrenzers zu untersuchen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come sono stati progettati gli esperimenti per studiare l'effetto della posizione del governatore?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何设计实验来研究支配词位置的影响？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '26', 'prompt_en': 'Answer the following question concisely given the English content: How well does a baseline classifier work training on imbalanced data?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie gut ist ein Basisklassifikator, wenn er mit unausgewogenen Daten trainiert wird?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanto è efficace un classificatore base se addestrato su dati non bilanciati?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 基线分类器在不平衡数据上的训练效果如何？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '27', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '28', 'prompt_en': \"Answer the following question concisely given the English content: What are the characters' names in the example conversation?\", 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißen die Personen im Beispielgespräch?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i nomi dei personaggi nella conversazione presa a esempio?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 示例对话中的角色名字是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '29', 'prompt_en': 'Answer the following question concisely given the English content: On which discourse phenomena do context-aware MT models improve over context-agnostic ones?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Bei welchen Diskursphänomenen schneiden kontextsensitive MÜ-Modelle besser ab als kontextagnostische Modelle?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su quali fenomeni del discorso i modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在哪些话语现象上，语境感知 MT 模型比语境无关模型更有优势？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '30', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wLmrUehthl.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wLmrUehthl.mp4', 'text': 'Hello everyone. We are going to introduce our paper \"LLM-Blender\", which is a simple yet effective ensemble learning framework for large language models and this key idea is based on pairwise ranking and generative fusion. We are a team from AI2 and USC, and my name is Yuchen Lin. There are so many large language models released every week, and many of them claim that they have achieved great performance. And from this leaderboard, we can indeed say some models are better than the others, but this is only about average overall performance. And when you have a particular input example, should you simply use a single top #1 model? And our findings suggest \"no\": the optimal selection of the models can significantly vary across different input examples. So, for example, although Vicuna has the best average overall performance out of these 11 models, but in only 21% of the examples, it is the best model, the optimal model that ranks to the top #1. This pie chart suggests that each language model has its own advantages and disadvantages. Therefore, we believe that we should consider using more large language models for each input, so that we can select and generate better output than using any single model for all inputs. To this end, we propose a two-stage framework named LLM-Blender. Given a certain input X, we will run n different models and get their outputs Y₁ to Yₙ. Then we use a pairwise ranking module named PairRanker to compare all these candidates and get a ranking of them. Specifically, we concatenate the input X and each pair of these candidates, Yᵢ and Yⱼ, and use a cross-attention module such as RoBERTa for learning to distinguish which candidate is better for the input X. And given the comparison matrix here, we can aggregate the results to get a final order of these candidates. And in the next stage, we pick the top K, let\\'s say top three candidates, and we use them as the input to a sequence-to-sequence model for learning and inference in a generated fusion model. So this fusion model will then output the final output for this input X by fusing the top three candidates ranked by this PairRanker. So this is the overall pipeline of our LLM-Blender. Let\\'s take a closer look at the PairRanker module. So comparing with the prior methods, a key difference of our PairRanker is in this encoding stage. The green boxes here are the encoders of these four methods, and our PairRanker encodes a pair of candidates alongside the input X for better analyzing the subtle differences between these two candidates. So this is very different from parameters, which look at each candidate individually and then score the candidate individually, and then rank all the candidates based on their scores. We believe PairRanker is a better solution because it uses pairwise comparisons to learn and infer the quality of all these candidates and compare them side by side more carefully. Given the pairwise comparison results, we can derive a matrix here, where each element in this matrix represents the comparison logits for candidate I being better than candidate J. Then from this matrix, we can have three methods to aggregate all these results. And we found that using the max logits to aggregate the order is the best solution, but if you worry about the efficiency, you can also use the bubble sort algorithm here. Then it\\'s very efficient, and we can also get a decent performance. And experiments here show that PairRanker is much better correlated with the oracle ranking, better than all the other ranking methods on various correlation metrics. To enable the evaluation of ensemble learning frameworks, we also create a new dataset named MixInstruct. It consists of existing instruction datasets, and we collect the candidates from 11 open-source large language models. And, we use our BERTScore, BLUERT, and BARTScore as the automatic metrics, and we also use ChatGPT as a judger to compare the results as well. So here we show that our empirical results, where we can see that the top two models, Open Assistant and Vicuna, their performance is consistently worse than our PairRanker and the full Blender framework on all these four metrics. And, particularly, Blender\\'s results can beat them in 68% and 76% of examples, respectively, for Open Assistant and Vicuna. And these results suggest that Blender is a very promising framework for ensemble learning, although it\\'s very simple and straightforward. So in the end, we want to give some take-home messages here, that Large Language Model Blender is a simple and effective ensemble learning framework for Large Language Models. It has two sub-modules: The PairRanker is a pairwise comparision module that we can get the matrix for all these results, and GenFuser takes the top three candidates and generates the final output. And, it largely improves the performance. And, MixIstruct is our dataset for evaluating the large language models here. And we also released a unified codebase on our data for evaluation and future research. Ok, So that\\'s all. Thank you very much.'}\n",
      "{'id': '31', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '32', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '33', 'prompt_en': 'Answer the following question concisely given the English content: How exactly does the introduced framework quantify the positionality?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie genau quantifiziert das vorgestellte Framework die Positionalität?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo il framework introdotto quantifica esattamente la posizionalità?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 引入的框架如何量化立场？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '34', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UzozuIPLRV.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UzozuIPLRV.mp4', 'text': 'Hello everyone. My name is Marcos Treviso, and today I\\'m going to present a work called \"CREST: A Joint Framework for Rationalization and Counterfactual Text Generation\". This is a result of a great collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. So let\\'s say we have an input like this one, for which the classifier predicts a particular decision. There are many methods for interpreting this decision. One cloud of methods uses a selective rationalization, which provides explanations by highlighting the tokens in a faithful way. Another method, which is more closely aligned with the human causal reasoning, is to generate counterfactuals by editing specific parts of the input. In this work, we propose to combine these methods to leverage their complementary strengths. The result is CREST, a joint framework for rationalization and counterfactual text generation. So, the first component of CREST is responsible for generating counterfactuals. Let\\'s say we have an input X, like this one. We pass this input to our rationalizer model, which has a trainable masker component that is trained to produce meaningful rationale, Z. And then the other component of the rationalizer, the predictor, receives this rationale as input for a decision. However, to generate counterfactuals, we focus only on this rationale. Specifically, we used them to mask the original input alongside prepending the gold label to it. For example, here we prepend the gold label \"positive\" and mask the words \"terrible\" and \"really bad\". Next, we pass the masked inputs to an editor, which in practice is a masked language model that fills in the masked response with new tokens. The output is X-tilde, a counterfactual example. To evaluate the quality of the counterfactuals produced by CREST, we compare CREST against related works. Then, we employ both automatic metrics and human evaluation for this, but in this presentation, I will focus only on the human evaluation experiments. Basically, we asked humans to judge 100 examples from IMDB and SNLI in terms of their validity and naturalness using a 5-point Likert scale. We found that, as expected, humans found manual counterfactuals to be more valid and natural than those generated by CREST and MiCE. And among the automatic approach, CREST and MiCE, humans judge CREST counterfactuals to be more valid and natural than those produced by MiCE. And this is great because we know that we can use this strategy to produce high-quality counterfactuals, but what else can we do with them? An interesting approach is to use them for data augmentation, and we test in our work. However, we also propose an alternative way that performs rationalization with both factual and counterfactual examples, and here is how it works. Let\\'s say we start with CREST-Generation, which produces two outputs in the end. One is basically the rational Z for the original input X, and the other is the counterfactual X-tilde alongside the demarcations for in-filled response Z-tilde. From here, we get two computation flows: a factual flow responsible for processing the original input and a counterfactual flow for the counterfactual input. Both inputs are passed through a shared rationalizer that learns to highlight meaningful rationales that are then passed through a predictor module that produces a final decision. Finally, to effectively leverage the paired structure of the inputs, we add a new regularization term that encourages the new rationales to be similar to those originally generated by CREST-Generation. Intuitively, this encourages the new rationalizer to focus on the specific parts of the input that encode factual and counterfactual reasoning. We carry out experiments by training on IMDB and SNLI and testing on in-domain, contrastive, and out-of-domain datasets, but here I will focus only on IMDB. Here are the results with four different setups. In the first setup, we train models using only factual examples. In the next two setups, we perform data augmentation either using human counterfactuals or counterfactuals generated by CREST. And finally, the models that use CREST-Rationalization. We see that CREST-Rationalization achieves the top result on IMDB itself. In the contrastive datasets, it performs on par with data augmentation using human counterfactuals. And finally, for out-of-domain data sets, it outperforms the other methods. These results are great. They basically show us that we can use CREST coutnerfactuals to improve the downstream models. So now we ask, \"Are the rationales generated by CREST interpretable?\" And we do this analysis in three dimensions, in terms of plausibility, forward simulability, and a new proposed metric that we call counterfactual simulability. Intuitively, this metric measures the ability of an explanation, Z, to change the classifier\\'s decision when the classifier receives as input a contrastive edit that was guided by this explanation. So here are the results. Overall, we note that CREST-Rationalization produces more plausible rationales than other approaches. In these rationales, it also achieves a higher counterfactual simulability than those produced by other methods, and this is by a large margin. So, to summarize, we propose CREST, a joint framework for selective rationalization and counterfactual generation that produces valid, fluent, and diverse counterfactuals in a controllable way. In that, by leveraging these counterfactuals during training, it leads to plausible explanations that focus on the contrasting parts of the input. Take a look at our paper and our code for more information. Thank you.'}\n",
      "{'id': '35', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '36', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/YBVEIewTjn.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/YBVEIewTjn.mp4', 'text': 'Hi! Welcome to ACL. I\\'m Telmo Pessoa Pires, and this is a sneak peek into \"Learning Language-Specific Layers for Multilingual Machine Translation\", a joint work with Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz. Multilingual machine translation has several advantages, namely scalability, as it\\'s easier to train and maintain a single model rather than one model per language direction. Speed, because you can directly translate between any two languages instead of having to pivot through a third language like English. And this also leads to less error cascading. Finally, you get improvements for low-resource language pairs. On the other hand, it also means limited capacity per language, and while you can increase the model size, that makes training harder and inference slower. Our goals in this work are to increase the capacity per language, but only where it matters the most. And while at the same time keeping the inference costs constant. Our solution is Language-Specific Layers, or LSLs, which we show you on this slide, where the idea is to have one regular transformer layer per language. And this language is used to select and train at inference time the correct sublayer, and can be either the source or the target language. In the example here, we select the Portuguese sublayer, meaning the other weights are ignored. And this is what lets us keep the inference cost constant, because at inference time only the Portuguese sublayer is called, not the rest. Next, we\\'ll look at LSL placement. While we could place LSLs everywhere in the model, that wouldn\\'t necessarily increase the model size. In our initial experiments, we didn\\'t see significant improvements from placing LSLs in the decoder, so we focused on the encoder. We could proceed by trial and error. However, that is exponential in the number of layers. So our idea was to let the model learn the best placement. To do that, we use the following approach. For each encoder layer, we have three weights: a shared weight, a source weight, and a target weight. And corresponding components. And we\\'re going to train this large model with all of these components. And then we\\'re going to look at the weights. This is only used to learn the placement, to figure out where we place the LSLs. After that\\'s done, after we select the architecture, we\\'re going to drop this model and train a new architecture from scratch. If you plot the weights, you will see something like what you are shown here on the right, where you see three main trends. The source weights are more or less of constant importance across the whole encoder, shown here in red. On the bottom of the encoder, you see that the source weights are more important and the target weights are less important. And on the top of the encoder, you see the opposite effect. To select the placement, we tried several approaches, including introducing a prior on the weights to try to make them sparse. However, the approach that we show on the paper and the one that performed the best in our experiments was to simply select the component based on which weight is the largest. So for example, for the bottom layer, the largest weight is the shared weight, so that\\'s going to be the shared layer. So if you do that, you\\'re going to get an architecture like the one we show here, where the bottom two layers are shared, the next two layers are source-specific LSLs, then we have a few more shared layers, then three target-specific LSLs, and the top layer is shared. In this case, we\\'re showing a target-specific decoder model, but in the paper, we also show results for shared decoder models. So for our experiments, we trained on WMT21 news translation mask sources for the 10 languages shown on the slide, which includes some European languages, some Asian languages, and Swahili as a low-resource language. We evaluate on Flores-101 and report chrF, spBLEU, and COMET. And architectures are all based on the deep encoder shallow decoder model. We report numbers for a baseline model, which is a regular transformer model with no language-specific layers but with different hidden sizes, and also for language adapters and our learned architecture. So you can see that our learned architecture in green has significant improvements over both the language adapters approach and even the largest baseline model, even though our approach at inference time is significantly faster, which you can check in our appendix. So the previous numbers I showed are averages over all the languages, which in our case means 90 different translation directions. Here in this table, we show averages per language, and you can see the same trend. Our approach gives improvements for every language and, in particular, these improvements are large for the low-resource languages. Also, we ran statistically significant tests, and we found that our improvements are significant for 84 out of the 90 translation directions. So that\\'s it for this short peek into the paper. If you have more questions or you want to see more setups like shared or separate decoders, ablation studies, different metrics, please check out the full paper, or come see us at our poster session. Thank you very much, and enjoy ACL. Bye-bye.'}\n",
      "{'id': '37', 'prompt_en': 'Answer the following question concisely given the English content: What was the finding of the previous study where human subjects were given the same persona prompts?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was war das Ergebnis der vorherigen Studie, bei der die menschlichen Teilnehmenden die gleichen Persona-Prompts erhalten haben?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è stato il risultato dello studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '38', 'prompt_en': 'Answer the following question concisely given the English content: Which sources of data were used in this study?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Datenquellen wurden in dieser Studie verwendet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali fonti di dati sono state utilizzate in questo studio?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 此研究使用了哪些数据来源？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '39', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '40', 'prompt_en': 'Answer the following question concisely given the English content: What are some closely related tasks for cognitive dissonance?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind eng verwandte Aufgaben für kognitive Dissonanz?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le attività strettamente correlate alla dissonanza cognitiva?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 与认知失调密切相关的任务有哪些？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '41', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/cXldXcbmRG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/cXldXcbmRG.mp4', 'text': 'Hi, this is Silin coming from the Natural Language Processing Lab at EPFL University. Now I\\'m going to introduce our work of \"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives\", collaborated with Sony Group Corporation. Sustaining coherent and engaging narratives, such as dialogues or stories, requires natural language processing systems to understand how the personas of speakers, listeners, or characters ground narrative. For example, an adventurous architect may be interested in talking about outdoor explorations with his friends who have similar hobbies, but he may prefer to discuss architectural design ideas instead with his colleagues at work. However, narrative systems have not yet learned good representations of real-world personas, which involve rich world knowledge and complex interconnections with each other. For instance, as shown in the picture, a singer preparing an album may have studied music at university. This would allow him to share the experience with a student majoring in composition, who may study music as a daily routine. In this work, we propose a Persona-grounded Commonsense Knowledge Graph, PeaCoK, to represent such world-level persona knowledge at scale. PeaCoK contains about 3,800 personas and 40,000 distinctive attributes, which form about 100,000 personal inferences or facts. Besides, about 9,200 attributes are connected to two or more personas which contributes to the rich interconnections of personas in PeaCoK. Based on the studies of human interactive behaviors, we frame the relations of personas and their attributes in three dimensions, including four types of main relations, as well as interactivity and distinctiveness. Detailed descriptions of these relations are listed here. We built PeaCoK in three steps: First, we select personas from existing commonsense graphs which include both human roles and event-based entities. Then we induce attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models. Finally, we crowdsourced the annotations of PeaCoK relations using a joint human-AI majority voting scheme. Expert study shows that a majority voting with AI in the loop yields high-quality relation annotations with average 87% of accuracy in F1. The AI annotator InstructGPT-3 efficiently mediates the disagreements between human annotators with lower temporal and financial costs. Now, the graph in general cannot have a comprehensive coverage of all world knowledge, so we need to face the question of whether PeaCoK can help language models to learn and generalize persona knowledge. We use PeaCoK to train a BART-based common knowledge generator on a persona attribute inference task where the model needs to predict an attribute of a given persona with a target relation. As baselines, we compare to large-scale pre-trained language models, including five-shot GPT-3 and zero-shot GPT-3.5. Compared to the baselines, Comet-BART trained on PeaCoK achieves overall better automatic evaluation results on various natural language generation metrics and also a higher accept rate in human evaluation. This indicates that PeaCoK can serve as a reliable persona knowledge base, which enables light-weight language models to learn knowledge generation capabilities comparable to large-scale language models. Finally, we explore whether PeaCoK knowledge can be used to improve downstream narrative modeling. We investigate a persona-grounded dialogue generation task on the ConvAI2 PersonaChat data set. Specifically, we use a knowledge linker to retrieve facts from PeaCoK that are relevant to each speaker\\'s original persona profile and utterances. Then we convert the retrieved facts into natural language statements to augment each speaker\\'s profile. We choose the P²Bot model as our baseline dialogue system. Human evaluation shows that PeaCoK augmented models achieve better dialogue generation on various aspects, including fluency, consistency, engagement, and persona expression. By comparing to the augmentation with Atomic2020 knowledge graph, we also find that PeaCoK\\'s persona-centric commonsense knowledge yields a more positive impact compared to general social commonsense knowledge. We also stratify our human evaluation results based on the overlap of the two speakers’ augmented PeaCoK knowledge, where we find that in terms of dialogue, consistency, and engagement, the winning rates of PeaCoK augmented model increase as the number of shared common attributes between two speakers becomes larger. Since more connections between speakers leads to more consistent and engaging conversations, this highlights the importance of learning PeaCoK\\'s interconnected world persona knowledge in narratives. In summary, we propose a world-level Persona Commonsense Knowledge Graph, PeaCoK, that contains large-scale, high-quality persona inferences. Our knowledge resource can be used to train reliable persona knowledge generators and also enable more consistent and engaging narrative modeling. Our paper and GitHub site for this work are public now, which can also be found on our lab\\'s website.'}\n",
      "{'id': '42', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '43', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '44', 'prompt_en': 'Answer the following question concisely given the English content: How does the introduced framework differ from the previous works?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Inwiefern unterscheidet sich das vorgestellte Framework von bisherigen Arbeiten?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo il framework introdotto differisce dai lavori precedenti?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 引入的框架与以前的研究有何不同？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '45', 'prompt_en': 'Answer the following question concisely given the English content: Which of the three compared setups overlaps the most with the lexicon of stereotypes?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches der drei verglichenen Setups hat die meisten Überschneidungen mit dem Lexikon der Stereotypen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale delle tre configurazioni messe a paragone si sovrappone maggiormente al lessico degli stereotipi?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在三个比较设置中，哪一个与刻板词汇的重叠最多？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '46', 'prompt_en': 'Answer the following question concisely given the English content: Which commercial systems were compared?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche kommerziellen Systeme wurden verglichen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sistemi commerciali sono stati messi a confronto?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 比较了哪些商业系统？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '48', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '49', 'prompt_en': 'Answer the following question concisely given the English content: Up to how many tokens context length were MPP evaluations performed?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Bis zu wie vielen Token Kontextlänge wurden MPP-Auswertungen durchgeführt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Fino a quanti token di lunghezza del contesto sono state eseguite le valutazioni MPP?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： MPP 评估最多涵盖了多少个词元的上下文长度？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '50', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '51', 'prompt_en': 'Answer the following question concisely given the English content: Which domains did they include in their dataset?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Domains haben sie in ihren Datensatz aufgenommen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali domini sono stati inclusi nel loro set di dati?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 他们的数据集中包含哪些领域？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '52', 'prompt_en': 'Answer the following question concisely given the English content: What is the definition of positionality in general?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lässt sich Positionalität im Allgemeinen definieren?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è la definizione generale di posizionalità?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 一般来说，positionality（立场）的定义是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '53', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '54', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '55', 'prompt_en': 'Answer the following question concisely given the English content: Does EDAtt adapt an existing offline ST model?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Passt EDAtt zu einem bestehenden Offline-ST-Modell?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: EDAtt adatta un modello ST offline esistente?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： EDAtt 是否适应了现有的离线 ST 模型？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '56', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '57', 'prompt_en': 'Answer the following question concisely given the English content: Does the tested model work on the test suite?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Funktioniert das getestete Modell in der Testsuite?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il modello testato funziona sulla suite di test?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 被测模型是否能在测试套件上运行？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '58', 'prompt_en': 'Answer the following question concisely given the English content: Which three variants of KITMUS are there?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches sind die drei Varianten von KITMUS?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le tre varianti di KITMUS?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： KITMUS 有哪三个变体？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '59', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '60', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '61', 'prompt_en': 'Answer the following question concisely given the English content: What is the last research question?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist die abschließende Forschungsfrage?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è l'ultima domanda di ricerca?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 最后一个研究问题是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '62', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/BIdHavzwsU.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/BIdHavzwsU.mp4', 'text': 'Hello. My name is Nitay Calderon, and I\\'m the main author of this ACL paper, \"A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training\". This is a fantastic collaboration with Amir and Subhabrata from Microsoft and my PhD advisor, Roi. So as we all know, natural language generation systems (or NLG systems) are based on large language models, and they become larger, more complex, and much more slower, which can also come with great financial cost. As a result, there is a growing demand in the industry for compressing these models. And the main problem is that while you want to compress your whole a huge model, you also want to preserve its performance. And this is exactly our goal of this paper. We want to explore the potential of NLG compression, or in other words, finding the recipe. So in order to compress the model, you should probably use a smaller version of it or apply pruning, which typically first fine-tunes the model and then discards complete layers from the encoder or the decoder components. After that, there is a knowledge distillation stage in which we transfer knowledge from a large teacher model to a small student one, basically by training the student to mimic the teacher. In NLG, there are two main types of noise distillation. The most common one is word-level distillation, where you train the student by minimizing the KL divergence between the logits of the student and the logits of the teacher. Or sequence-level distillation, where we use the teacher to generate pseudo-targets. Pseudo targets are output text or generated by the teacher, and after generating them, you augment the training data and fine-tune the student. So in contrast to many, many, many knowledge distillation works which focus on classification tasks, NLU, or on the pre-training — which we call a task diagnostic — and also in contrast to the NLG works which typically focus on a single specific task like translation, NMT, or summarization and use really, really large data sets with hundreds of thousands of parallel labeled examples... So, in contrast to all of these works, we conduct a systematic study of task-specific knowledge distillation for NLG. We consider a variety of NLG tasks in realistic setups. And what are exactly realistic setups, or as I like to call them, industry-driven setups, where we have five criteria. So first, we use a medium-resource labeled data set. This is because the annotation is very costly and labor-intensive, especially in NLG. Second, we assume we have large amounts of unlabeled data, and we also use medium-sized off-the-shelf models, which are more practical. And we focus on inference time efficiency, meaning high compression rate. And finally, one-time training resources are negligible because compared to the accumulated costs of inference time. We have four NLG tasks in the study. We have a summarization, we have a question generation, common sense reasoning, and simplification and style transfer. In all of the datasets, we have ratio of 1 to 4 labeled to unlabeled examples. And this slide summarizes the whole paper. We have eight stages and also an extreme setup, which I won\\'t discuss in this video. The first two stages explore architectural decisions; for example, in the first stage, we compare encoder/decoder to decoder-only architectures. Then we continue on to understanding the impact of pruning on the task performance or computational performance. And then we have two more stages, where we compare different approaches for knowledge selection and state-of-the-art baselines, you can call them. Then we move to the main contribution of this study, which explores the extensions of the usage of pseudo-targets. So as I said, we challenge the traditional sequence-level knowledge distillation in which you generate a single mode approximation pseudo-target using beam search. And we first show how crucial unlabeled data is in order to boost the distillation. We then show that generating multiple pseudo-targets rather than a single one improved the student. And we also show that sampling pseudo-targets and not using beam search, or even sampling with high temperature, which makes the pseudo-targets to be more diverse, exposes the student to more diverse knowledge of the teacher. And finally, we also propose a novel knowledge distillation technique we call joint-teaching, which aims to address the student exposure bias, grounded learning, and also teach the student to correct its own mistakes. The joint-teaching applies word-level knowledge distillation on pseudo-targets generated by both the teacher and the student. And for more details about the study, the methods, and the first exposure bias motivation we provide in knowledge distillation setups, you can scan the QR code at the first slide or read the paper. And of course, I would like to see you next to my poster, and we will discuss this paper. So finally, I can\\'t leave you without the recipe for distillation in NLG.'}\n",
      "{'id': '63', 'prompt_en': 'Answer the following question concisely given the English content: How does the metric sensitivity work?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie funktioniert die Sensitivitätsmetrik?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come funziona la sensibilità della metrica?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 指标灵敏度是如何工作的？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '64', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '65', 'prompt_en': 'Answer the following question concisely given the English content: Does greater sensitivity indicate improved model performance, or does it suggest the opposite?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Bedeutet eine höhere Sensitivität eine bessere Leistung des Modells oder ist das Gegenteil der Fall?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Una maggiore sensibilità indica una performance del modello migliore o suggerisce il contrario?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 更高的灵敏度是否表示模型性能得到了提高，还是表明相反？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '66', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/zqzsnoFTCA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/zqzsnoFTCA.mp4', 'text': 'Hello everyone. We are glad to share our ACL paper, \"Deep Learning for Mathematical Reasoning.\" Mathematical reasoning is a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numerical data and language. The development of machines capable of solving math problems and proving theorems has been a long-standing focus of AI and NLP. In recent years, there has been a surge of interest in this area, so our survey discusses the task of mathematical reasoning and the development of a deep learning method. For example, the text of math word problems can involve arithmetic operations with single or multiple operation steps. Mathematical reasoning isn\\'t limited to text-based data; it can extend to multimodal information like images, figures, and tables. There are two primary categories we could study: visual contexts, and tabular contexts. Solving geometric problems is an essential subject in high school education. As an example shown here, given the problem text and the corresponding diagram, we need to identify the geometric relations, apply theorem knowledge and perform calculations to obtain the numerical answer. So these tasks can be formalized as a neuro-symbolic reasoning problem over geometric diagrams, theorems, and solvers. Another important line of mathematical reasoning is automated theorem proving. A theorem prover aims to demonstrate the truth of a mathematical claim via a sequence of larger arguments. You know it is not trivial to write the proof for a theorem for us humans, but the automated prover helps us. Some datasets have been proposed to probe the human-level intelligence of language models, such as the Numeric Commonsense Knowledge and High-Level Problem Solving. In recent years, a number of neural network architectures has been proposed for mathematic reasoning tasks. For example, a sequence-to-sequencemodel uses an encoder to code architecture and usually formalizes the mathematical reasoning task as a sequence generation task. The basic idea is to map an input sequence, such as a math problem, to an output sequence, such as the equation, problem, or proof. It is noteworthy that mathematical expressions can be represented as tree-based structures. Therefore, sequence-to-tree models have been proposed to explicitly model the tree structure while encoding the equation expressions. In the last few years, we have witnessed the remarkable development of pre-trained language models, such as the large language models, or LLMs. These language models have demonstrated remarkable performance on a wide range of NLP tasks. We can also apply LLMs to solve math word problems. For example, given the input question here, we can prompt the LLM with one single example of the chain-of-thought process. A chain-of-thought is a series of intermediate reasoning steps that lead to the final output. It enables language models to solve complex problems by guiding them to produce a sequence of steps before giving the final answer. Despite the advantages, LLMs still face inherent limitations. One notable example is the lack of ability to perform precise mathematical reasoning. One effective solution to boost the LLMs’ performance is to replace the greedy decoding strategy with self-consistency. Instead of generating just one reasoning path, a diverse set of reasoning paths are sampled from the language model\\'s decoder, and the most frequent one is chosen from the answer set. Instead of designing effective prompting method for LMM a novel line of work is to design the two augmented LMMs. For example, program-aided LMMs are very helpful in complex mathematical reasoning tasks. We can augment LMMs with various tools to perform complex tasks giving different input queries. Chameleon, a recently proposed approach, can generate natural language programs to compose different tools for use. Despite the creation of various datasets, mathematical reasoning in low-resource settings remains underexplored. Recently, there have been attempts to build non-English datasets for Chinese, Korean, and Arabic. Additionally, pioneering research has developed mathematical reasoning benchmarks for financial, scientific, and medical domains. Despite impressive progress, the learning models commonly display generalization and robustness failures on reasoning tasks. First, language models struggle with large numbers. Second, large language models are inconsistent with mathematical reasoning. With that, thank you so much for your attention.'}\n",
      "{'id': '67', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/jmvCmyLxap.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/jmvCmyLxap.mp4', 'text': \"Hi. I'm Uri. Today I want to talk to you about interference in multilingual translation models. These models can benefit from synergy between different language pairs or suffer from interference. For example, training to translate English to Finnish may improve the quality of English to Estonian, while English to Chinese might have a negative effect. Many methods have been proposed to alleviate interference. However, they are often demonstrated using small models and do not always work better than a tuned baseline. So when does interference occur, and do we really need specialized algorithms to mitigate it? In this work, we identify the main factors that contribute to interference or synergy. We find that severe interference happens when the model is very small compared to the data size, and that tuning the sampling temperature is key for strong performance. For the simpler bilingual case, there are model and data size scaling laws to successfully predict a loss. But the multilingual case is more tricky, right? It also has other factors that can potentially impact performance, such as the data size of other languages, language similarity, and the total number of languages. These are a lot of factors. How do you take all of them into account? Luckily, we don't have to, as we find that both language similarity and the number of languages do not have a large impact. First, we look at the loss of a bilingual model trained to translate S to T and the S-T loss of a multilingual model, and define interference for S-T to be the relative difference between those. When the loss of the bilingual model is lower than the loss of the multilingual one, this value is negative. For our experiments, we used four variants of the Transformer architecture. The two middle ones are similar to the base and big configurations from the original paper. They are very often used in multilingual translation literature. We use 15 languages from WMT, ranging from over 50 million sentence pairs to about 150K. To find out whether language similarity has a significant impact on interference levels, we consider a focus language pair (S-T), and train trilingual models to translate S-T into an interfering target language O. For example, we use French and Russian as two of those interfering languages when the focus language is Spanish, and we measure interference for S-T across different O languages. In this example, Spanish and French are both Romance, while Russian is Slavic. So one can expect French to be more beneficial or introduce less interference to English-Spanish. Well, when using Spanish as a low-resource language with only 118,000 sentences, we can see some difference, but those largely vanish with a decent amount of English-Spanish data. So, we conclude that language similarity is not a dominant factor for interference. We invite you to take a peek at the paper for experiments on the effect of number of languages as well as the results with more languages sharing similar trends. Now we experiment across the model and focus data sizes when varying the total number of examples of interfering languages, and these are the results, where every dot represents a multilingual model. We see that severe interference occurs only for the smallest models and that the problem actually goes away with this in the amount of scale. When using one-quarter of the Spanish data, we see even less interference. So severe interference happens in parameter poverty settings. And again, check out the paper for more results. So what is the best way of controlling the trade-offs? The simplest solution is temperature sampling, when T greater than 1 allows to sample more training examples from lower-resource languages. The most common value used is five, often without calibration. We train multilingual models on all languages across sizes and temperatures and these are the results. The X-axis is the average interference for the low-resource language pairs and Y is for the high-resource ones. Temperature values from one to five are on the markers. Based on this, we can say that a baseline for battling interference is weak due to size in the small models, and weak due to uncalibrated temperature for a larger one that uses values that are too high. The lesson here is that tuned temperature is key for strong performance. To conclude, we find that model and data size affect the levels of interference in multilingual translation, while other factors like language similarity affect much less. And that modest scale and tuned temperature can reduce the problem significantly without any other specialized method. Thank you.\"}\n",
      "{'id': '68', 'prompt_en': 'Answer the following question concisely given the English content: What kind of linguistic context do models receive during pretraining?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welchen linguistischen Kontext erhalten die Modelle während des Pre-Trainings?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Che tipo di contesto linguistico viene messo a disposizione dei modelli durante il pre-addestramento?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在预训练期间，模型会接收什么样的语言上下文？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '69', 'prompt_en': 'Answer the following question concisely given the English content: How many clean validation samples are typically needed for good performance in WSL?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele saubere Validierungsbeispiele werden normalerweise für eine gute Leistung an der WSL benötigt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti campioni di convalida puliti sono in genere necessari per il raggiungimento di buone prestazioni in WSL?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在 WSL 中，通常需要多少个干净的验证样本才能获得良好的表现？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '70', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '71', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '72', 'prompt_en': 'Answer the following question concisely given the English content: Why is there a need to develop new methods for measuring media biases?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Warum ist es notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Perché è necessario sviluppare nuovi metodi per misurare i bias dell'informazione?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 为什么需要开发新的方法来衡量媒体偏见？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '73', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '74', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vmItCrZCsx.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vmItCrZCsx.mp4', 'text': 'Hello everyone. Today I will introduce our paper, \"Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths\". I\\'m Xiangqing, and here are the other two co-authors. Commonsense knowledge describes facts and related judgments in our everyday world, which is essential for machines when interacting with humans. ATOMIC is a large-scale commonsense knowledge base which covers event-centered social aspects of inferential knowledge tuples. With only B-to-A links, ATOMIC contains very few multi-hop paths, since an annotated tail event cannot become the head event of a triplet. Missing B-to-B, A-to-B, and A-to-A links cause unsatisfactory knowledge coverage, despite its high-quality human-annotated commonsense knowledge. We construct Dense-ATOMIC upon ATOMIC. By comparing them, we can see that Dense-ATOMIC completes many missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links DenseATOMIC also contains multi-hop paths, for example, a 2-hop path: X asks Y to marry, and then Y says yes, and then X smiles. Here is our process for DenseATOMIC construction. It mainly consists of three parts: normalizing tail events, training a relation prediction model, and constructing DenseATOMIC. Normalizing tail events converts tail events into the same equation as the head event. It consists of four parts: subject removal, third-person singular form conjugation, subject recovery, and relation grouping. Traditional methods for the completion of ATOMIC have two limitations: Firstly, sparse graph structure making it difficult for a GCN to propagate information. Secondly, unable to sufficiently utilize semantic information of events. To this end, we propose Rel-CSKGC, which predicts the relation given the head event and the tail event of a triplet. Given a head event \"X is forgiven\" and a tail event \"X smiles\", we first encode them with RoBERTa and then use the representation of the start token for linkable prediction. Meanwhile, we apply MaxPooling on the head and tail events and concatenate them for link prediction. This has two advantages. Firstly, utilizing no graph structure information, thus avoiding the problem caused by the sparsity. Secondly, taking advantage of semantic information by encoding both the head and tail events with the pre-trained language model. It\\'s computationally expensive to iterate over all pairs of head and tail events during the inference. To this end, we consider each base event and its annotated tail events as a cluster. We design an Intra- and Inter-Cluster Completion Strategy. Intra-cluster completion infers missing links inside a cluster, and the inter-cluster completion infers missing links between different clusters. Following original split of ATOMIC, we randomly sample negative triplets from the training split with negative sampling strategies. We combine sampled negative triplets and the training split to construct the training set for Rel-CSKGC. To test the performance of Rel-CSKGC, we construct a ground-truth subgraph by randomly sampling three clusters from the test split and annotating all pairs of head events and tail events with the most reasonable relation. Here is the statistics. We compare Rel-CSKGC versus relation prediction methods, and Rel-CSKGC outperforms relation prediction methods on both automatic and human evaluation. We also compare Rel-CSKGC with translation-based methods, and Rel-CSKGC also performs better. We also perform evaluation on the constructed DenseATOMIC. Firstly, we can see that DenseATOMIC yields higher knowledge coverage since DenseATOMIC, has more 1-hop, 2-hop, and 3-hop paths. Secondly, DenseATOMIC also benefits the performance of COMET. We can see that COMETours can generate more diversified results. We also perform the evaluation of multi-hop paths on DenseATOMIC. We can see that by randomly sampling from DenseATOMIC, the aggregates of multi-hop paths are relatively high, and with the heuristic rule, we can also achieve better results. Here are some randomly sampled paths of DenseATOMIC. For example, \"X misses Y\\'s opportunity,\" and then \"X goes home sadly\", and then \"X is melancholy\". In this paper, we construct a densely-connected commonsense knowledge graph, DenseATOMIC. And then we propose a new CSKG completion method for inferring the missing links on ATOMIC. We perform extensive evaluations that demonstrate DenseATOMIC\\'s advantage in knowledge coverage and multi-hop paths, and the potential for commonsense reasoning. Here is our code and our website. Thanks.'}\n",
      "{'id': '75', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/plLqMXJjMo.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/plLqMXJjMo.mp4', 'text': 'Hi. My name is Zheng Yandan. Today I\\'m very pleased to present our work, Jointprop. This is a joint work with my friend Hao Anran and my supervisor, Luu Anh Tuan. First, I am going to talk about the motivation of our work. Name entity recognition and relation extraction are two crucial tasks in information extraction. Supervised learning schemes have made significant progress in NER and RE research by leveraging rich label data. However, fully-supervised models require extensive labor to obtain high-quality data annotation, and it requires diverse annotated data for various domains and applications. Semi-supervised learning employs a small amount of label data to obtain powerful models at a lower cost. Semi-supervised NER and RE models have performed very well in recent years. However, current studies neglect the underlying interconnections between NER and RE tasks. This could be an issue. For example, one can easily see the syntactical similarity between generative model and probabilistic model and their relation indicator \"used to\" and \"use in\". If such similarities are ignored, the model might thereby miss label alignment as an entity that shares the same type as dependency parsing. Moreover, it is necessary to consider the interconnections among labeled data, among unlabeled data, and between labeled and unlabeled data. For example, it can be hard to invert the correct pseudo-label from dependency parsing to alignment or NLI alignment. This is also the case to infer \"apply\" for \"use to\" and \"use in\". If we fully exploit all the connections, we are able to fully integrate all the information to infer the correct labels. So we propose a joint semi-supervised learning framework to model the NER and RE tasks by propagating labels over heterogeneous graphs, and perform label propagation across the graph, and consider the inter- and intra-connections among both labeled data and unlabeled data. Here comes the method parts. Our jointprop framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and finally, model optimization. For span feature generation, we suppose X-k is the contextualized representation of K-th input token, which initializes span and span pairs representation as H-e and H-r. From the label tokens, we will obtain a trained classifier on the base of the base model. Using the trained classifier, we generate the unlabeled span and span pair representations. For heterogeneous graph construction, we construct a k Nearest Neighbor graph for computational efficiency, and we examine the similarity relations among pairs of unlabeled data as well as the similarity relations between the labeled data in order to take advantage of the smoothest constraints among neighbouring unlabelled data in our semi-supervised joint entity and relation extraction task. The entity nodes and the relation nodes are automatically associated via their representation. For the joint label propagation, a conceptual demonstration of the label propagation process is shown here. Through the heterogeneous graph, our proposed joint semi-supervised learning method propagates labels to entity or relation candidates in the unlabeled data. Alternatively, as shown in the figure, the pseudo-label for entities or relations will be refined every time (t) until converged. Label propagation diffuses labels through the whole graph along high-density areas formed by the unlabeled data. Finally, we come to the model optimization. We obtain the converged pseudo-label, and we use the softmax function followed by a standard argmax operation to determine the pseudo-labels. We filter those of lower quality with a confidence (g) and combine the rest of the confidence above the threshold with the label data to retrain the classification model. The retraining model remains the same as the baseline model, as does the joint NER-RE classification function. Finally, the experiment part. We conducted our experiments on four datasets. There are joint-task and single-task datasets. There is no previous baseline on semi-supervised joint-task. Comparisons are only made with base model performance. As we can see in the experiment tables, the joint learning of two tasks benefits from the codependency between the two tasks in joint datasets. For single-task datasets, our framework shows significant and consistent improvement over all baselines, both for NER and relation tasks. Thank you very much for your attention.'}\n",
      "{'id': '76', 'prompt_en': 'Answer the following question concisely given the English content: What does the political bias propagation pipeline look like?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie sieht die Pipeline für die Verbreitung politischer Vorurteile aus?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Che aspetto ha l'infrastruttura di propagazione dei bias politici?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 政治偏见传播流程是怎样的？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '77', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ENFYRpspKg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ENFYRpspKg.mp4', 'text': 'This video is for sharing our work, \"On Improving Summarization Factual Consistency from Natural Language Feedback\". This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and we offer further insights into the factual consistency of the summarization models. Our premise states that we propose three new NLG tasks and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction. The task we studied in this work is abstractive text summarization, and we specifically study the factual consistency of the summarization models. This quality requires that all of the information in the summary should be supported by the input document. The human demonstrations and feedback we collected in this work is based on the original system-generated summaries of the existing summarization models. We asked the annotators to provide labels to decide whether the summary is factually consistent, and we require them to provide human-corrected, factually consistent summaries if they think the original summary is not correct. Also, we require them to provide human feedback which contains the instructions, explanation, and evidence. More specifically, their explanation is to explain why the annotators think the summary is factually consistent or not. The instructions are for changing the original summary to make it factually consistent, and the evidence is one of the most relevant sentences in the source documents which will support the claim of the annotators. We collect the data on the XSum dataset, which is the most commonly studied dataset for summarization factual consistency. And the initial system outputs are collected from the pre-trained Pegasus model. In this slide, we show an example of the annotated data points. In this slide, we show the basic data statistics. We collect around 2.5K data points, and 70% of them contain factual errors. For the human-edited summaries, we show that they can receive higher automatic factuality scores compared with the initial system output. However, we also observe a lower textual overlap between the reference summaries and the human-edited summaries. We think the reason is that a majority of the reference summaries under XSum datasets already contain the factual errors. In this slide, we show the data distribution of the annotated editing instructions and their relation with the different error types. The first task we started is summary editing, where the model needs to follow the human feedback to edit the initial summary. We found that both the fine-tuned models and zero-shot large language models can effectively leverage the human feedback for this task. The second task we studied is feedback generation, where a critic model needs to generate the feedback that can be used by the editing model. We found that this remains a challenging task for both the fine-tuned models and the large language models. The third task is to automatically correct factual errors while generating the corresponding explanation. We found that the editor model can achieve comparable performance compared with the baseline models while being trained on much fewer data. And training the model to generate the explanation can help the model to achieve better performance. Apart from providing a test bed for the proposed NLG tasks, our dataset also has other advantages thanks to its fine-grained annotations, which can be valuable for training factuality metrics and factuality meta-evaluation. We have released our collected DeFacto dataset on GitHub and please check our paper for more details. Thank you for listening.'}\n",
      "{'id': '78', 'prompt_en': 'Answer the following question concisely given the English content: Does the simplification process differ for DEplain-apa and web?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Unterscheidet sich der Vereinfachungsprozess zwischen DEplain-apa und Web?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il processo di semplificazione differisce per DEplain-apa e web?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： DEplain-apa 和网站的简化过程是否有所不同？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '79', 'prompt_en': 'Answer the following question concisely given the English content: Is Coscript publicly available?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Ist Coscript öffentlich verfügbar?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Coscript è disponibile pubblicamente?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： Coscript 是否公开可用？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '80', 'prompt_en': 'Answer the following question concisely given the English content: How exactly is the watermark inserted into the text?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie genau wird das Wasserzeichen in den Text eingebettet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come viene inserita esattamente la filigrana nel testo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 水印是如何插入到文本中的？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '81', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '82', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rYnSfdzEJh.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rYnSfdzEJh.mp4', 'text': 'Hi everyone. This is a video about our work, which is titled \"Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring\". Automated Essay Scoring, or AES for short, aims to score the writing quality of essays without human intervention, which is an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora, comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written specific to new prompts, and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirements of groundtruth scores for training, and as such, has significant potential in both scientific research and practical applications. There are mainly two works tackling the unsupervised AES task. The first work is proposed by Chen and others in 2010, which uses a heuristic quality signal, the number of unique terms, as the initial score of each essay, and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work is proposed by Zhang and Litman in 2021, which uses a heuristic quality signal — word count — as a weak supervision to train a neural AES model. However, such direct regression process also leads to poor performance. The two works inspire us that, since a single quality signal cannot comprehensively describe the quality of essays, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for Unsupervised AES by Learning from Rank Aggregation, or ULRA for short. The core idea of our ULRA is to introduce multiple heuristic quality signals as a pseudo-groundtruth and then train a neural AES model by learning from the aggregation of these quality signals. Specifically, our ULRA contains a heuristic essay ranking module, or HER alpha-shot, which can generate partial order pairs by ranking essays according to heuristic quality signals As illustrated in the figure, the HER module contains three components: quality signals, essay ranking, and partial-order pairs generation. Among them, multiple classic quality signals are introduced to describe the quality of essays from different aspects. Each quality signal can then be used to rank essays according to signal values and generate a rank list. Finally, each rank list can be transformed into many partial-order pairs for later model training. Next, our ULRA contains a Deep Pairwise Rank Aggregation Module, or DPRA for short, which trains a neural AES model by aggregating the partial-order pairs derived from multiple quality signals into a unified supervision. This module mainly deals with how to address the inconsistent partial-order supervision from multiple quality signals so that the neural AES model can learn how to judge the partial-order relationship of essay quality. To address this problem, we designed a Deep Pairwise Rank Aggregation loss, which set a learnable confidence weight for each signal to measure the importance of each signal. Finally, in the model inference stage, considering that the essay scores predicted by the neural AES model may have a different range from the pre-defined score set, we propose a Scoring Strategy to transform the predicted scores given by the neural AES model into the range of the pre-defined score set through a minimum-maximum transformation. We conduct experiments on both transductive and inductive settings, which demonstrates that our ULRA outperforms all unsupervised baselines with a large improvement. Compared with the cross-prompt and one-shot methods, ULRA achieves competitive performance. By observing the general supervised methods, the performance of ULRA is still much lower than theirs due to the lack of strong supervision. To sum up, in this paper, we aim to perform essay scoring under the unsupervised setting. To this end, we proposed a novel ULRA framework to train a neural AES model by aggregating the partial-order knowledge contained in multiple heuristic quality signals. To address the conflicts among different signals and get a unified supervision, we designed a deep pairwise rank aggregation loss for model training. Experimental results demonstrate the effectiveness of ULRA for unsupervised essay scoring. Thanks.'}\n",
      "{'id': '83', 'prompt_en': 'Answer the following question concisely given the English content: Can encoder-decoder models such as mt5 improve by training on a mixture of languages?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Können Encoder-Decoder-Modelle wie mt5 durch Training mit einer Mischung von Sprachen verbessert werden?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: I modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进吗？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '84', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QXemDvEGOt.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QXemDvEGOt.mp4', 'text': 'Hello everyone. I\\'m Shwai He. Today I\\'m going to talk about my paper for ACL 2023, \"PAD-Net: An Efficient Framework for Dynamic Networks\". First of all, I want to talk about the background knowledge about dynamic networks. Most of the traditional networks are static networks given the input value. The network computes it with the fixed or static parameters, which cannot change with the input. However, for dynamic networks, they can change the architecture or parameters based on the input. These are two examples. Mixture of Experts can select specific sub-networks based on the input, Dynamic Convolution linearly combines π layer convolution kernels as the linear weight is based on the input. The implementation of dynamic networks is why we use it. Just replace the static layers with dynamic ones, and we can see that dynamic is always better than static, and the more dynamic, the better. However, I want to see the existing dynamic networks are fully dynamic networks because all parameters are dynamic. And this matter contributes to excessive use of parameters. For example, if we replace the feed-forward layers of BERT-Base with the eight Mixture of Experts, we can see the model size is about 5 times the original size. This problem is unacceptable in many situations and limits the use of dynamic networks. Towards this, we have two questions: whether there exist redundant dynamic parameters in fully dynamic networks, and whether the coexistence of static and dynamic parameters perform better. We have our own hypothesis. We guess that a fully dynamic network contains partially dynamic sub-networks that maintain or exceed the representation power of the original network. Based on [INAUDIBLE 2:40] With this hypothesis, we build our own framework, PAD-NET: Partially Dynamic Network. We partition the parameters into dynamic parameters and static parameters, and set up two scale factors to describe the intensity of the two modes, [INAUDIBLE 3:00] state the constraint to speed up the training process. The method we use to partition the two modes is Iterative Mode Partition. Our goal is very easy, just making the redundant dynamic parameters static, as the redundant dynamic parameters is a massive effect on the loss value. For example, if we transform a parameter into a static one and we cannot see a big change in the loss value, we can see that this parameter is unimportant and can safely transform it to a static one. Based on the experiment, we can see our PAD-Net achieved much better performance than the static network and dynamic networks And compared to a fully dynamic network, PAD-Net can maintain much fewer parameters and less computation. We also conduct some ablation studies to find out the optimal Dynamic Ratios for Dynamic Convolution and Mixture of Experts. Also, we found the Scale Factors for dynamic parameters and static parameters is very important, as the constraint on the submission of the two scale factors is also very important in the accuracy of the different dynamic networks. We also compare our methods with network pruning and find our performance is significantly better because we maintain the static parameters. And we also find PAD-Net can make the output more discriminating, which contributes to the better performance compared to the fully dynamic networks. We think these are some future works that deserve to be explored, such as extending our methods to other mainstream networks. Also, try to extend our method to some hardware friendly structured manners. And, last but not least, further introduce more modes such as the combination among zero elements, static parameters, and dynamic parameters. And that\\'s all for [INAUDIBLE 6:06].'}\n",
      "{'id': '85', 'prompt_en': 'Answer the following question concisely given the English content: What is an example of constrained language planning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist ein Beispiel für eingeschränkte Sprachplanung?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è un esempio di pianificazione linguistica vincolata?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 受限语言规划的一个示例是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '86', 'prompt_en': 'Answer the following question concisely given the English content: How do they make sure of the covertness of their method?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie stellen sie die Opazität ihrer Methode sicher?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo gli autori si accertano della segretezza del loro metodo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 他们如何确保其方法的隐蔽性？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '87', 'prompt_en': 'Answer the following question concisely given the English content: How does the work use existing PLMs to build a new one?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie nutzt die Arbeit bestehende PLMs, um ein neues PLM aufzubauen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo il lavoro utilizza i PLM esistenti per costruirne uno nuovo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 研究如何使用现有的 PLM 来构建新的 PLM？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '88', 'prompt_en': 'Answer the following question concisely given the English content: Which country is GPT-4 the least aligned with?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Auf welches Land ist GPT-4 am wenigsten ausgerichtet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Con quale Paese GPT-4 è meno allineato?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： GPT-4 与哪个国家/地区的立场最不一致？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '89', 'prompt_en': 'Answer the following question concisely given the English content: On which example sentence does the speaker show how the model leverages knowledge learned though the attention mechanism?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Beispielsatz zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In quale frase di esempio la relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '90', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/VpEwDdXfNG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/VpEwDdXfNG.mp4', 'text': 'Hello. I\\'m Haneul Yoo, one of the authors of \"Rethinking Annotation: Can Language Learners Contribute?\". As language models advance, data annotation gets essential. In NLP, it has been customary to recruit native speakers of the target language. Even though it is difficult to recruit native speakers for many languages, we have lots of language learners. For instance, there are no monolingual native speakers, and only 73,000 daily-using L1 speakers in Irish, but more than 1.2 million learners exist. In this paper, we question whether we really need native speakers for data annotation and conducted a proof-of-concept study to examine the feasibility of using language learners as annotators. We carefully designed experiments considering the following control variables. First, we targeted three languages: English, Korean, and Indonesian, considering the amount of available resources and the language difficulty to learn. We chose four tasks from each common task type in the GLUE benchmark: Sentiment analysis for a single sentence classification, NLI for sentence pair classification, and NER for sequence tagging and MRC for span prediction. We adopted and revised the CFR criteria to categorize learners into three levels: basic, intermediate, and advanced. And for a fair comparison, we recruited native speakers as well and conducted the same experiments. We randomly sampled 120 annotation samples from existing datasets and categorized them into five groups based on their difficulty level. Finally, we divided language learners into two groups with the additional resources we provided. We suppose learners might consult dictionaries or machine-translation systems to fully understand annotation samples and aim to compare annotation accuracy and learning effect concerning the types of additional resources that they used. This figure shows a high-level flow chart of our experiments. First, we conducted a preliminary survey to ask participants about their self-rated language proficiency, language background, and language learning experiences. One session of the main experiments consists of three steps: pre-test, annotation, and post-test. Participants were asked to solve 15 test questions with standardized test questions from official language tests and word meaning questions asking about the meaning of the words to check their language proficiency level. After taking a pre-test, participants annotated 10 questions with the help of additional resources assigned. After completing the annotation, participants were asked to solve the same 15 questions again to investigate whether the data annotation leads to any learning effects. The main experiments consist of a series of multiple sessions over six days. Finally, participants completed a post-survey asking about their thoughts on annotation and self-rated language proficiency again. Through the experiments, we show that labels annotated by language learners are nearly accurate, especially for simpler tasks and easy-to-medium level questions. Moreover, language learners are almost on par with native speakers if their labels are aggregated with others by majority voting. Finally, we prove language learners capable in the case of NLP data annotation by showing training simulations with learners\\' annotations. Language models trained on learners\\' less accurate labels achieved about 95% of ground truth performance and sometimes outperform the model trained with native speakers\\' labels. Unlike the conventional ways of building data in low- to mid-resource languages, which are basically translating existing datasets, this paper suggests a novel way for data construction by recruiting language learners as annotators. Furthermore, we observe that learners\\' language proficiency and vocabulary and grammar tend to improve as they carry out the annotation tasks, by comparing the scores of the pre-test and the post-test of a single session and the pre-test of the first and the last session. In conclusion, this paper questions the necessity of recruiting native speakers for data annotation and shows that language learners could definitely contribute NLP annotations. We believe this work showed the possibility of broadening NLP research for many languages, jumping over geographic and technological barriers to building benchmark datasets for low-resource languages where it is hard to recruit native speakers. Thank you for listening. Please refer to our paper for more details, such as how control variables affect annotation performances. Any questions are welcome via the email below.'}\n",
      "{'id': '91', 'prompt_en': 'Answer the following question concisely given the English content: How does the amount of tasks impact the model performance?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wirkt sich die Anzahl der Aufgaben auf die Leistung des Modells aus?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo la quantità di attività influisce sulla performance del modello?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 任务的数量如何影响模型的性能？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '92', 'prompt_en': 'Answer the following question concisely given the English content: Name three treeless baselines that the authors compare their method with.', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Nennen Sie drei baumlose Baselines, mit denen die Autoren ihre Methode vergleichen.', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Elenca tre approcci di riferimento con cui gli autori confrontano il loro metodo.', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 请说出作者用来比较其方法的三个无树基线。', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '93', 'prompt_en': 'Answer the following question concisely given the English content: In what relation are the two co-authors with the first author?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: In welcher Beziehung stehen die beiden Co-Autoren zum ersten Autor?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che rapporto si trovano i due coautori con il primo autore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 两位合著者与第一作者有什么关系？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '94', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '95', 'prompt_en': 'Answer the following question concisely given the English content: Who is the first author of PaLM?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wer ist der erste Autor von PaLM?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Chi è il primo autore di PaLM?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： PaLM 的第一作者是谁？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '97', 'prompt_en': 'Answer the following question concisely given the English content: How many problems of SimulST does the speaker mention?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Auf wie viele Probleme von SimulST geht die Referentin ein?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti problemi associati a SimulST menziona la relatrice?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者提到了 SimulST 的几个问题？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '98', 'prompt_en': 'Answer the following question concisely given the English content: What is an effective way to mitigate social and political biases in datasets when training NLP models?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie können soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv reduziert werden?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale può essere un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '100', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/nemCivJybo.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/nemCivJybo.mp4', 'text': 'Hello. Multi-hop QA is about answering questions that require multiple reasoning jumps to answer. Each jump typically corresponds to a document in the corpus. For example, to answer this question, \"What 1988 Christmas comedy film did Brian Doyle-Murray star in?\", we first need to find all the movies that Brian Doyle-Murray starred in and find the movie that was released in 1988. We\\'re going to be referring to this set of documents required to answer the question as the \"chain.\" Multi-hop retrievers are trained by maximizing probability of the ground-truth chains given questions. So for a set of training examples {(q₁, c₁), (q₂, c₂)...etc.}, where q is a question, and c is the goal chain, retrievers are trained by maximizing the probability of cᵢ given qᵢ. And most state-of-the-art multi-hop retrievers fall under this paradigm. Existing systems require thousands of examples of questions and ground-truth chains for good performance. This can be expensive, especially for low-resource domains and domains that require special expertise. Our approach, PromptRank, is data-efficient. It gives good performance with as few as 128 examples and therefore addresses this issue. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranker. There are two main steps: Retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. And rerank these candidates using the few-shot language model reranker. Two points to consider here: what scoring function should we use, and how do we prompt the language model to extract this score? We use the likelihood of the question given the chain according to a language model. So given the language model, we have a chain prompt that I will explain how we construct in a few slides. And given the question, we score C as the probability of the question given the chain prompt. Let\\'s go through a working example. Given this question, in an underlying corpus, we retrieve initial documents using TF-IDF. Then we expand and prune chains by following hyperlinks. Then we convert each of the non-pruned chains to prompts. Then we score each chain by the probability of the question given the chain prompt. How do we construct the chain prompt? Given this question and this chain, what we do is we have a prompt that looks like this, where we insert the chain documents into the prompts, and we have an indicator token to designate that this is a document. And we have an instruction, which in our case here is something like \"Read the previous documents and ask a question.\" And the instruction serves to elicit the language model\\'s reasoning ability over the chain documents. We explore additional techniques, like instruction search to find optimal instructions, and instruction sampling, where we compute chain scores by aggregating multiple scores computed with different instructions, and also temperature scaling, where the language model logits are scaled by some constant temperature. We experiment with GPT2-XL and T5-XL, and we evaluate our approach on HotpotQA. And as for metrics, we use [INAUDIBLE 4:02] R@K recall at K and answer recall AR@K. As for instruction search, we generate 200 diverse instructions and evaluate each on a set of 128 examples. All PromptRank experiments use only 128 examples in total. So let\\'s start with the retrieval results. We see that PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. We also did ablation to verify the importance of each component we propose, and we find that each component definitely plays a role in the performance of the final performance of PromptRank. We also evaluate the downstream QA performance when using PromptRank as the retriever, and so we use a reader model, which is ELECTRA-Large, and we compile it with PromptRank. And we see that PromptRank exhibits very good downstream multi-hop QA performance, underperforming MDR by only around four exact match points. Check out our paper for more results and extensive analysis. To summarize, language models can be used for few-shot ranking of candidate paths for multi-hop QA. PromptRank exhibits strong few-shot path retrieval performance compared to fully supervised systems. The likelihood of the question given the chain works significantly better as a scoring function than the reverse. The instruction plays a strong role in eliciting language models’ reasoning abilities over the chain documents. With that, I conclude my talk and thank you so much for listening.'}\n",
      "{'id': '101', 'prompt_en': 'Answer the following question concisely given the English content: How good is the fluency of PaLM?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie gut ist die Sprachgewandtheit von PaLM?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanto è buona la fluidità di PaLM?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： PaLM 的流畅度如何？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '102', 'prompt_en': 'Answer the following question concisely given the English content: What are the important properties of a watermarking method?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind die wichtigsten Eigenschaften eines Wasserzeichenverfahrens?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le proprietà importanti di un metodo di filigrana?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 水印方法的重要属性是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '103', 'prompt_en': 'Answer the following question concisely given the English content: Which are the 14 different languages into which the English TED talks have been translated?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: In welche 14 Sprachen wurden die englischen TED Talks übersetzt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： TED 英语演讲已被翻译成哪 14 种不同的语言？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '104', 'prompt_en': 'Answer the following question concisely given the English content: How many instances are sampled from one dataset for reannotating?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Instanzen werden aus einem Datensatz für die erneute Annotierung extrahiert?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quante istanze vengono campionate da un set di dati per la riannotazione?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 从一个数据集中抽取多少个实例用于重新注释？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '105', 'prompt_en': 'Answer the following question concisely given the English content: Which distance metrics are used for measuring the difference between benign and backdoor datasets?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Distanzmetriken werden verwendet, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali metriche di distanza vengono utilizzate per misurare la differenza tra set di dati benigni e backdoor?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些距离度量用于衡量良性和后门数据集之间的差异？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '106', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/oSdmnPEPkf.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/oSdmnPEPkf.mp4', 'text': 'Hello. My name is Chaitanya, and I\\'m going to be talking about our paper called QUEST. This is work done in collaboration with Pete, Ming-Wei, Kenton, and Kristina from Google DeepMind. To motivate this work, let\\'s consider the example of Jane, who is a zoologist on a field trip in Costa Rica, and she observes a species of reptile that is unknown to her. In our second example, let\\'s consider Austin, who is an avid book reader who just finished a book and is looking for his next read. Now Jane would like to find the name of the species that she encountered on her field trip by describing it, based on her recollection, as a red reptile, not more than 12 inches long, found in Costa Rica. Similarly, Austin expresses his preferences for finding historical fiction novels set in France. These examples showcase the fact that people often express their information needs with multiple constraints or preferences. Such information needs naturally give rise to queries that contain implicit set constraints. From the previous examples, we see that Jane\\'s constraints on her query involve a complement and an intersection of three sets, whereas Austin\\'s preferences involve an intersection of the two sets of novels: historical fiction novels and novels set in France, both of the sets that he\\'s interested in. To operationalize this problem and study the effectiveness of systems for handling such selective information needs, we present a dataset called QUEST. QUEST is a retrieval dataset that includes more than 3,000 entity-seeking queries where queries contain implicit set operations, the answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. In our work, we show that the dataset poses a challenging retrieval problem since systems need to effectively search over a large document corpus to find multi-answer sets where the attribution for different query constraints can come from different parts of the document. To construct QUEST, we rely on Wikipedia category names from four domains of interest: films, books, plants, and animals. We then perform set operations over these atomic categories to get queries with set constraints. We then ask human annotators to paraphrase templatic queries, where they ensure that the paraphrase queries have the same meaning and are fluent. Another set of annotators then validates these queries for fluency and naturalness, which we use to filter the set of queries. Finally, we ask annotators to verify the relevance of entities in the answer set, and also mark evidence in the document as its attribution. For example, for this paraphrase query, \"historical fiction novels set in France\", annotators first marked the span of text that indicates relevance for the constraint \"historical fiction novels\", and then marked the span of text relevant for \"set in France\". They would then mark the document as containing complete evidence and definitely relevant to the query. To evaluate systems on our dataset, we require systems to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document\\'s relevance can come from multiple parts of the document. To set up baselines for the dataset, we consider sparse and dense retrievers as well as a T5-based reranker that takes in the top 100 candidates from the retriever. First, we show that there is a large room for improvement on retriever performance based on the recall of the complete answer set, indicated here by the MRecall@100 scores. The end-to-end system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Finally, through our analysis, we find that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores. We hope that, along with Jane and Austin, QUEST can help future researchers build improved systems for their information-seeking scenarios with selective information needs. Thank you for watching. Please read our paper, and hope you can come to our presentation at ACL. Thanks a lot.'}\n",
      "{'id': '107', 'prompt_en': 'Answer the following question concisely given the English content: How were the multilingual encoder-based models used for this task?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurden Modelle, die auf einem mehrsprachigen Encoder basieren, in dieser Aufgabe eingesetzt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come sono stati utilizzati i modelli basati su codificatori multilingue per questa attività?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何将基于编码器的多语言模型用于这项任务？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '108', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '109', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/lhnhdPEQVw.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/lhnhdPEQVw.mp4', 'text': 'Hi. I\\'m Or, and I will present \"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor\". Instruction tuning enables pre-trained language models to generalize to unseen tasks in a zero-shot setting. And one way to obtain examples for instruction tuning is to reformulate existing NLP datasets. However, the resulting data is limited to existing academic benchmarks only, while instructions can actually be used to describe any textual task. An alternative is to collect user-generated prompts and manually annotate their expected outputs. And this was done in the InstructGPT paper and would lead to an arguably more desirable distribution of the instruction space. However, it would also require a live application with existing users and a major annotation effort. In this work, we ask whether we can create a large dataset of instructions that is diverse in tasks, content, and phrasing without any human labor. So we introduce Unnatural Instructions, which is a dataset of natural language instructions and their corresponding inputs and outputs. The data was collected in a fully automatic manner and without any human annotations. So to collect instructions, we prompt a pre-trained language model — here specifically a variant of GPT-3 — with three examples taken from the Super-Natural Instructions dataset. And we ask the model to generate a fourth example. And you can see our data generation prompt, here in the image. So at the first step, we prompt the model to generate an instruction and a corresponding input. And at the second step, we take the generated instruction and input, and we ask the model to generate a corresponding output. We further diversify the dataset\\'s format by generating additional paraphrases of each instruction. So again, this is done in a completely automatic manner. We prompt the model with two examples of instructions and the alternative formulation. Then we provide the model with an additional instruction, and the model generates an alternative formulation of the instruction. The resulting dataset contains 64,000 examples, and if we consider the instruction paraphrases, we have about 240,000 examples. We analyze the generated examples, focusing on creativity, diversity, and correctness. As for correctness, we find that more than 50% of the generated examples are indeed correct, and even incorrect examples often contain valuable information for instruction tuning. In terms of creativity and diversity, Unnatural Instructions contains highly creative tasks, some of which are very different from the classic NLP tasks. And you can see here two examples: we show here only the instructions without the inputs and outputs. In the first example, the task is to verify that a given scientific experiment is well-designed. In the second example, the task is to invent a new word. To measure the utility of the generated data, we fine-tune an 11 billion-parameter T5 model on Unnatural Instructions. We show that the model can outperform both T0++ and Tk-instruct across several benchmarks. On top of that, when the cost of generating examples is amortized, training on Unnatural Instructions outperforms our baseline on all benchmarks. So our baseline is an 11 billion-parameter T5 model identical to the one trained on Unnatural Instructions, only we train it on Super-Natural Instructions. And the tested benchmarks were Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry. To conclude, we introduce Unnatural Instructions, which is a dataset of instructions for a wide variety of natural language tasks. Unnatural Instructions was collected in a completely automatic process requiring only a small seed of manually constructed examples. Unnatural Instructions highlights the ability of language models to produce creative and diverse data. This is difficult to obtain with crowd workers, who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are also faster and cheaper than human annotations. Thank you.'}\n",
      "{'id': '110', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '111', 'prompt_en': 'Answer the following question concisely given the English content: How do the authors decide what moderate-frequency words are?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie entscheiden die Autoren, was Wörter mit mittlerer Häufigkeit sind?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo gli autori decidono quali sono le parole a frequenza moderata?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者如何确定中等频率的单词？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '113', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '114', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ypuoCwFUMF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ypuoCwFUMF.mp4', 'text': 'Hi everyone. I\\'m going to introduce our work on ACL 2023 called \"Finding the Pillars of Strength for Multi-Head Attention\". So we are from Nanyang Technological University of Singapore. To begin with, as we all know, the large language models are game-changing. From the task-specific models for each field of natural language processing, now the large language models can learn all tasks in one model, and this is revolutionary. However, several limitations are identified. For example, the heavy parameters. They usually contain billions of parameters, which is not deployable on small clusters. They usually require long training time. For example, the LLaMA-65 takes one million GPU hours even with our best GPUs. And they are token-hungry; they require huge corpus. For example the LLaMA-65 is trained on 4.5TB corpus. Today we\\'re going to focus on the heavy parameter problem of the larger language models. So as we all know, the multi-head attention are designed to attend to different subspaces of the input, and each head attends to a unique, but different input subspace. But as we all know that some heads can be pruned without sacrificing any performance. As shown on the right chart, we can even prune 40% of parameters without losing the performance. So for the multi-head attention redundancy optimization, there have been several threads of works. The first thread is homogenization-based, which aims to make the heads become more similar. However, they sacrifice performance. For the second thread of work are diversification-based; they intend to make the attention heads become more diversified. However, they are not parameter-efficient because they don\\'t have any model compression. So the third work is focused on giving some significant scores for each head and prune those with low scores. However, this thread of work remain considerable redundancy. So we proposed a grouped head attention which uses a divide and conquer strategy to compress multi-head attention. It contains two strategies. The first stage of our model is the group-constrained training. It aims to divide the attention heads into several groups, which makes intra-group heads become more similar and inter-group heads become more separate. So this is the overall pipeline, and this is the original multi-head attention. This is a feature map extracted from the multi-head attention, and we usually use an unsupervised hidden unit discovery system — for example, the K-means — to supervise the projected feature map. So using the objectives on the left, we have two conditions. When Z is categorical with the first condition, when Z is a vector with the second condition. So for both conditions of loss functions, we have two terms. The first term is a homogenization term, which aims to become to make intra-group heads become more similar. The second term is a diversification term which aims to make inter-group heads become more separate, For our second stage, it is called Voting-to-Stay algorithm. It aims to prune the redundant multi-head attention and remain only one head for each group. It is performed after the group constrained training. So in Step 1, we collect voters on the whole training set, each batch as a voter. And in Step 2, the heads will receive votes based on their score given by the evaluator. In Step 3, we will prune the heads with low votes. So as we can see, there will only remain one head for each group and achieve significant parameter compression. In extreme conditions, we can compress 90% of parameters which will be last rated later We can evaluate on three tasks: the machine translation, language modeling, and abstractive summarization. In our two models — namely the GHT and GHT-PS, which are from the group constraint training — and from both the group constraint training and voting-to-stay algorithm, they perform well on the machine translation task. The GHT and the GHT-PS has achieved 3.8%, and 4.4% BLEU improvement over the SOTA baselines and because the GHT-PS has been pruned before, it compresses 32.1% parameters with comparable performance, which is very considerable. For the abstract summarization task, it achieves 6.7% and 7% improvements, respectively, and 32.1% compression. On the language modeling task, it achieves 2.8% and 2.9% performance improvement and 16.9% model compression. We also conduct some further efficiency analysis, and we can see that our LITE model achieves 90% of pruned parameters, 62% faster inference speed, and 80% for FLOPs against the model which yield the same performance on the same data set. In the future, we identify that the task-specific automatic pruning is very promising direction. So, given the Lottery Ticket Hypothesis, we know that networks contain subnetworks that reach test accuracy comparable to the original network, which means that we believe that we can prune the networks without sacrificing that performance. So this gives us the confidence to prune the redundant large language models. So, the large language models nowadays are redundant in real scenarios because they usually are able to perform all or nearly all tasks. However, we only need a few tasks in real applications. For example, when we are doing machine translation, we don\\'t need the ability to perform image caption, so the related parameters can be pruned. And we believe this pruning will not sacrifice the performance, like when we uninstall apps on our iPhone. And just imagine that if we don\\'t uninstall the unused apps on our iPhone, they will be too heavy to use. So this is all for today\\'s video. If you want to know more about our work, please do not hesitate to attend to our poster session. Thank you.'}\n",
      "{'id': '115', 'prompt_en': 'Answer the following question concisely given the English content: What speech segment size does the approach use?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Sprachsegmentgröße wird bei dem Ansatz verwendet?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Che dimensione del segmento parlato utilizza l'approccio?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该方法使用的语音片段大小是多少？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '116', 'prompt_en': 'Answer the following question concisely given the English content: In the example with Servin and Kea, what entity-specific knowledge is needed?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches entitätsspezifische Wissen wird im Beispiel mit Servin und Kea benötigt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Nell'esempio con Servin e Kea, quali conoscenze specifiche dell'entità sono necessarie?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些特定于实体的知识？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '117', 'prompt_en': 'Answer the following question concisely given the English content: What is the most important factor between the example quality and the similarity to the source sentence?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist der wichtigste Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 示例质量和与源句子的相似度相比，哪个因素更为重要？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '118', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/blJSEbNQDN.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/blJSEbNQDN.mp4', 'text': 'Hello everyone. We\\'ll be presenting our ACL 2023 submission which is \"Improving Pretraining Techniques for Code-Switched NLP\". So first, we define what code-switching is. Here we have an example: Laptop, mere, bag, me, rakha, hai. This is a code mix sentence of English and Hindi, so some of the words are English and some of the words are Hindi. This is a pretty common occurrence in linguistically diverse communities like India, so building computational models for code-switching is very important for this case. Multilingual pre-trained models like mBERT and XLM-R don\\'t perform that well on code-switched tasks like question answering and sentiment analysis. So our main contributions in this work are we proposed novel MLM techniques, which are tuned to the case of code-switching, and we motivate architectural changes and auxiliary loss, which are again tuned to the case of code-switching. So, we propose SwitchMLM. To define what SwitchMLM is, we first define what a switch-point is. Switch-point refers to a group of two tokens which are a transition in languages. So here for example, we have a transition from English to Hindi, Hindi to English, and back from English to Hindi. So here we have four words which are switch-points, and in this variant of MLM only these words are maskable. In standard MLM, all the words are maskable with uniform probability. This is not the case here. But the limitation here is that it requires access to LID tagged dataset or a LID tagger for code-switched sentences, which is not always available. So we offer a surrogate method called FrequencyMLM. Here we define the negative log likelihood of the word in each monolingual corpora, and then compare the negative log likelihood to assign the LID tags. We also propose some architectural modifications to help with code-switching. So, first of all, we propose some residual connections. With layer probing techniques, we found that certain intermediate layers of BERT encode more switch-point information than the final layer. To utilize this, we can just put some residual connections from that intermediate layer to the final layer and increase the amount of switch-point information in the final layer. We further encourage this intermediate layer to encode language information by imposing an auxiliary LID-based loss here. So, we force this layer to learn more LID information. So here, MLP(xᵢ) is the probability with which the MLP assigns xᵢ to be either a switch-point or not a switch-point, where xᵢ is the input token. So here are the results, and as you can see, on the sentiment analysis task, our combined method — which is the Switch or FrequencyMLM (whichever applies) combined with ResBERT (which are the residual connections) and an auxiliary loss — performs the best on all the language pairs of sentiment analysis. Next we\\'ll come to the set of probing experiments. We use probing classifiers to verify our claim about the switch-point information. We claim that our proposed methods increase the amount of switch-point information in the intermediate and final layers. So to verify this claim, we use two methods: linear probing and conditional probing. The linear probe is just a simple feedforward network that takes in layer-wise representations as its input and is trained to predict switch-points. So switch-points are just sequences of zeros and ones. And here we have conditional probing. We need conditional probing because linear probing cannot detect when representations are more predictive of switch-point information in comparison to a baseline. So here Perf (performance) can be anything: we take it to be the soft timing distance. ƒ is a linear probe or a simple feedforward network. 𝘉 is the baseline. Zero is just array of zeros. Phi is our model. X is the input token sequence. So these are the results for our probing experiments. Here we can see StandardMLM combined with zeros and StandardMLM combined with SwitchMLM. So StandardMLM representations combined with SwitchMLM representations has more switch-point information compared to just StandardMLM. So, this is expected and what we claim. Here we have some linear probing results. Here we show StandardMLM layer 9 has more switch point information than StandardMLM layer 12. So it might be a good idea to add a residual connection from layer 9 to layer 12. This is what we do, and we see it indeed increases the amount of switch-point information in the final representation. So in summary, we propose a new MLM objective which is tuned to handle code-switch information. We hypothesize and verify using probing classifiers that our methods increase the amount of switch-point information present in the intermediate layers. And with this result in mind, we motivate some architectural changes and add an auxiliary loss to further enhance this switch-point information content. Thank you.'}\n",
      "{'id': '119', 'prompt_en': 'Answer the following question concisely given the English content: Which language models does the paper focus on in the extended experiments?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Auf welche Sprachmodelle konzentrieren sich die Arbeiten in den erweiterten Experimenten?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su quali modelli linguistici si concentra l'articolo negli esperimenti estesi?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在扩展实验中，论文侧重于哪些语言模型？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '120', 'prompt_en': 'Answer the following question concisely given the English content: Does the model use attention scores from a specific layer or combine the scores from several layers?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Verwendet das Modell Aufmerksamkeitswerte aus einer bestimmten Ebene oder kombiniert es Werte aus mehreren Ebenen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il modello utilizza i punteggi di attenzione di un livello specifico o combina i punteggi di più livelli?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '121', 'prompt_en': 'Answer the following question concisely given the English content: What are the examples of direct inference?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind Beispiele für direkte Inferenz?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono gli esempi di inferenza diretta?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 直接推断的示例有哪些？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '122', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '123', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '124', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/jAwoZwQYyv.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/jAwoZwQYyv.mp4', 'text': 'Hi everyone. This is Tan Qingyu from the National University of Singapore and Alibaba. I\\'m glad to share our work, \"Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models\". Time is a fundamental axis in the real world. We first breakdown temporal reasoning into three different levels. The first level is time-to-time reasoning, such as \"What is the year after 2010?\" Answering this question will only need the understanding of the time axis. The second level is time-to-event reasoning. In this work, we include long-duration facts as events as well. An example question is \"What team did Lionel Messi play for in 2010?\" This question not only requires understanding of time, but also needs the grounding of events to time. The third level is event-to-event reasoning, such as \"What team did Lionel Messi play for after FC Barcelona?\" This type of reasoning requires time grounding of multiple events. We found that prior works on temporal studies overemphasized on the L2 reasoning type, whereas we intend to study temporal reasoning in a more comprehensive manner. We first conduct a preliminary experiment on L1 prediction of a year. The question templates are shown in Table 1. This is essentially an integer calculation task. We evaluated three LMs, T5-L fine-tuned on Natural Questions, the instruction-tuned FLAN-T5-L, and ChatGPT. We found the first two LMs demonstrated a strong bias in favor of the 2000 to 2020 time period, which could be correlated to the term frequencies in the pre-training corpora. We can see that ChatGPT is close to solving the year prediction problem, but later we do find that its performance deteriorates significantly with performing month prediction. Given such findings, we aim to study the temporal reasoning in a comprehensive manner. We propose the TempReason dataset, which covers all three levels of reasoning and long temporal coverage. For L1 questions, we increase the difficulty from year prediction to month prediction. For L2 and L3, we constructed the question and answer pairs by Wikidata Knowledge base and Wikipedia articles. Our dataset statistics are shown in Table 3. We evaluate temporal reasoning in three QA problem settings. The first is Closed Book QA, where only the question will be prompted to LMs. The second is Open Book QA, where a Wikipedia article of the subject — in this case, Leo Messi — will be provided as context. Last but not least, we propose a new setting to study temporal reasoning and name that \"Reasoning QA\". In this setting, all the relevant temporal knowledge will be provided to the LMs, and they are required to reason based on the questions and temporal knowledge. For example, in this question, all teams Leo Messi played for and their corresponding time periods will be provided to LMs, and they need to find the correct answer based on the given information. In order to improve the temporal reasoning capability of LMs, we also propose a training strategy with two nominal components. The first is Temporal span extraction pre-training, which is an intermediate pre-training strategy to reconstruct mask, temporal, and entity spans in a raw text. The second component is time-sensitive reinforcement learning, where we reward the model for correct predictions and give a special penalty to temporally wrong predictions. As shown in the example, the correct answer here is FC Barcelona, highlighted green, and temporally wrong answers are highlighted in yellow. We denote our final model as TempT5. We show the experiment results on TempReason in Table 4. We compared FLAN-T5-L, ChatGPT, T5-base fine-tuned on task data, T5-SFT, and TempT5. We can see that the performance of ChatGPT significantly drops on L1 month prediction. Besides, its performance on L2 and L3 reasoning is also not promising, even losing to the significantly smaller FLAN-T5-L in L2 reasoning. For the two models that are fine-tuned on TempReason, namely T5-SFT and TempT5, their performance is significantly better than the zero-shot performance of the instruction-tuned LLMs. Last but not least, our proposed TempT5 can improve the performance of T5-SFT significantly in OBQA and the ReasonQA set. If you take a closer look at L2 reasoning by time period, we found that ChatGPT\\'s performance varies greatly across different time periods, showing that ChatGPT is flawed in temporal reasoning. And even though TempT5 has the best performance, we also observe some performance fluctuation over different time periods, which could be related to the training data imbalance. Future work can work on overcoming such reasoning biases. In conclusion, we analyze and expose the temporal reasoning biases of LMs. We also proposed the TempReason benchmark dataset that covers all three temporal reasoning types and comprehensive time periods. Last but not least, we propose a training paradigm to improve LLM\\'s temporal reasoning. And that will be all for my presentation. Thank you all for your time.'}\n",
      "{'id': '125', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '126', 'prompt_en': 'Answer the following question concisely given the English content: Was translating the natural language query using a machine translation model before semantic parsing considered as a baseline?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wurde die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing als Baseline betrachtet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: La traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stato considerato come un approccio standard?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '127', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/qfivzIuNbe.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/qfivzIuNbe.mp4', 'text': 'Hi, thanks for tuning in. I\\'m Namgyu Ho, a master\\'s student at KAIST AI in Korea. I\\'d like to introduce our work, \"Large Language Models Are Reasoning Teachers\". This is a joint work between me, Laura Schmid, and our professor Se-Young Yun. First, I\\'ll give a brief summary of our paper. So as you may know, chain-of-thought reasoning was introduced as a technique to enable large language models to solve complex tasks. The problem is, this technique only works on huge models such as GPT-3 or PALM. Now, this is an issue because these models require huge memory and computation, making them costly or even impossible to deploy in many situations. To solve this issue, we propose to use these huge models as reasoning teachers to transfer their reasoning abilities to much, much smaller models. We also propose a novel technique to boost teaching called diverse reasoning. We also went through hundreds of examples to identify if or whether these small models can do complex reasoning, which previously required very large models. With that summary, let\\'s dive right into the main presentation. All right, so let\\'s start from chain-of-thought reasoning again. Many papers on foundation models have found that even the largest language models suffered in tasks that required multiple steps, such as math questions. However, it was found that if you prompt the models to solve the problem step by step, the models could perform the complex reasoning and solve them successfully. This technique is called chain-of-thought prompting. But again, this comes at a major limitation. Only the largest models could understand and execute these prompts, and this hugely limits the applicability of the method. And our solution is simple. We apply the aforementioned prompting techniques on very large models to generate step-by-step solutions for complex tasks, and we simply use them as training data to fine-tune small models. Essentially we use the larger models to teach the smaller ones. So, our method is simple. We start with a question from an existing benchmark dataset. Then we apply zero-shot chain-of-thought prompting, basically asking the teacher model to solve the question step by step. If the final prediction is correct, then we reformat the reasoning solution into a training sample for the student. Essentially, the student is fine-tuned to respond to the questions with a step-by-step solution followed by the final answer. Now you may have seen a similar idea in recent works. We were actually beaten by a similar paper from Google by just a few days on arXiv. But we\\'d like to emphasize our novel technique, that has been underexplored in concurrent work, that we call Diverse Reasoning. If you see here, we don\\'t only generate one reasoning sample from the teacher; we generate many of them using stochastic temperature sampling. And since these are very complex questions, all of these solutions have slightly distinct solutions, and that is why these samples can be used to train the student even better. And we found that students under this method, that we call fine-tuned CoT, can perform complex reasoning tasks quite well. Let\\'s just quickly glance over the numbers. We compared our method with existing baselines on 12 tasks. If you see the prompt-based baselines, they achieve almost near-random performance in many cases. But our method can achieve notable performance in many tasks, and this is especially true for text-based ones, from data understanding to coin flip. And Diverse Reasoning can substantially increase the performance here, with the performance on Multi Arithmatic going from 33 to 55%. So, our method also significantly outperforms vanilla fine-tuning on most tasks, even with the smallest model that has 0.3 billion parameters. So this is a summary of our results. Now, if you are not satisfied with the performance on some of the harder tests, we show that there are many ways to scale student performance even further. Let\\'s just quickly glance over the figures here. So for the first one, we see that Diverse Reasoning scales the performance quite a bit. Obviously, more datasets, or a better teacher model, or a bigger student model are all ways that you can achieve better performance using our method. So the performance of our method is highly scalable, but also these axes give us a lot of trade-offs that we need to consider. So Diverse Reasoning, dataset size, and teacher models involve development-time costs, and the choice of student model involves inference-time costs. And we need to deal with these trade-offs when we actually apply them to the real world. Now there\\'s a lot of analysis and discussion that is in the paper, but we don\\'t have time for that, so you can check the paper for more details. The basic takeaways is that simple distillation can transfer the reasoning abilities from very large teachers to small students, smaller than 1 billion parameters, and this may be possible for other emergent abilities in the future. Our method with Diverse Reasoning is accessible, and it\\'s a very effective approach, and it\\'s also very highly scalable. And this kind of distillation poses various tradeoffs between development costs and inference costs, as well as the quality of the inference. That’s it for the video. Please check out our paper that goes through a bunch of details over 40 pages, including how reasoning emerges in small models as well as results on open-source models. We provide the code and data from all of our experiments, even the smaller ones, including $1000 or more worth of teacher inference from OpenAI for your pleasure. We encourage you to take our material for future work. Also, feel free to reach out for discussions. Thank you for listening, and hope to see you at the conference.'}\n",
      "{'id': '128', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '129', 'prompt_en': 'Answer the following question concisely given the English content: What example did the authors give as a marked group?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Beispiel haben die Autoren für eine markierte Gruppe gegeben?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale esempio hanno fornito gli autori come gruppo contrassegnato?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者给出的“显性群体”(marked group) 的示例是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '130', 'prompt_en': 'Answer the following question concisely given the English content: Which model architectures do not generalize well?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Modellarchitekturen generalisieren nicht gut?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali architetture dei modelli non generalizzano in modo adeguato?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些模型架构泛化能力较差？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '131', 'prompt_en': 'Answer the following question concisely given the English content: What are the names of the testing datasets?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißen die Testdatensätze?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i nomi dei set di dati di test?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 测试数据集的名称是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '132', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '133', 'prompt_en': 'Answer the following question concisely given the English content: Does the author work with multiple modalities or only text?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Arbeiten die Autoren mit mehreren Modalitäten oder nur mit Text?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: L'autore opera con più modalità o solo con il testo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者是否采用了多种模态，还是仅使用文本？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '134', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '135', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '136', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ngOSQaTycA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ngOSQaTycA.mp4', 'text': 'Hello everybody. My name is Jasivan, and today I\\'ll be presenting the work that I conducted with my supervisor, Nafise, at the University of Sheffield titled \"FERMAT: An Alternative to Accuracy for Numerical Reasoning\". You\\'ve got a QR code there which gives you access to the paper, the GitHub repo, my Twitter, and LinkedIn. Let\\'s get started. So what\\'s the motivation behind this work? There are lots of real-world applications for numerical reasoning, and you also have lots of downstream tasks which require the factual correctness of those numerical reasoning tasks. For example, fact checking. Let\\'s have a look at a concrete example. So Infotabs has a pair of statements, such as \"Chris Brown became famous when they were 16 years old,\" and a table. And from the table, one needs to infer with the statement if it\\'s an entailment, a contradiction, or neutral. And here, for example, you\\'d need to subtract these two numbers to be able to correctly classify. But the subtraction might be successful with some models and not with other models. So that\\'s definitely an issue that we have. Depending on the size of the language model, so larger ones seem to perform better than smaller ones; we can see this on the graph. So from this graph, we can see that on the right-hand side, the models are larger. The X-axis represents the size of the models in billions of parameters, and the ones that do well are the ones that have at least 10 billion parameters. But what we\\'re interested in is this 3-billion mark, which we find that are the more accessible models. And we want to understand why these models perform so poorly at numerical reasoning. The current benchmarks actually don\\'t facilitate that either. They actually tend to give an accuracy score, an F1 measure, which aren\\'t very informative as to what the strengths and shortcomings are in terms of mathematical ability of these models. So based on these motivations, what we introduce is FERMAT, which is a flexible evaluation set based on arithmetic types. We look at number understanding, mathematical operation, and training dependency. So what FERMAT is, is basically a bunch of maths worded questions, and they\\'re extracted from Illinois and CommonCore. For example, \"A euro is 5 yens. How much is €25?\" And what we do in FERAMT is we change the representation of those numbers to maybe 5.0, to maybe mimic what can exist in real life. We also change the numbers to large integers, smaller integers, and decimal numbers as well, just to test the range and the breadth of these models and what they\\'re good and bad at. We have a different axis as well, which is mathematical operations. We look at whether these models — instead of just a number of the setting — whether they perform better at easier operations or combinations of two operations. So we also have that information. We\\'ll leave the training dependency for now. Before that, what we do is we perform a baseline evaluation, which is a zero-shot evaluation. And we find that actually most models tend to perform quite poorly across all these aspects that we\\'ve introduced. The original set, which is made up of CommonCore and Illinois, tends to perform slightly better, which shows that these benchmarks might not actually be representative of the necessity of the real world. We then perform some fine-tuning. And to do the fine-tuning, we get math teachers to write the templates like the one below, where the numbers are replaced by number holders, num1 and num2, and there\\'s the expression as well, so that we can generate as many as possible. And we generate 200,000 examples, and we train some models. And what we find is that when we generate the models, we generate questions that involve small integers, large integers, and decimals as well. And we see that actually the performance increases on original, but also increases across the board on a lot of the aspects, and this is quite promising results. Going back to training dependency, what we really look at is these five classes at the bottom, and let\\'s just focus on the first one, \"Exact\". So we look at if at testing, I need 3 + 7, have I had a question that needed 3 + 7 at training time? And has seeing that at training time made the model perform better? And what we see is that even when the exact expression is seen, the accuracy is still below 50%, so the model doesn\\'t necessarily memorize these things. And what we believe is that maybe at training, maybe the word \"increases\" was present to suggest addition, whereas at testing, the word \"another\" might be used, so that linguistic notion is quite important. Our last investigation that we perform is looking at the impact of training templates. So what we have is we\\'ve got four graphs: zero-shot, which is the black one in the middle; base, which is 200,000 questions which is the one we used to fine-tune the models; base scaled up, which is up to 300,000 using the same templates; and base diversified, which uses extra templates from GSM8K and AQUA, which is the green graph on the outskirt. And this shows that having this kind of language diversity from GSM8K and AQUA, and also mathematical diversity, as they combine different operations as well, shows that this is a lot more promising in improving the performance. So in terms of conclusions, we find that the existing benchmarks tend to be unrepresentative, and single scores don\\'t help with that. And this is why FERMAT is there, to provide a more informative alternative to fill in that gap. We find that language and mathematical diversity is important, and also that, with other analyses, find that number encoding and tokenization are areas of improvement. So again, you\\'ve got the QR code and the links. Thank you very much for listening and I do encourage you to read the paper.'}\n",
      "{'id': '137', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/GzwcngoTzb.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/GzwcngoTzb.mp4', 'text': 'Hi. I\\'m Sicong from the Singapore University of Technology and Design. I will share our work named \"Tell2Design: A Dataset for Language-Guided Floor Plan Generation\", published in ACL 2023. Recently, text-conditional generative AI models have demonstrated impressive results in generating high-fidelity images. Such models generally focus on understanding high-level visual concepts from sentence-level descriptions, and the generated images are valued for looking realistic and being creative, thereby being more suitable for generating artwork. However, besides less-constrained generation like artwork, generating designs that meet various requirements specified in natural languages is also much needed in practice. In particular, a design process always involves interaction between users, who define objective, constraints, and requirements that should be met, and designers, who need to develop various solutions with domain-specific experiences and knowledge. For example, users may dictate their house design requirements in text and expect expert architects to perform the floor plan generation. So to allow people without expertise to participate and further enhance the design process, we aim to enable users to design by \"telling\" instructions, with a specific focus on the floor plan domain as the initial area of the research. And this sets forth a new machine-learning task where the model learns to generate floor plan designs directly from language instructions. Here, we define the task as follows. Given a set of language instructions describing a floor plan\\'s intrinsic components, we aim to generate reasonable 2D floor plan designs that comply with the provided instructions, as shown in the figure below. To be more specific, for each data sample, the input is a set of natural language instructions that characterize the key components of the corresponding floor plan design, which includes: Semantics, that specifies the type and functionality of each room. Geometry, that specifies the shape and dimension of each room. And topology, that describes the relationships among different rooms. And the desirable output is a structured interior layout that aligns with the input language instructions. We use publicly available floor plans to construct our Tell2Design dataset, and each floor plan is associated with collected language instructions. 5,051 human-annotated language instructions are collected from crowd workers from the Amazon Mechanical Turk, and around 76,000 language instructions are generated artificially from pre-defined templates. The statistics of our collected language instructions are shown in Table 1, where the average number of words for a single floor plan is more than 200, resulting in more than 10 sentences. The main challenges of this novel task are threefold. The first challenge is to perform the design generation under much stricter constraints compared with artwork-like text-conditional image generation. And the second challenge is to understand the big picture of the entire floor plan from document-level unstructured text with fuzzy and entangled information. And the third challenge comes from the ambiguous, incomplete, or misleading information in human instructions. Unlike existing floorplan generation methods that use a regression head to generate the bounding box of each room one at a time, we cast the floor plan generation task as a sequence-to-sequence problem under the encoder-decoder framework, where room bounding boxes are re-constructed into a structured target sequence. This way, our method can easily deal with various lengths of instructions for floor plans with different numbers of rooms. Recall that our aim is to generate a floor plan layout from language instructions, where each room can be represented by a room type label and a bounding box. One bounding box can be determined by four values which indicate the X and Y coordinates of the central point, and height, H, and width, W, respectively. For example, the target sequence of a balcony is given below. To solve language-guided floor plan generation as a sequence-to-sequence problem, we treat the instructions as the input sequence and consider the bounding boxes of the room as a target sequence. We use the popular transformer-based encoder-decoder structure to build our sequence-to-sequence model for floor plan generation. The model is initialized by a pre-defined, or pre-trained language model T5 for better language understanding abilities. And we use a normal language modeling objective where X is a set of instructions in natural language and Y is the target bounding box sequence, and L is the target sequence length. Table 2 shows the floor plan generation results on the T2D dataset. To evaluate how floor plan generation methods generalize to unseen instructions, there is no overlapping between the annotators of the training set and test set. The T2D model achieved the highest IoU scores, with a Micro IoU of 54 and a Macro IoU of 53, outperforming other text-conditional image generation baselines by a large margin. This can be attributed to our sequence-to-sequence model in controlling the target box sequence generation based on salient information extracted from the language instructions. In contrast, text-conditional image generation methods fail to perform well. This is probably because those models are designed to generate artwork-like images with high-level visual concepts from the short text, instead of following multiple instructions with various constraints for a specific design. When training only on artificial instructions while testing on human-written ones, our method cannot perform well. This indicates there is a language distribution gap between artificial and human instructions. Nevertheless, when artificial instructions are used for warming up before training on human instructions, the performance of our method is significantly improved with over 10 IoU scores increment. This suggests that despite the language gap, artificial and human interactions are mutually beneficial data portions during training. Here we provide a case study of different baseline generations conditioned on the same human instructions. You can see that although several text-conditional image generation models can produce realistic floorplan images, they all fail to align well with the requirements specified in the human instructions. To conclude, we initiate the research of a novel language-guided design generation task, with a specific focus on the floor plan domain as a start. And we introduce Tell2Design, a large-scale dataset that features floor plans with natural language instructions to describe user preferences. And we then propose a sequence-to-sequence model as a strong baseline and compare it with several text-conditional image generation models. We hope this paper will serve as a foundation and propel future research on the task of language-guided design generation. And that\\'s all. Thanks for listening.'}\n",
      "{'id': '138', 'prompt_en': 'Answer the following question concisely given the English content: What do the authors claim is an understudied area in NLU?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist nach Ansicht der Autoren ein zu wenig erforschtes Gebiet im Bereich der NLU?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Secondo gli autori, quale area della NLU è poco studiata?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者认为哪些是 NLU 中研究不足的领域？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '139', 'prompt_en': 'Answer the following question concisely given the English content: What are the names of the speakers?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißen die Referenten?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i nomi dei relatori?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '140', 'prompt_en': 'Answer the following question concisely given the English content: Did Coscript undergo any quality checks?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Hat Coscript eine Qualitätskontrolle durchlaufen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Coscript è stato sottoposto a controlli di qualità?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： Coscript 是否经过了质量检查？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '141', 'prompt_en': 'Answer the following question concisely given the English content: What are the limits of existing resources for on context-dependent translation?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wo liegen die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i limiti delle risorse esistenti per la traduzione dipendente dal contesto?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 对于依赖上下文的翻译，现有的资源有哪些局限性？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '143', 'prompt_en': 'Answer the following question concisely given the English content: To which existing SimulST policies is the approach compared?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Mit welchen bestehenden SimulST-Richtlinien wird der Ansatz verglichen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Con quali politiche SimulST esistenti viene confrontato l'approccio?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该方法与哪些现有的 SimulST 策略进行了比较？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '144', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '145', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '146', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/GPKqavlRfX.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/GPKqavlRfX.mp4', 'text': 'Hi everyone. I\\'m Yicheng, a PhD student from Fudan University. Now I will give you a talk about our paper on the analysis of omission in dialogue summarization. First, I\\'m going to briefly introduce the background of dialogue summarization. Dialogue summarization is a subtask of text summarization. It is the process of creating a concise summary that represents the most important information within a dialogue. There are many scenarios in dialogue summarization. How to extract key information of dialogues in different domains is valuable and worth exploring. In recent years, we have achieved great progress in dialogue summarization, especially using large-scale pretrained language models. Although they can generate fluent and coherent summaries, these summaries still cannot be used in real-world applications due to their common errors. For example, the generated summaries may have factual errors that conflict with the original dialogue. Among these common errors, omission is a major factor in affecting the quality of dialogue summarization, leading to incomplete summaries where critical facts are lost. However, fewer works have systematically analyzed the omission problem, let alone addressed this problem. So, how serious is this problem in dialogue summarization? We analyzed the percentage of summaries which suffer from the omission problem. The left figure shows the omission rate of summaries from five domains and six pre-trained models. We find that even the state-of-the-art model still reaches a high omission rate. About 70% of the generated summaries have the omission problem, which means omission is a general and serious problem in dialogue summarization. The right figure shows the position distribution of omission information in dialogues. We find that the omitted information are randomly distributed in each position of the dialogue regardless of its length and domain. It means dialogues are unstructured, and how to identify the key information is still difficult for current models. To better analyze the omission problem and further solve the problem, we need first to detect all possible omissions in dialogue summaries. Here we do the task definition for omission detection. Omission usually refers to the missing content in the generated summaries, which is presented in the gold reference. In this task, we mainly focus on the utterance-level omission, and the detection model needs to predict which utterances of dialogue are omitted in the candidate summary. However, there are no omission-related datasets in dialogue summarization to support such analysis and detection tasks. So in this work, we construct the OLDS dataset, which provides high-quality omission labels for dialogue summarization. Our dataset is built upon five existing benchmarks covering five domains. For each dialogue, we use different abstractive models to generate diverse candidates. We further propose an automatic method to produce omission labels for these candidate summaries. To ensure the quality of labels, we also perform human evaluation to assess the quality. Here shows an example. We produce 10 different candidate summaries for the dialogue, which are generated by different models and different decoding strategies. Here shows the statistics of our dataset, including the total number and the ROUGE-1 score of candidate summaries for each model and domain. Our dataset is publicly available. To build a foundation for the omission detection task and explore what model architecture is better, we explore three frameworks as baselines, which have different input formats and structures, including pair-wise classification, sequence labeling, and pointer network. We use the Precision, Recall, and F1-score to evaluate our omission detection models. Furthermore, we calculate the percentage of gold emission words that are hit in the detected utterances to measure the word-level omission recall, denoted as WR score. The figure above shows the label imbalance problem exists in the dataset. For the results in the table, you can find that the F1-score is around 50%. This performance indicates the task is very challenging, which calls for more advanced detection models. Another question is, \"\"What happens if we use the omissions to refine the summary? Will the summary quality be improved?\"\" Here, we use a post-editing method for summary refinement. The method is straightforward. We concatenate the candidate summary with omission content as input, and the model outputs the refined summary in a sequence-to-sequence manner. From the figure, we find that the performance is largely boosted when the omission is provided. It indicates that the omission detection is a valuable task, and the refinement based on the detected omission is a promising direction for quality improvement in dialogue summarization. That\\'s all for the introduction. If there is any question please contact me. Thank you.'}\n",
      "{'id': '147', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '149', 'prompt_en': 'Answer the following question concisely given the English content: Is the dataset publicly available?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Ist der Datensatz öffentlich zugänglich?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il set di dati è disponibile pubblicamente?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 数据集是否公开？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '150', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/HiWpKvzyxh.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/HiWpKvzyxh.mp4', 'text': 'Hello everyone. I\\'m Archiki, and I\\'ll be presenting our ACL paper \"MEETINGQA:Extractive Question-Answering on Meeting Transcripts\". I\\'m really thankful to all my collaborators from Adobe Research and UNC Chapel Hill. We know that millions of meetings take place every day worldwide. This results in vast amounts of meeting transcripts that can serve as a new domain for NLP research. What makes this domain unique and interesting is that meeting transcripts are long documents which are often domain-specific and information-rich. However, prior works in this area only focus on the task of summarization and extracting action items. This means they underutilize the inherently significant QA component in meeting discussions. Typically, a participant in a meeting will ask a question which elicits detailed responses and discussions from others. We address this gap by introducing a new dataset called MeetingQA, which is an extractive question answering dataset based on questions asked by participants in a meeting and the corresponding answer sentences. Here is an example from MeetingQA with a question shown in red, and the answers are highlighted in blue. As seen in the figure, questions asked by participants are longer, open-ended, and actively seek discussions from others. We also have interesting answer scenarios, such as multiple speakers contributing to the answer, multiple discontinuous sentences forming the answer span, as well as rhetorical questions. Here is a brief overview of our data collection process. We begin with public meeting transcripts from the AMI corpus, which corresponds to nearly 100 hours of manually transcribed multi-party meetings. We perform question selection based on punctuation and filtering out really short questions. To annotate answers, we recruit annotators to label sentences in the answer span. We obtain a high inter-annotator agreement, reflected by a Krippendorff\\'s alpha of 0.73. In total, MeetingQA contains 7.7K questions split between the Train, Dev, and Test sets, as shown in the table. 30% of our questions are unanswerable. Out of the remaining, 40% have multispan answers and 48% have multi-speaker answers. Here is a distribution of question types. A majority of our questions are framed in a yes/no manner and still elicit detailed responses, as well as our opinion seeking. 20% of questions are framed rhetorically, and lastly, 70% of multiple-speaker answers contain some disagreement. Lastly, we show the length distribution of meeting transcripts, questions, and answers in MeetingQA. Questions and answers in our dataset are roughly composed of 12 and 35 words, respectively. We also achieve a high human performance on the Test set with an F1 of 84.6. We employ a variety of methods in our paper. First of all, for short context models which cannot fit the entire transcript in the input, we perform context-retrieval. Then we build single-span models which are trained to output the first to last relevant sentence in the answer span. We also have multi-span variants in which QA is a token classification task; that is, whether a given token is in the answer span or not. Lastly, we automatically annotate interview questions from the MediaSum dataset and use the silver annotations for data augmentation. Let us now discuss the results in the fine-tuned setting. First of all, from this table we observe that there is over a 25 F1 point gap between fine-tuned models and human performance. Next, we find that short-context models like RoBERTa slightly outperformed long-context models like Longformer. Finally, comparing single-span variants (abbreviated by SS) and multi-span variants (abbreviated by MS) in this table, we find that multi-span models have slightly less or comparable performance than single-span models. Next, let\\'s look at zero-shot performance in this table. First of all, we observe that there is nearly a 50 F1 point gap between zero-shot performance and human performance. Next, we show that silver data augmentation effectively improves zero-shot performance. Lastly, zero-shot results from larger instruction tuned models such as FLAN-T5 are comparable to the results from remaining models. Error analysis in the first half of the figure shows that models are bad at identifying rhetorical questions, especially in the zero-shot setting. Also, predictions of single-span models contain more irrelevant sentences than their multi-span counterparts. The bottom half of the figure shows that models struggle to identify which speaker answers a question, and this gets worse in the zero-shot setting. To summarize, MeetingQA is an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios, and the dataset is far from being solved as it is challenging for existing QA models in both fine-tuned and zero-shot settings. Thank you so much for listening, and you can find more details on our project page or in our paper.'}\n",
      "{'id': '152', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/jFMhdOSrOn.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/jFMhdOSrOn.mp4', 'text': 'Hello everyone. My name is Frederick Riemenschneider and I am here to talk about our work at the fascinating intersection of NLP and classical philology. In this presentation titled \"Exploring Large Language Models for Classical Philology\", I will introduce valuable resources for Ancient Greek and Latin. Moreover, we will explore the implications and challenges of multilinguality in these models. Before we dive in, let\\'s take a quick look at the current landscape of language models and classics. There have been several models recently developed. Latin BERT was introduced in 2020, then came Ancient Greek BERT in 2021, and another Ancient Greek BERT was released just last year in 2022. So with these advancements, it might seem like we are done, right? Well, of course not. First of all, all these models are BERT models, which means that they are a specific type of encoder-only models. Moreover, these models are all monolingual, yet it seems realistic that scholars might want to utilize a model that is proficient in both Ancient Greek and Latin. Common multilingual models, on the other hand, are usually not pre-trained on Ancient Greek texts. And then the performance of these existing models is somewhat uncharted territory. This lack of robust evaluation inhibits us from truly understanding their capabilities and areas for improvement. So we have undertaken the exciting task of creating new language models specifically designed for classical philology. Our concrete goals in this project are fourfold. To make existing models comparable, to push the state-of-the-art further, to explore different model architectures, and to introduce multilingual models, we have pre-trained two monolingual models for Ancient Greek, namely GreBERTa and GreTa. GreBERTa is a monolingual RoBERTa model for Ancient Greek. GreTa, on the other hand, is a monolingual encoder-decoder model based on the T5 architecture, capable of both understanding and generating Ancient Greek texts. In addition to these, we have also developed PhilBERTa and PhilTa. These models are multilingual equivalents pre-trained on Ancient Greek, Latin, and English data. So in essence, we have varied our models along two dimensions, language and architecture. The first step in building these models is to gather pre-training data. For Ancient Greek, previous models have relied on Open Greek & Latin, which we naturally also use. But in addition we leveraged previously unused resources, and to go a step further, we have developed a new pre-training corpus from the Internet Archive. The Internet Archive contains numerous book scans with OCR transcriptions. However, as we can see in this example, the Greek text appears as unreadable rubbish because the OCR did not allow Greek characters. Unfortunately, most texts have not been OCR\\'d with settings allowing Greek characters, and they aren\\'t tagged as containing Greek texts. So how can we use the Internet Archive in a beneficial way? Our solution? We searched for incorrectly transcribed Greek stop words such as γάρ, which is often transcribed as \"yap\". This method allows us to reliably identify Greek texts. Then we scan the books again, but with Greek OCR settings enabled. After rigorous post-processing, we had a new and high-quality pre-training corpus. For the multilingual models, we leveraged additional resources: the Corpus Corporum for Latin, and for English, we used English texts that are somehow related to antiquity. Once we train our models, the next important step was benchmarking. For Greek, we used the Universal Dependencies treebanks, for Latin, we chose the EvaLatina 2022 dataset, which allows us to compare our models to a very recent competition. We focused on three main tasks: part-of-speech tagging, dependency parsing, and lemmatization. To make a long story short, our models are clearly outperforming the current state-of-the-art for both Ancient Greek and Latin. We also took the opportunity to analyze how GreTa behaves if we use only the encoder of the T5 model separately for tasks that do not require a decoder. Interestingly, as we can see, the performance of GreTa\\'s encoder is extremely bad after one epoch, even worse than a randomly initialized model. After more training, however, the T5 encoder approaches the performance of the native encoder-only model. This shows that encoders of T5 models behave fundamentally differently than native encoder-only models. Lemmatization is where the real strengths of our encoder-decoder models shine through. We did not use a pipeline setting. Instead, we simply input the sentence and marked the word to be lemmatized. With this approach, our models have elevated lemmatization performance by an impressive 5 percentage points above the existing state-of-the-art for Ancient Greek. We can also see performance gains for Latin lemmatization. We also probe our models for their semantic and world knowledge capabilities. Can our models distinguish synonyms from antonyms? Furthermore, can they identify relations between heroes and gods? Lastly, does our multilingual model perform better because it can learn from three languages? Results have shown that our models significantly outperform previous models. However, there doesn\\'t seem to be a significant difference between the performances of the multilingual and the monolingual models. This is the case both for semantic knowledge as well as world knowledge. To conclude, we have presented new powerful language models for classical philology that are initialized from scratch and use a native tokenizer. We pre-trained both encoder-only and encoder-decoder architectures as well as multilingual models so that Latin and Greek text can be processed by the same model. Additionally, we have introduced a high-quality pre-training dataset for ancient Greek. We have rigorously benchmarked previous and our own models. We have analyzed how T5’s encoder behaves, and we have investigated the implications of multilinguality in our language models. This video provides only a brief overview of what we did. For more details, check out our paper. Thank you for your attention.'}\n",
      "{'id': '153', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/aLREISxzrN.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/aLREISxzrN.mp4', 'text': 'Hello. My name is Ninareh Mehrabi. I am a postdoctoral scientist at Amazon Alexa AI\\'s Responsible AI team, and I will present our work, \"Resolving Ambiguities in Text-to-Image Generative Models\". In this work, we were interested in studying existing ambiguities in prompts provided to text-to-image models. For instance, the following prompt is ambiguous because it can have various different interpretations. Or the following prompt, \"The girl enters the room with flowers,\" is ambiguous because it\\'s not clear whether the flowers should be in the room, with the girl, or a combination of both. And without resolving these ambiguities, it\\'s going to be challenging for text-to-image models to generate faithful images to user intention. So in this work, we are studying prompts that are ambiguous and are provided to text-to-image models, and our goal is to propose frameworks to mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to user intention. Here is our pipeline. First we curate a benchmark dataset that covers different types of ambiguities. Then these prompts are provided to a prompt disambiguation framework that tries to gather external signals to disambiguate the prompt, through either asking clarifying questions from the user or generating different possible visual setups. Once we have the prompts disambiguated, we are going to evaluate them. We are also going to input these disambiguated prompts into a text-to-image model, generate the images, and evaluate whether the generated images are faithful to user intention. So let\\'s start talking about our benchmark dataset. Our benchmark dataset is the modified version of a previously existing corpus called LAVA, and it covers different types of ambiguities as listed in the table below. In our framework, the language model first generates clarifying questions using in-context learning. So given an ambiguous prompt, the language model will generate a clarifying question. The user will then answer the clarifying question based on the intention that they have in mind, and we are going to obtain a disambiguated prompt by concatenating the signal to the original ambiguous prompt. Notice that the user can also provide the answer that satisfies another interpretation. In the second setup, instead of the language model generating a clarifying question, it will generate different possible visual interpretations. So we still have the original ambiguous prompt; here, the language model instead of generating clarifying questions, will generate different possible visual setups and the user will interact with the system and provide the answer that satisfies its intention. And we are going to again gather a final disambiguated prompt by concatenating the signal to the original ambiguous prompt. Notice that it\\'s possible that the human is picking the other interpretation. Now that we have our prompts disambiguated, we want to evaluate whether the generated images are faithful to user intention. To do that, we are going to propose our automatic evaluation framework. So we have the original ambiguous prompt versus the one that is disambiguated. We are going to input this as text-to-image models, and we are going to generate the image corresponding to each prompt. To evaluate whether these images are faithful to user intention, we are going to use a VQA model. We are going to input the images as well as the human\\'s intention in question format as input to the VQA model, and we are going to evaluate whether the human\\'s intention is satisfied in the image or not. If the answer is yes, it means that user intention is satisfied, so the image is faithful; if the answer is no, it means that the generation was not faithful to user intention. Then we have some findings in the paper: we show that there is disparity in resolving ambiguities for different ambiguity types. We show that disambiguation using our framework has overall a positive effect in faithful generation. And we show that our automatic evaluation framework is in agreement with human evaluation, so it can be used reliably to evaluate text-to-image models. And we also have additional findings and discussions in the paper. So if you\\'re interested, please refer to our paper. So, to conclude, in this work, we study ambiguities in text-to-image models. We curate a benchmark dataset covering different types of ambiguities, and we propose frameworks to both mitigate as well as evaluate ambiguities provided to text-to-image models. So with this, I\\'m going to conclude this talk and thank you so much for your attention.'}\n",
      "{'id': '154', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '155', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '156', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '157', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/tWTZmtSvqy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/tWTZmtSvqy.mp4', 'text': 'Hi. My name is Shen Gao from Shandong University, and today I\\'m going to introduce our work, \"Dialogue Summarization with Static-Dynamic Structure Fusion Graph\". This is a joint work with Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao, and Rui Yan. Dialogue summarization aims at distilling the salient information from a dialogue context into a concise summary. It is one of the most challenging and interesting tasks in the text summarization research field. It can help people quickly capture the highlights of a semi-structured and multi-participant dialogue without reviewing the complex dialogue context. In this slide, an example dialogue is shown on the left, which is talk about three people who will go to a concert, and the right shows an example of a summary of the dialogue, which describes the main ideas of each person. Existing dialogue summarization methods mainly focus on modelling dialogue with pre-computed static graph structure using external linguistic tools such as discourse parsing and dialogue state tracking. However, there are two fundamental drawbacks of using these pre-computed dialogue structures. Such methods heavily depend on the reliability of the external linguistic tools, which may not deliver the accurate output and cause error propagation. The second drawback is that the static graph construction is disjoint with the graph representation learning phrase, and such a fixed graph could not dynamically adapt to the downstream dialogue summarization task. In our SDDS model, there are four main components. We first employ an Utterance Encoder to encode the utterance in the dialogue context into the vector representation. We use the existing data structure modeling method to construct the static graph. Then we propose a Static-Dynamic Graph module which first combines multiple static graphs computed in the previous step and then uses the dynamic graph module to capture the semantic relationship between the utterances based on their deep vector representation. Finally, we employ a pre-trained language model as the Summary Generator to fuse the static dialogue structure and the dynamically learned dialogue structure into the final summary. This slide shows the detailed model structure of our proposed SDDS model. To capture the static dialogue structure information, we first propose four heuristic dialogue structure modeling methods to build the relationship between utterances using a graph network. The first is the Discourse Parsing Graph, which uses the discourse parsing toolkits to build a dependency-based dialogue structure. It is intuitive that when two utterances contain the same keywords, they may focus on the same topic, and they may be semantically correlated. We employ the function Key Co-occurrence (KeyCo-occ) to denote the function that calculates the number of common keywords in two utterances. Since it is essential to understand the fine-grained interaction between the speakers in the dialogue context, in our model, we propose a simple yet efficient speaker relationship modeling method. We use a sliding window around each utterance and count the frequency of the occurrence for each speaker in the sliding window and obtain a speaker interaction frequency matrix. For example, in this figure, we can find that the speaker C usually talks after speaker A, which indicates a strong relationship between two speakers. To capture the position information of utterances, we use the relative distance between utterances as the edge feature of the utterance position graph. We also employ an embedding matrix to map the discrete distance into vector space. After obtaining the adjacent matrixes for static graphs, to conduct cross-graph fusion and interaction, we can see these adjacent matrixes as different channels and use a simple but efficient 1 x 1 convolutional layer to integrate these adjacent matrixes into a fused representation. To capture the semantic relationship between utterances based on their deep vector representation, we propose a Dynamic Graph module that does not use any pre-computed or heuristic method to build the connections between nodes. We employ a multi-head attention model to calculate the relationship. To integrate the static and dynamic graph, we propose a fusion method to combine the relation matrix A of the dynamic graph and the adjacent matrix Gˢ of the static graph into a unified graph, Gᵘ. And to incorporate the graph representation which captures the dialogue structure information in the generation process, we use the dual cross-attention mechanism by proposing a graph attention layer on the top of original self-attention layer. Thanks for watching. The code and data have been released on GitHub, and you can scan the QR code to download it. Thank you.'}\n",
      "{'id': '158', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/VtZPHhYgYr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/VtZPHhYgYr.mp4', 'text': 'Hi. My name is Qipeng Guo from AWS. Today, I will introduce our work \"Dual Cache for Long Document Neural Coreference Resolution\". So let me first introduce the task of coreference resolution. The entities in a document may have multiple mentions across the text. The coreference resolution task is to identify the mentions and cluster the mentions that refer to the same entity. In this simple example, there are three entities and five mentions of them. So the goal is to link \"John\" with \"he\" and then link \"Maria\" with \"her\". Conventional methods for this task need to enumerate all the pairs of mentions, which have quadratic complexity for both computation and memory consumption. The recently proposed cache-based methods use a fixed-size cache and reduces the complexity to a linear level. In the cache-based method, when the cache is full, it evicts an entity with some eviction policy, such as LRU, which means removing the Least Recently Used entity from the cache. However, in long documents, the topic may switch multiple times, and this causes the mention of an entity to be scattered across a wide range of text. So, the LRU policy will lead to a high cache miss when encountering a new mention. Our case studies show that the high-frequency entities are mentioned globally, and they account for most of the cache misses. With this consideration, we propose a dual cache that has a local cache and a global cache that work together. The local cache stores local entities with LRU eviction policy. And the global cache stores global entities with LFU policy, which evicts the Least Frequently Used entity when the global cache is full. The dual cache works this way. The model scans the document from left to right. When it encounters a new mention, it first classifies whether it is a new entity or belongs to an entity in the cache. Then, it evaluates the frequency of this new or updated entity. If qualified, it is added to the global cache. Otherwise, it is added to the local cache. Whenever the cache is full, it triggers the eviction policy to evict an entity from it. We evaluate dual cache on four public benchmarks, and this slide shows three of them. The LitBank and OntoNotes contained training data, while WikiCoref datasets does not. With training data, dual cache performs better than the baselines even when they use unbounded memory. However, we also observed that without training data, the model with unbounded memory performs slightly better, but dual cache is still faster. To evaluate the capability of dual cache, we annotated a book with 30,000 words. We can see that the performance gap is much larger between the baseline and dual cache for the book-level document. Also, we show that dual cache significantly reduced the cache misses compared with a single cache. Last but not least, there are always trade-offs between model efficiency and performance for cache-based models, but we show that dual cache has the highest performance/cost ratio. To conclude, dual cache uses a local and global cache to separately store local and global entities. It outperforms single cache methods and largely reduces the cache misses. Also, dual cache is the most cost-effective compared with single cache methods. So that\\'s all for my talk. Thank you for listening.'}\n",
      "{'id': '160', 'prompt_en': 'Answer the following question concisely given the English content: What type of tokens does the first step of the method map the input tokens to?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Art von Token werden die Input-Token im ersten Schritt der Methode zugeordnet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In quale tipo di token il primo passaggio del metodo mappa i token di input?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该方法的第一步将输入词元映射到什么类型的词元？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '161', 'prompt_en': 'Answer the following question concisely given the English content: How many scripts are represented in Coscript?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Skripte sind in Coscript vertreten?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti script sono rappresentati in Coscript?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： Coscript 中包含了多少个脚本？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '162', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '163', 'prompt_en': 'Answer the following question concisely given the English content: What is the best alignment method for DEplain?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist die beste Ausrichtungsmethode für DEplain?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il metodo di allineamento migliore per DEplain?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： DEplain 的最佳对齐方法是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '164', 'prompt_en': 'Answer the following question concisely given the English content: What is the benefit of weakly supervised learning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist der Vorteil von schwach überwachtem Lernen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il vantaggio dell'apprendimento scarsamente supervisionato?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 弱监督学习有什么好处？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '165', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/fRfVahTYdO.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/fRfVahTYdO.mp4', 'text': 'Hello everyone, I\\'m excited to be here to present our recent paper titled \"Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations\". My name is Wenting Zhao, and I\\'m a PhD student at Cornell University. Before diving into our approach to abductive reasoning, I will first provide a concrete example to help illustrate what it means, followed by a more formal definition. And abductive reasoning starts with a Context X, \"Emily was stuck in traffic,\" and ends with an Outcome Y, \"Emily made it to her flight.\" Additionally, a set of possible explanations are given; here the set includes Explanation 1, \"Her flight was delayed,\" and Explanation 2, \"Her flight left on time.\" The goal of abductive reasoning is to identify a plausible explanation which can bridge the information gap between the context and outcome. In this example, Explanation 1 is plausible as it successfully bridges the gap between the given context and the resulting outcome by providing the necessary missing information. More formally, our paper considers a closed-world setting for abductive reasoning, where a candidate set of explanations Z is given and the goal is to identify a plausible subset of the explanations. Current approaches to abductive reasoning predominantly rely on supervised methods. However, these methods necessitate the annotation of plausible explanations, which can be noisy and subjective. A recent experiment revealed that crowd workers disagree on 60% of over 1,000 explanations. Consequently, we pose the question, \"Is it possible to learn abductive reasoning without supervision regarding the plausibility of explanations?\" And our answer to this question is \"Yes.\" We introduce an unsupervised learning method called LiPoR, which stands for Likelihood Learning with Posterior Regularization. In LiPoR, we treat Explanation Z as a latent variable. This naturally leads to an unsupervised objective in which we maximize the marginal likelihood of the Outcome Y given the Context X by marginalizing our other possible explanations in Z. Therefore, optimizing this objective does not require knowing which explanations are plausible. However, the unsupervised objective L only maximizes the likelihood of the outcome given the context. It doesn\\'t really do anything to prefer plausible explanations. Therefore, we also need an additional regularizer to achieve this. To build the regularizer, we depend on a significant characteristic of explanations, which is their mutual exclusivity. We refer back to the abductive reasoning example shown at the beginning; we can see that the explanations cannot be both true at the same time, and that if the explanation \"Her flight was delayed\" were true, it automatically rules out the other explanation, \"Her flight left on time.\" Therefore, our regularizer aims to enforce this mutual exclusivity among explanations. More formally, the LiPoR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over the others. We denote the regularizer by Omega. And Omega takes the max between the entropy of P of Z given XY and the log of M, where M is the number of plausible explanations. When the entropy of P of Z given XY is larger than the log of M, that means there are more than M explanations received probability mass. We then minimized the entropy of P of Z given XY, in that case preferring a subset of explanations. Here is a brief overview of our result on AlphaNLI, the most widely-used abductive reasoning dataset. We compare it to a number of zero-shot models and the previous best unsupervised approach. LiPoR outperforms all of them, including a strong zero-shot GPT-3 baseline, by over 4 absolute points in accuracy. And this concludes my talk. Thank you for listening. Our paper can be found at tinyurl.com/zhao-lipor.'}\n",
      "{'id': '166', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/nGuerJBXGU.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/nGuerJBXGU.mp4', 'text': 'Hello everyone. I\\'m Yunxin from Harbin Institute of Technology, Shenzhen. It\\'s my pleasure to introduce our new work, \"A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text\". This image retrieval from linguistically complex text is a challenging image text reasoning task because these images are highly similar and the description is long. Typical methods, such as visual language models, perform well on image sentence retrieval tasks, but their performance drops drastically when confronted with linguistically complex text. To address this issue, we get inspired by the Divide-and-Conquer strategy and Dual-Process Theory. Divide-and-Conquer is the strategy of solving a large problem by breaking the problem into smaller ones, solving the sub-problems, and combining them to get the desired output. Dual-Process Theory for human thinking: human brains contain two thinking systems. System 1 performs analogical reasoning, while System 2 is capable of abstract logical reasoning, well-suitable for complex reasoning problems. Pre-trained visual language models focus on analogical reasoning as System 1, based on the analysis of deep learning networks. When confronted with complex tasks, their performance drops drastically. We may need a logical reasoning System 2 to perform this complex retrieval task via a logical operation. Combining the advantages of System 1 and System 2 may be a significant way for complex reasoning, and they can be integrated with the Divide-and-Conquer Strategy. The first model for our proposed method is the Proposition Generator. It aims to decompose the complex proposition text into representations of a simple proposition. For explaining what simple propositions represent, we also use the decoder of BART to generate the corresponding sentences. System 1, which is the Visual-Linguistic Interactor, aims to perform the visual-propositions’ information interaction, resembles the System 1. The outputs of this module are matching scores of propositions and images, and their reasoning states. Then we introduce the Neural-Symbolic Reasoner as a System 2. It is responsible for integrating the reasoning states and results of simple propositions to obtain the final solution of a complex proposition on images. It consists of the negation executor and conjunction operation. Negation executor aims to gain the negational reasoning state of positive proposition. Conjunction operation is responsible for obtaining the inference results based on joint positive and negational reasoning states. Finally, we combine the inference results of System 1 and System 2 to gain the final solution. By doing so, the whole system utilizes the advantages of the analogical inference System 1 and logical reasoning System 2. Here we present two tables of our experimental results. We can see that the proposed method, NDCR, outperforms other baselines, and in the right part, the abolition experiments on the testing set also verifies the effectiveness of each module. Here we present two cases to further check the performance of the proposed method. We can see that our proposed method can present the inference states and inference results in the middle step. So the proposed method is processing interoperably. To conclude, we present some suggestions. First, neural symbolic calculation may be a worthwhile approach to improve the compositional reasoning and planning of large language models. Divide-and-Conquer is similar to the self-asking chain-of-the-thought, aiming to decompose the complex reasoning into simple problems and construct a reasoning path. Both are effective for solving complex problems. Dual-Process Theory could be integrated with the Divide-and-Conquer. Ok, thanks everyone.'}\n",
      "{'id': '167', 'prompt_en': 'Answer the following question concisely given the English content: The documents in DEplain-web were aligned with manual and automatic alignment methods. How was the allocation exactly?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet. Wie war die Zuteilung genau?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici. Com'è avvenuta esattamente l'allocazione?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '168', 'prompt_en': 'Answer the following question concisely given the English content: How was the CoNLL++ dataset created?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurde der CoNLL++-Datensatz erstellt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come è stato creato il set di dati CoNLL++?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： CoNLL++ 数据集是如何创建的？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '169', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '171', 'prompt_en': 'Answer the following question concisely given the English content: What are the existing works on this?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Arbeiten wurden bereits durchgeführt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i lavori connessi in tal senso?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 关于这方面的现有研究有哪些？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '172', 'prompt_en': 'Answer the following question concisely given the English content: Are multilingual LLMs such as Codex or Bloom sufficient for CLSP?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Sind mehrsprachige LLMs wie Codex oder Bloom für CLSP ausreichend?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Gli LLM multilingue come Codex o Bloom sono sufficienti per il CLSP?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： Codex 或 Bloom 等多语言 LLM 对于 CLSP 来说是否足够？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '173', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '174', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/zCGmJoleYN.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/zCGmJoleYN.mp4', 'text': 'Hi, I\\'m Thea and I\\'m one of the co-authors of the paper \"ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis\" In this video, I\\'m going to quickly explain why this dataset is unique from other datasets that you\\'ll find on a similar topic. This is just going to be a quick overview of the special features that we have, so do make sure to check out our paper and our poster at the conference for better insight into the results, dataset collection process, dataset annotation process, etc. So, very quickly: What is argument quality analysis? It\\'s simply judging how good or bad an argument is on a scale from 0 to 1. So something like \"Big banks are bad\" is likely to be rated low, but something like \"Big banks have no accountability, take heavy risks, and lead to major collapses, which is why they should be broken up\" — which is a coherent argument which is more persuasive in terms of what it\\'s trying to achieve — is going to be rated high, probably close to a 1. So let\\'s brainstorm some problems with current datasets. Meaning that current datasets lack quality because they are often collected from crowdsourcing platforms. They lack diversity because they often only consist of 30 or 40 motions and source arguments on those. They often lack depth in explaining why a specific argument is true, and they often have a motion associated with every single argument that exists. So, how does ArgAnalysis35K do this differently? This is just an index of the special features that I\\'m going to cover in this presentation. So firstly, it\\'s the largest dataset with really high-quality arguments. So as you can see from the title, first off, it has 35K argument-analysis pairs, which is the largest dataset in this field to our knowledge. And as you can see from this table, around 85% of those arguments are sourced from either speeches from really high-quality tournaments or from expert debaters or from intermediate debaters. And in the last 15% is sourced from novice debaters, everyday people, etc. So it has a higher quality of arguments compared to if you want to just crowdsource all of your arguments. It has a diverse range of arguments. So instead of generating arguments on specific 30 or 40 motions that are picked before, we pick 24 themes based on our experience in the circuit, based on websites like Hellomotions.com, based on expert advice, etc. For every theme, we\\'ve captured as many motions as possible using the same sources. And this just creates a better diversity in terms of the motions that you\\'ll encounter in a parliamentary debate setting, rather than just pre-selecting a few motions. We added in an element of analysis instead of just keeping arguments. So analysis is not similar to claims or premises. It\\'s likely a combination of all of those things. And this is the term that we\\'ve introduced to the NLP community, so to speak. So I\\'ll quickly give an example of what this looks like. Analysis can be something like a premise if you just say that \"Educated people are 80% more likely...\" if you just give a statistic, for example. But it can also be a claim and a premise if you draw a link. So if you say that rich people send their kids to private school, which leads to them having better jobs and being richer, that\\'s you combining a claim and one premise, and making it into one analysis. You can also have analysis and multiple claims and premises, as this argument will tell you. So at that point in time, you need to see analysis as one coherent thing that explains the argument better. So the argument in this case is this idea that education is the basis of everything the person achieves Normally in datasets, you will just have a column for arguments. But we have expanded this, creating an idea of analysis which explains this claim better. So instead of just saying, \"The claim is that education is the basis of everything a person achieves\", we have pushed the burden to explain why that is true. So analysis just ends up being any combination of claims, premises, etc. Which is also why it needs to be introduced, because it\\'s nothing that exists in the general terms that exist in NLP. We have introduced an idea of instance-based annotator reliability. So, annotators have human biases about certain topics, right? So someone who experiences racism on a daily basis might have strong sentiments about those arguments, but probably is not biased about arguments when it comes to art. So we can still use their judgment in judging those motions. So instead of just eliminating annotators that may be unreliable for a few topics, instead of eliminating all of their judgments, we only eliminate their judgments that we think are biased. So when you capture annotation reliability on an instance-based level — so on a level for every argument — you are able to better utilize the annotations that you have. So we think that this dataset is a good use case for instance-based reliability, which is something that already exists. Lastly, we introduce something called a relevance model. So usually datasets will just have an argument connected to a specific motion. We think that motions are not singular in that way. Arguments like \"accountability is important\" can be used in debates about governments, churches, corporations, schools...anything. Accountability is important in all of those. Arguments that deal with the premise of free speech can be used to defend it for the LGBTQ community or defend people\\'s right to protest against a corporation. So, a relevance model simply assigns a score from 0 to 1 for each of these and for each theme, and has more arguments and better captures the relevance that each argument has to a topic. So we think that this dataset is just a culmination of a bunch of unique things. And at the end of it, you\\'re able to get something that is more diverse, that has a score for relevance, so you\\'re able to capture how relevant it is to a particular theme. You have a higher quality of arguments, and in general, you just have more reliable scoring because we captured an instance-based level. So do make sure to check out our paper, and give us your feedback on it. Thanks.'}\n",
      "{'id': '175', 'prompt_en': 'Answer the following question concisely given the English content: How does the method deal with the ambiguity of permutations?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie geht die Methode mit der Mehrdeutigkeit der Permutationen um?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo il metodo affronta l'ambiguità delle permutazioni?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该方法如何处理排列的不确定性？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '176', 'prompt_en': 'Answer the following question concisely given the English content: How is the fairness of a downstream NLP model defined?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wird die Fairness eines nachgeschalteten NLP-Modells definiert?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come viene definita l'equità di un modello NLP a valle?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何定义下游 NLP 模型的公平性？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '177', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '178', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '179', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/Kfipbqcswm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/Kfipbqcswm.mp4', 'text': 'Hi everyone. I am Melanie Sclar and I\\'ll talk about \"Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker\". Theory of mind is the ability to reason about the mental state of others. It is traditionally measured both in humans and in language models in reading comprehension tasks involving multiple characters. A great way of probing the understanding is through false-belief questions. These are situations where reality may not match the belief of certain story characters. Let\\'s look at a classic example of a theory of mind test, the Sally-Anne test. In the story, Alice and Bob are in a room where there is a basket and a box. Alice puts an apple in the basket and then leaves the room. Bob then moves the apple to the box. Then the human or the language model are probed with a bunch of questions. For example, where will Bob search for the apple? In the box? Where does Bob think that Alice will search for the apple when she comes back? In the basket. So questions may be classified as first-order or second-order depending on whose mental state you are asking about. First-order is asking about a character\\'s mental state, and second-order is asking about a character\\'s estimation of another character\\'s mental state. True-belief questions are those where the expected answer matches the true location of the object, and false-belief, otherwise. We know that large language models still perform poorly on false-belief tasks, for example ChatGPT or GPT-3. So our research question is, \"How can we improve Theory of Mind reasoning skills in Large Language Models?\" We present SymbolicToM, an inference-time method to improve Theory of Mind reasoning skills in Large Language Models using explicit graphical representations. SymbolicToM uses several graphical representations, since mental states cannot be represented with a single graph. For example, on the left we see a representation of what Bob believes is the current world state, and on the right we see a representation of what Bob thinks that Alice believes is the current world state. We call these graphs BBob and BBob,Alice. In general, we compute these graphs for all combinations of characters p₁ through pₘ up to a predefined maximum Theory of Mind level m. Graphs are computed using an inference-time algorithm that leverages off-the-shelf NLI and OpenIE models. See the paper for details. Having pre-computed these graphical representations for a given story, we can efficiently answer any given question. For example, let\\'s say we want to answer, \"Where does Alice think that Bob will search for the apple?\" We first detect the entities in the question. Then we retrieve the appropriate belief graph and perform recursion over the question so that now we\\'re asking a factual question over the graph. Then we retrieve the sentences captured by the graph and finally take the sentences plus the factual question and feed it to a language model to get the final answer. So now let\\'s look at the experiments. We test our method with a myriad of LLMs and compare it against supervised baselines, specifically a fine-tuned GPT-3 model and Textual Time Travel, which is a model specifically designed for Theory of Mind reasoning. We analyze in-domain performance in the well-known ToMi dataset, and we evaluate robustness with two out-of-domain setups that we designed. Let\\'s look at the in-domain performance results for second-order false-belief questions, and all others can be found in the paper. The X axis represents the out-of-the-box LLM performance, and the Y axis represents the performance using SymbolicToM for a given LLM. Then at any point in the upper left triangle means that using SymbolicToM increases the performance versus not using it. We see performance gains across the board, for example, 65 accuracy points gained for GPT3-Davinci, 67 for Macaw, 51 for Flan-T5-XXL, among many others. For testing our method\\'s generalization capabilities, we design two new datasets, both modifying ToMi\\'s benchmark. We test for storage structure generalization by, for example, concatenating two stories and asking half the time about the first story and half the time about the second one. This is the D₁ dataset, and we design two more, D₂ and D₃. We also test for linguistic generalization, which is our second dataset called ParaphrasedToMi, generating a dataset that has more linguistic diversity. This is important since ToMi is generated automatically with only one way of phrasing each sentence. For the story structure generalization datasets, we observed that supervised models heavily degrade the performance on the three datasets we created, for example showing around 50% performance in the dataset D₁ that I just described. On the other hand, using SymbolicToM still shows significant gains for all models, allowing stronger models like GPT-4 to fully solve the datasets, giving, for example, a 42 point accuracy boost for dataset D₁ . So in conclusion, we introduced SymboliToM, a plug-and-play method to improve Theory of Mind reasoning skills in large language models. It is an inference-time algorithm which avoids overfitting risk. It uses explicit graphical symbolic representations, which yields more interpretable reasoning, and SymbolicToM dramatically improves out-of-the-box LLM performance, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset, ParaphrasedToMi. For more details, please refer to the paper, and don\\'t hesitate to reach out to chat. Thank you so much for listening.'}\n",
      "{'id': '180', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '181', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '182', 'prompt_en': 'Answer the following question concisely given the English content: What does tropicalism indicate in the context of this paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was bedeutet Tropikalismus im Zusammenhang mit dieser Arbeit?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa indica il tropicalismo nel contesto di questo articolo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在本文的背景下，热带主义 (tropicalism) 意味着什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '183', 'prompt_en': 'Answer the following question concisely given the English content: How did the authors create the human-written portrayals of target groups?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie haben die Autoren die von Menschen verfassten Beschreibungen der Zielgruppen erstellt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo gli autori hanno elaborato le rappresentazioni umane dei gruppi target?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者是如何创建目标群体的人工描写？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '184', 'prompt_en': 'Answer the following question concisely given the English content: What was used to measure context usage in this work?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was wurde in dieser Arbeit zur Messung der Kontextnutzung verwendet?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa è stato utilizzato per misurare l'utilizzo del contesto in questo lavoro?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 本文中使用了什么来衡量语境使用情况？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '185', 'prompt_en': 'Answer the following question concisely given the English content: What is the difference between DrBERT and ChuBERT?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist der Unterschied zwischen DrBERT und ChuBERT?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è la differenza tra DrBERT e ChuBERT?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： DrBERT 和 ChuBERT 有什么区别？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '186', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '187', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '188', 'prompt_en': 'Answer the following question concisely given the English content: What is iterative transfer learning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist iteratives Transferlernen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cos'è il trasferimento iterativo dell'apprendimento?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 什么是迭代迁移学习？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '189', 'prompt_en': 'Answer the following question concisely given the English content: What is the goal of the dataset?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist das Ziel des Datensatzes?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è l'obiettivo del set di dati?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 数据集的目标是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '190', 'prompt_en': 'Answer the following question concisely given the English content: How can an attacker extract model parameters through an EaaS?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie kann ein Angreifer Modellparameter über einen EaaS extrahieren?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 攻击者如何通过 EaaS 来提取模型参数？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '191', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '192', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vJjjFtJzEm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vJjjFtJzEm.mp4', 'text': 'Hi everyone. Very happy to be here and give a short presentation today. I\\'m Yang Luo. Today I\\'m going to give a presentation on our work, \"CAME: Confidence-guided Adaptive Memory Efficient Optimization\". Nowadays, robust training of large language models often relies on adaptive gradient-based optimization methods. However, some widely-used optimizers like Adam always triple the required memory for keeping the first and second moment estimates of per-parameter gradients. Some existing memory-efficient optimizers like Adafactor have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. So here\\'s our challenge. How to design an optimizer to simultaneously achieve two goals: a fast convergence as in traditional adaptive methods, and a low memory usage as in memory-efficient methods. Here are some preliminaries. The first one is non-negative matrix factorization, which is a group of algorithms where a matrix V is factorized into two matrices. And we can find out that for an m x n matrix, NMF reduces the memory requirements from O(mn) to O(m +n), which is a huge memory reduction. And for NMF operation, Adafactor presents an analytic solution to achieve the minimum I-divergence between the matrix V and the approximation matrix W x H in the special case of rank-1 factors, which is shown below. However, the NMF operation in Adafactor will inevitably incur erroneous updates in the training of deep neural networks. Adafactor always converge slowly compared to Adam due to the existing error, which further limits the application scenarios of memory-efficient optimizers. And we introduced two scenarios to demonstrate how two types of erroneous updates are supposed to be handled in the ideal case. For example, in Figure (a), the difference between the momentum of updates (mₜ) and the current updates (uₜ) is large, illustrating the historical experience. For the update of the original, Adafactor contains a high level of errors that will inevitably influence the stability of the training process. And if we utilize the row mₜ to take an optimization step, the direction of optimization will diverge increasingly from the desired direction. And inspired by the erroneous update, we firstly consider an efficient approach to decrease the side effect caused by insecure updating. Given mₜ and uₜ, we take the residual between them as the instability in the preserved momentum and set generated instability as a denominator of original mₜ to more adaptively take an updating step. As compared to the original Adafactor parameter, we calculate the instability matrix (uₜ), and update the rₜ and cₜ in the same manner of Adafactor. With the approximation instability matrix (Sₜ) calculated, we apply the square root of it as the denominator for mₜ to take an optimization step. In the Experiments chapter, we perform experiments on the BookCorpus and English Wikipedia, and present extensive comparisons with existing optimizers on training tasks of three important large language models: BERT, GPT-2, and T5. As illustrated in the in the left picture, CAME achieves a significant improvement compared with Adam and Adafactor. To be specific, CAME increases validation accuracy with an increment of about 3.4% in comparison to Adafactor, using the same number of training steps. And apart from Adafactor, our proposed CAME achieves better performance than Adam in the pre-training of very large models, with a huge reduction of memory cost in respect to batch sizes getting larger from 8K to 32K. As shown in the right picture, CAME brings more enhancements to the training of BERT-Large in comparison with Adam and Adafactor. We also compare the end task performance of BERT-based models with the baseline on typical downstream tasks, and the experimental results demonstrate the efficiency of our proposed CAME optimizer by showing that BERT-based models trained with CAME on two batch sizes both achieve comparable performance to the baseline with less memory cost. Before that, we set the batch size to 1 to measure the memory usage of each optimizer more efficiently. As shown in the table, Adam and LAMB consume the highest amount of memory usage. Meanwhile, our proposed CAME optimizer reduced the memory footprint over the existing SM3 memory-efficient optimizer. And here, inspired by the erroneous update in existing memory-efficient optimizers, we propose a confidence-guided memory-efficient optimizer, CAME, which supports adaptive confidence-based updating guided by the residual between predicted update and the generated update. And extensive experiments show that our CAME achieves outstanding effectiveness on large language model training tasks. Moreover, CAME works well for large batch training, which serves an important extension for existing memory-efficient optimizers. That\\'s all. Thank you.'}\n",
      "{'id': '193', 'prompt_en': 'Answer the following question concisely given the English content: How many annotators were used to create the initial dataset?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti annotatori sono stati impiegati per creare il set di dati iniziale?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 有多少个注释者用于创建初始数据集？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '194', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '195', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/HxIUyoCdXZ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/HxIUyoCdXZ.mp4', 'text': 'Hello everyone. Today I will introduce our work, \"Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering\". Explainable question answering (XQA) aims to answer a given question and provide an explanation why the answer is selected. Recent work in XQA can be grouped into two directions. Neuro-symbolic methods, which translate natural language questions into formal representations such as SPARQL, and decompose-based methods, which generate natural language intermediate steps that lead to the final answer, such as decomposing a complex question into sub-questions and chain-of-thought. However, most of them have limitations. For neuro-symbolic methods, the formal representation can only be executed on structured KBs, but even the largest KB is incomplete; thus, the recall of answers is limited. For decompose-based methods, they only employ free-text corpora as the knowledge source, and the diversity of natural language makes XQA difficult. In fact, integrating knowledge from heterogeneous sources is of great importance to QA, especially for answering complex questions intuitively. Leveraging question decomposition is a promising direction since we can flexibly select the appropriate knowledge source for each sub-question. However, there are two main challenges. The first is how to determine the granularity of question decomposition, since certain complex questions can be directly answered with a knowledge source and further decomposition increases the possibility of error. For example, Q1 in the figure can be answered with the Wikipedia corpus without further decomposition. The second is how to find the optimal solution among various possible ones, since question decomposition and answering are both uncertainties. To this end, we propose a novel framework RoHT, Reasoning over Hierarchical Question Decomposition Tree. RoHT is a two-stage framework. Firstly, we propose to understand the hierarchical compositional structure of a complex question by building its Hierarchical Question Decomposition Tree (HQDT). In this tree, the root node is the original complex question, and each non-root node is the sub-question of its parent. The leaf nodes are atomic questions that cannot be further decomposed. Second, we propose probabilistic reasoning over HQDT to fuse the knowledge from a knowledge base and a text corpus at different levels of the training, and take into consideration the probability score of both string generation and answering. Specifically, to build the HQDT for a given complex question, we first use a question decomposer to generate the leaf loss, which are atomic questions. Then, we use another question generator to generate intermediate questions based on the grouped leaf questions according to their reference tokens. Besides, we also compute a certainty score of each node based on the likelihood to represent the certainty of its generation. After building the HQDT, we can solve the complex question. We are conducting probabilistic reasoning over the HQDT. The reasoning process is conducted, from the root to leaves, in a recursive way and contains three steps for each node. Firstly, a scheduler will determine the appropriate knowledge sources for this node. The sources include a KB, a text corpus, or solving its children recursively and sequentially. Second, the corresponding executors will get the answers with probabilities from the selected knowledge sources. Finally, an aggregator will aggregate the candidate answers from all the knowledge sources and output top key answers with the highest probabilities. We evaluate RoHT framework on two challenging complex QA datasets, KQA Pro and Musique. KQA Pro is originally a KB QA dataset to simulate the realistic case where KB is incomplete. We randomly discard 50% triples in the KB and take Wikipedia as the supplementary text corpus. Musique is originally a QA comprehension dataset. In addition to the given text paragraphs, we choose Wikidata as the supplementary KB. Here are the results. On the KQA Pro dataset, when only using the incomplete KB, our RoHT KB model outperforms existing KB QA methods, showing the benefit of integrating the answers of sub-questions of different levels. After adding Wikipedia as a supplementary text corpus, RoHT makes yields of substantial improvement compared with RoHT KB, demonstrating the effectiveness of utilizing knowledge from KB and text together. Our RoHT also outperforms TransferNet by a large margin, which is end-to-endly trained with a mixed relation graph, showing the superiority of explicit decomposition. On the Musique dataset, when only using the given paragraphs, our RoHT text model improves F1 by 11.9 compared with SOTA method EX(SA). With both text and KB, the performance of RoHT-mix is also remarkably better than TransferNet. Comparing our RoHT-text and RoHT-mix, we can also see some benefits of supplementing the text information with knowledge from KB. That\\'s all. Thanks for listening.'}\n",
      "{'id': '196', 'prompt_en': 'Answer the following question concisely given the English content: What is the example that the governor is on the left?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lautet das Beispiel mit dem Begrenzer auf der linken Seite?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è l'esempio in cui il governatore è a sinistra?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 以左侧为支配词的示例是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '197', 'prompt_en': 'Answer the following question concisely given the English content: What are the state-of-the-art models in dialogue systems?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist der Stand der Technik für Dialogsysteme?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i modelli all'avanguardia nei sistemi di dialogo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 对话系统中的最先进模型是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '198', 'prompt_en': \"Answer the following question concisely given the English content: Why do we need to evaluate the models' acceptability throughout the context window?\", 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Warum müssen wir die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Perché si rende necessaria la valutazione dell'accettabilità dei modelli nell'intera finestra di contesto?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 为什么我们需要在整个上下文窗口中评估模型的可接受性？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '199', 'prompt_en': 'Answer the following question concisely given the English content: Did training in multilingual fashion cause performance drop compared to monolingual English model?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Hat das mehrsprachige Training zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 与单语英语模型相比，多语言训练是否会导致表现下降？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '200', 'prompt_en': 'Answer the following question concisely given the English content: Do the annotators know about the entity in advance?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Kennen die Annotatoren die Entität im Voraus?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Gli annotatori conoscono l'entità in anticipo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 注释者是否提前知道该实体？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '201', 'prompt_en': 'Answer the following question concisely given the English content: Which MT metrics were used for the evaluation?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche MT-Metriken wurden für die Bewertung verwendet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali metriche di MT sono state utilizzate per la valutazione?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 评估使用了哪些 MT（机器翻译）指标？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '202', 'prompt_en': 'Answer the following question concisely given the English content: Does the regress in generalization impact specific NER types?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wirkt sich die Regression bei der Generalisierung auf bestimmte NER-Typen aus?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il regresso nella generalizzazione influisce su specifici tipi di NER?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 泛化中的回归是否会影响特定的 NER 类型？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '203', 'prompt_en': 'Answer the following question concisely given the English content: Why does positionality in NLP matter?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Warum ist Positionalität für NLP wichtig?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Perché la posizionalità nella NLP è importante?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 为什么 NLP 中的立场很重要？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '204', 'prompt_en': 'Answer the following question concisely given the English content: Were the multilingual LLMs like BLOOM fine-tuned with adapters or full fine-tuning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wurden mehrsprachige LLMs wie BLOOM durch Adapter oder eine vollständige Feinabstimmung angepasst?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Gli LLM multilingue come BLOOM sono stati affinati mediante adattatori o con una messa a punto integrale?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 像 BLOOM 这样的多语言 LLM 是采用适配器微调还是完整微调？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '205', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '206', 'prompt_en': 'Answer the following question concisely given the English content: Which model do they use for transfer learning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Modell verwenden Sie für das Transferlernen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: A quale modello fanno ricorso per il trasferimento dell'apprendimento?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 他们使用哪种模型进行迁移学习？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '207', 'prompt_en': 'Answer the following question concisely given the English content: Which are the recent test sets used to assess the PaLM capabilities?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche aktuellen Testsets wurden zur Bewertung der PaLM-Fähigkeiten verwendet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i recenti set di test utilizzati per valutare le capacità di PaLM?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些是最近用于评估 PaLM 能力的测试集？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '208', 'prompt_en': 'Answer the following question concisely given the English content: How many recommendations did the authors propose at last?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Empfehlungen haben die Autoren schließlich vorgeschlagen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti suggerimenti hanno proposto gli autori alla fine?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者最终提出了多少条建议？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '209', 'prompt_en': 'Answer the following question concisely given the English content: How much is the gain of the proposed method over the strongest baseline?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie hoch ist der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il guadagno del metodo proposto rispetto al metodo di riferimento?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 与最强的基线相比，提议的方法获得了多少收益？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '210', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '211', 'prompt_en': 'Answer the following question concisely given the English content: Can the results and dataset in the paper be used as a benchmark?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Können die Ergebnisse und der Datensatz der Studie als Benchmark verwendet werden?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: I risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 论文中的结果和数据集可以用作基准吗？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '212', 'prompt_en': 'Answer the following question concisely given the English content: How many smaller models do they experiment with in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Mit wie vielen kleineren Modellen wird in der Arbeit experimentiert?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti modelli più piccoli vengono utilizzati nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 他们在论文中进行了多少个较小模型的实验？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '213', 'prompt_en': 'Answer the following question concisely given the English content: Which model is used as the base model for investigating multi-model instruction tuning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Modell wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale modello viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪个模型被用作研究多模型指令调整的基础模型？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '214', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four datasets via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '215', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '216', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '217', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/pYJxpZSfts.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/pYJxpZSfts.mp4', 'text': 'Hello everyone. I\\'m getting to introduce our work here, \"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation\". I\\'m Weihao Zeng and I work with Lulu Zhao and Keqing He at the Beijing University of Posts and Telecommunications. Now I\\'ll talk about our works in the following seven aspects. I will introduce our motivations first. Previous methods for generating controllable dialogue focus on single attributes, ignoring the practical setting of multi-attribute generation. Methods for multi-attribute text generation combine controllers learned from single attributes with specific labels, but not for continuous attributes. The controllability of the CDG is limited by annotated data, and a unified evaluation metric is needed to explore it further. Our contributions are as follows. We explore compositional generation for multi-attribute controllable dialogue generation and found that existing models lack generation capability. We propose DCG, a Disentangled Controllable Generation that learns attribute concepts from seen values and uses the disentanglement loss to disentangle different attribute combinations. We introduce a unified reference-free evaluation framework, MAE, for different granularities of attributes. We establish two benchmarks and prove the effectiveness of our method and evaluation metrics through experiments. Our models, as shown in this figure, is based on the DialoGPT framework with the compositional prompt module. To effectively use control signals, we have designed two types of prompts that use attribute-related information from the pre-trained language model. The first is an attribute-oriented prompt. We use the combination of controllable attribute values corresponding to each instance as prompts to guide the model\\'s focus on the specific information in the dialogue. The second is a task-oriented prompt. Although attribute-oriented prompts capture the instance-specific control signals, the dialogue response generation task is also guided by the instance-independent global features. Finally, we concat the two prompt embeddings to create whole prompt embeddings. To improve the generation ability of our model and force it to distinguish between different attribute value combinations, we design some pseudo combinations to enhance the diversity of the prompts. A disentanglement loss is also introduced to train multiple compositional prompts while disentangling the combination representations. To address the lack of a matrix for multi-attribute controllable dialogue generation, we propose a unified and efficient evaluation framework that does not require additional large-scale labeled data. We designed a template that consists of discrete prompts such as emotion/act/persona controls the response [MASK]. To reduce the potential bias of different handcrafted patterns, we also add a trainable continuous dialogue-oriented prompt to improve stability and robustness. Shown are DailyDialog-CG controllable dialogue generation results for new attribute combinations. Our DCG outperforms all other baselines in attribute controllability and text equality. We test our model with attribute-oriented prompts, task-oriented prompts, and disentanglement learning. Results show that attribute-oriented prompts guide the model to focus on controllable information, while task-oriented prompts improve text equality, and disentanglement learning improves the ability of compositional generalization. In this table, the figures for DailyDialog-CG is compared to E-ACC and A-ACC controllability metrics as well as the BLEUs of the fine-tuning method control, and our DCG. Our DCG successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC. Additionally, our DCG outperforms CTRL on both controllability and text equality of unseen attribute combinations. This result confirms the effectiveness of our method for transforming seen attributes to unseen combinations. We use three correlation coefficients to evaluate the quality of different metrics, including our automatic metric, MAE. Compared to human judgments, our method outperforms classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes. We tested variants of MAE and found that removing the continuous prompts decreases the correlation scores since the task-oriented prompts are the only parameters that can be fine-tuned and are therefore important for MAE. We also implemented MAE on another PLM, BART, to show its generality. We demonstrated the impact of prompts on compositional generalization with a visualization of the concatenated prompt embeddings of three attributes via PCA on DailyDialog-CG, as shown in this figure. This result proves that our method can disentangle attribute combinations and learn the relations between different attributes with the ability to generalize from seen attributes to unseen combinations. Our proposed attribute-oriented prompt method outperforms models that learn an independent prompt for each attribute value. The shared embedding mapping helps learn attribute concepts from seen values to unseen combinations. This is the conclusion. We studied the compositional generalization for multi-attribute controllable dialogue generation and propose a prompt-based disentangled controllable dialogue model.'}\n",
      "{'id': '218', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '219', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/pKbRuaSOYV.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/pKbRuaSOYV.mp4', 'text': 'Hi everyone. I\\'m Jia-Huei Ju, a research assistant at Academia Sinica. I will present our work, \"A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports\". This work is done with Yu-Shiang Huang, Cheng-Wei Lin, and our advisors Professors Che Lin and Chuan-Ju Wang. I will talk about the background of financial report analysis, which is the goal of this work, and the task definition and approaches. In this work, we consider the Form 10-K as our target corpus, which is an annual report required by the SEC. It contains many details of companies\\' important activities. However, mining useful information requires lots of human efforts. This work was motivated by two observations. First, we observed that the words in the company\\'s report are very similar—about 80% of tokens are the same— and the contents are yearly-dependent. This figure illustrates the text similarity between two reports in continuous years: for example, the report in 2018 is similar to the one in 2017. Based on the observation, we introduce a highlighting task and a multi-stage pipeline. Following the motivation, we first define the reference-to-target structures in our task. The target and reference refer to the report of our interest, and the report at its previous year. So basically, a highlighting model should compare and contrast the context between targets and reference, like this picture. The goal of this highlighting task is to find the rationale (words) of relations between a given pair, T and R. Formally, the model will predict the word importance, and therefore, we can measure the performance of highlighting. For example, the word \"decrease\" is supposed to have higher importance in this context. This is our proposed pipeline. Stage 0 is document segmentation. Stage 1 is the relation recognition. Stage 2 and Stage 2+ are out-of-domain and in-domain fine-tuning. Due to the time limit, I will not talk about Stage 0; more details can be found in our paper. For Stage 1, we classify all the pairs into three types. Type β refers to the pairs that have the highest syntactic and semantic similarities. These pairs frequently appeared, such as companies’ regulations. Revised pairs have similar syntactical patterns, but in fact, the two segments disclose very different meanings. Mismatched pairs are more like a debut of information or a company\\'s new operations. For the model tuning stage, we first use an external dataset, eSNLI, for out-of-domain fine-tuning; it is a natural language inference dataset with token annotation. For example, the word \"frowning\", is the rationale according to the context of this pair. For intermediate fine-tuning, we use the revised pairs, the revised words as pseudo positive labels, and we randomly label a few other scores as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross-entropy laws and KL divergence. Therefore, we can alleviate the problem of low-quality pseudo-labels. The evaluation dataset includes eSNLI pairs and our released FINAL dataset. We use two metrics to judge the performance. Precision indicates the precision over recall. PCC means the correlation between prediction and annotations. This table shows that our domain-adaptive highlighting model achieved the best performance on FINAL and even preserves the generalization capability, as you can see in the performance of eSNLI. We further observe that our methods can benefit on simulation with the mismatched pairs, which we didn\\'t use during training. In conclusion, we proposed a highlighting task with our released FINAL dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try, including improving effectiveness or adding more features or many other techniques in information retrieval, which can enhance the application as well. That\\'s it. Please refer to our paper and GitHub for more details, and feel free to ask us if you have any questions. Thank you.'}\n",
      "{'id': '220', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '221', 'prompt_en': 'Answer the following question concisely given the English content: Which language pairs were analyzed in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Sprachpaare wurden in der Arbeit untersucht?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali coppie linguistiche sono state analizzate nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 论文分析了哪些语言对？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '222', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MRUOlPljnN.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MRUOlPljnN.mp4', 'text': 'Hi everyone. The title of this work is \"To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering\". To motivate this work, let\\'s look at this question: What is produced in the plants of Narora, Kakrapur, Tarapur? So in Open-domain QA settings, we need to first look up relevant passages from a document corpus — in this case, which is Wikipedia — with some retriever model. Then, a reader model takes the question and all the relevant passages as input to generate the answer as \"nuclear power\", which is the correct prediction. Please note that both the retriever and the reader models are trained on a general-purpose domain like Wikipedia. Now let\\'s say we want to answer a biomedical question. In that case, Wikipedia corpus can probably answer a few domain-specific questions since it\\'s a general-purpose domain. We could also replace Wikipedia with a biomedical corpus, but that may not be sufficient, as these domains are often very sparse, and the source model is trained on Wikipedia corpus. It is not exactly clear how to enable transfer. The most naive thing to do would be to use both Wikipedia and biomedical corpus together. So let\\'s say we expand the document collection to also include biomedical documents. Ideally, this should not affect the prediction of the question it was doing already well on. However, when we retrieve passages, we see that the top passages focus on words like \"plants\". As a result, the reader gives an incorrect response. Since the source model was not trained with biomedical passages, it was unable to discriminate well between them and got confused. In this work, we make three main contributions. First, we investigate different data interventions that would be useful in enabling out-of-domain generalization in open-domain QA. Second, we identify the type of dataset shift a new domain exhibits. And finally, we determine what kind of data interventions are effective for a specific type of shift. Our setup looks something like this, where we have a general-purpose Wikipedia-based source domain on which both Reader and Retriever are trained. And to test the generalizability of the source model, we consider these seven target passage and datasets spanning across six different domains. First, we want to investigate different data interventions. There are two overarching methods we look at for generating these interventions, which are zero-shot and few-shot. In case of few-shot methods, we use few examples from target domain to prompt large language models for generating more examples. We do this by asking the model to suggest a fact they learn from the passage. For instance, here we have an example from biomedical domain where we say that after reading this article, a doctor said, \"Malnourishment, tuberculosis, and silicosis are comorbid conditions in children who live close to stone mines.\" This sentence was manually constructed based on a given QA pair in the target dataset. Now, we asked the model to generate some fact, given a new passage. Then we convert the generated sentence into a cloze-style question for further adapting retriver and reader models. Overall, we observed that the retriver performance improves by 8% on average, while reader performance improves by 11% on average. For zero-shot techniques, we don\\'t have access to any examples from target domain, unlike few-shot. And what we want to do basically is control the interactions among three random variables in open-domain QA, which are question, answer, and context. We do this by keeping two variables fixed while we vary the other one in a controlled manner to understand its impact on model learning. To vary the question, we considered standard WH format and cloze-type questions for adapting the two models. We observed that change in format does not really affect model performance; however, cloze-style questions are much easier to curate than standard WH questions. To vary answer distribution, we first perform named entity recognition on target corpus and use the entity types as categories to sample from. Then we sample answer spans from these entity or answer type categories like location, person, numbers, etc., and generate cloze-style questions based on the sample answer spans. We find that uniform distributions that covers all types of answers equally works best. To vary context distribution, we pull passages from source and target domain. We compare existing retriver models and find that the learn methods are sensitive to change in distribution, while an unsupervised method like BM25 has the best overall performance. Now moving on to the second part, how do we ascertain the nature of incompatibility the target model and domain exhibits? We consider existing data shift taxonomy in machine learning to understand the nature of shift in target datasets with respect to source model. So here, \"No shift\" is when both source retriever and reader are compatible with the target domain. Now concept shift happens when retriever is able to find the right passage, but the type of reasoning the source reader needs is not actually the same as the one the source model has learned. Covariate shift is the reverse of concept, where the retriver has a hard time identifying the right passage, but when the reader gets the right passage, it\\'s able to answer the question. Full shift happens when neither retriever nor reader are compatible. Now we need some way to measure compatibility. So what we do is we take some fixed number of question/answer and context triples from target dataset and then compute the likelihood, let\\'s say likelihood the source retriver model assigns to all contexts in the set. Now the probability assigned to the cold context is considered as a measure of compatibility. The higher is this value, the more compatible is the source model for the target dataset. For reader, we do a similar thing where we compute the likelihood of all the answers in the fixed set for each question and then normalize over all answer likelihoods. To get a single value, we just average the compatibility values over all examples in the set. Now that we have our compatibility measure, we can basically map different target datasets onto this 2D grid and essentially estimate the type of dataset shift. Which means that datasets like CliCR and NewsQA in the bottom left corner are incompatible for both retriever and reader and exhibit full shift, while SearchQA on the top right exhibits no shift. Now, if we take datasets under each category of the shift and find what type of data intervention would be most useful for them, we find that all target sets respond well to few-shot adaptations as they use a few examples from target domain. While datasets with concept and covariate shift respond well to zero-shot adaptations as well. As in the case of no shift, we don\\'t observe a lot of changes in performance because source model already understands the target domain to a great extent. To conclude, we experiment with a variety of data interventions and improve reader performance by up to 24%. We also show that only certain types of data interventions are effective based on the type of shift a target dataset exhibits. Thank you.'}\n",
      "{'id': '223', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '224', 'prompt_en': 'Answer the following question concisely given the English content: Which models were investigated during the experiments?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Modelle wurden während der Experimente untersucht?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali modelli sono stati studiati durante gli esperimenti?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在实验过程中研究了哪些模型？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '225', 'prompt_en': 'Answer the following question concisely given the English content: From the 62 diverse tasks used in MultiInstruct, how many tasks are used for training and testing purposes?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele der 62 verschiedenen Aufgaben in MultiInstruct werden für Training und Tests verwendet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Delle 62 diverse attività utilizzate in MultiInstruct, quante di queste vengono utilizzate per scopi di addestramento e test?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '226', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '227', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/NJhfGcPbsg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/NJhfGcPbsg.mp4', 'text': \"We all know that language models have achieved great success recently, providing a general solution to many different NLP tasks. Here we want to ask what's missing in current language models’ research. We think the answer to this question is grounded language understanding, which basically means grounding a natural language expression into something that can be executed over a specific target environment, which is also referred as a plan or a program, so depending on whether it's described by a program language. There are many applications to grounded language understanding, such as smart assistants like Siri and Alexa. There's also semantic search on Google and querying a medical database using natural language, and also domestic robots that follow natural language instructions. So all these applications involve mapping a natural language expression onto some representation in a specific environment. For example, when querying a medical database using natural language, we need to map the query onto a SQL query or SQL problem, while for the robotic scenario, we need to map the natural language instructions into a sequence of actions the robot can take. But why is grounded language understanding challenging? The main reason is the lack of grounding during pre-training. We know that most of the language models, including the recent large language models, are mostly pre-trained with textual corpus and without grounding. So this is also related to the octopus test, and the gap between the pre-training and the downstream application makes the task of grounded language understanding particularly challenging for language models. Existing research on different grounded language understanding tasks typically uses language models to directly generate a plan via autoregressive decoding that's capitalizing on the generation ability of language models. However, the main issue is that the generated plan or program may not always be grammatical or valid. For example, in the scenario of knowledge-based question answering, it's very likely a KB query generated by T5 is not executable over the KB, or maybe returns an empty set of answer entities. But in our work, we propose a novel framework for grounded language understanding where we let language models focus on discrimination instead of generation. We argue that discrimination is something much easier for language models to excel. So, specifically in our framework, a symbolic agent interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. So in this case, the language model does not need to handle the validity and grammar of the target plan by itself because it does not need to do the generation by itself. For more details, please refer to our paper, and we always welcome offline discussions. So we name our framework after Pangu, who is the primordial being in Chinese mythology who separates heaven and earth, just like how our framework separates the newer world and the symbolic world. We instantiate our idea on knowledge-based question answering, which is a typical scenario with a massive, heterogeneous environment for grounded language understanding. Note that our framework is generic and is not specific to knowledge-based question answering. Here we just use it as a representative testbed. We experiment with language models of different natures, including BERT, T5, and also large language models like Codex. Also, we experiment with both fine-tuning and in-context learning. Pangu achieves outstanding performance across all settings. Here's the results for fine-tuning, and also here's the results for in-context learning. In particular, Pangu also demonstrates strong sample efficiency. For example, when using Codex with in-context learning, Pangu can achieve an accuracy of over 50% of GRAIL query with only one demo example, significantly outperforming all other settings. And also for fine-tuning experiments, Pangu consistently outperforms a baseline model, ArcaneQA, in terms of sample efficiency when using different language models. We also have an interesting finding that may explain Pangu's strong generalizability under non-i.i.d. settings, as specifically we observe that autoregressive models like ArcaneQA tend to overfit seen structures during training. While for Pangu, we can see that for both the seen structures and unseen structures, the distributions of probability are almost the same, so this might be a signal for Pangu's strong robustness under the non-i.i.d setting. Here comes the most important message and the one-sentence takeaway of our work. For grounded language understanding, generation may not be a good idea. Instead, discrimination might be a much better strategy of using language models for grounded language understanding. We are always open to different forms of discussions and collaborations. And also, we are eager to hear your thoughts on our work, and we appreciate your time and your attention. Thanks a lot.\"}\n",
      "{'id': '228', 'prompt_en': 'Answer the following question concisely given the English content: Which datasets did the authors experiment on?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: An welchen Datensätzen haben die Autoren experimentiert?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su quali set di dati gli autori hanno effettuato i test?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者在实验中使用了哪些数据集？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '229', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/VqvkOTtAoi.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/VqvkOTtAoi.mp4', 'text': 'Hello everyone. I\\'m Gabriella Skitalinskaya, and today I\\'m going to present our joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. Let\\'s start with a brief introduction into text revisions and why they are important. Text revision is an essential part of professional writing and is typically a recursive process until somehow optimal phrasing is achieved. From the author\\'s point of view, finding the right words and expressions is especially important in argumentative writing as it can directly influence the effect the text has on the audience. As such, optimal phrasing is essential for effectively communicating a message in order to stimulate a desired reaction. To get a better understanding of what this process can look like, let\\'s follow the revision process of the argumentative claim found below, stated as \"Cell phones cause brain cancer\". In the first revision, the author further specifies that it\\'s cell phone radiation that causes brain cancer, and in the following revision, the claim is further clarified that it\\'s not definite, and it has changed to \"Cell phone radiation may cause brain cancer.\" But how to know whether an argumentative claim is phrased well enough and no more revisions are needed? Our paper focuses on answering this question that puzzles so many novice writers. To formalize what we want to learn, we introduce two new tasks formulated as follows. Task 1: Suboptimal-Claim detection. We\\'re given a claim, and we decide whether it needs revisions or it can be considered phrased optimally. And Task 2: Claim Improvement Suggestion. Where, given a claim, we need to select the types of quality issues that should be improved when revising the claim. When thinking about how to tackle this problem, one may intuitively think, \"Why not learn directly from human patterns of revisions instead of explicitly defining what makes a claim good or bad?\" In this work, we explore the challenges that arise when working with such revision-based data, as different domains have different goals, different notions of quality, and subsequently different revision types performed. In this paper, we exclusively focus on argumentative text. As such, we explore how to best model the quality of argumentative text based on implicit revision patterns found in collaborative online debate platforms such as Kialo. On the slide below, you can see an example of how such information on the quality of claims can be extracted from revision-based data. There are two revision histories where the final versions are considered as optimal (colored green), and all the predecessors (colored red) are suboptimal and in need of a revision. While working with revision-based data provides many opportunities, it also comes with several challenges that arise at different stages of the experiment design process. In the paper, we delineate the main challenges originating from the nature of revision-based corpora and from the notion of argument quality. Specifically, we explore four challenges. The first one, Representativity and Reliability, focuses on how to compile a reliable dataset from claim revision histories that represents arguments that have claimed quality well. It addresses such questions as \"Is the final version of a claim truly optimal and cannot be further improved, or was it simply overlooked by the community?\" The next challenge we explore is Model Complexity and Architecture for the tasks at hand. It is important that the selected model aligns with the idea of revisions and is sensitive to such small changes in delivery. In our experiments, we explore models with various architectures and complexities and try to disentangle how pre-training, fine-tuning, and the final classification affect the performance of claim assessment. The third challenge stems from the fact that certain argument quality dimensions may be dependent on contextual information. However, determining what kind of context is relevant to the decision-making process is an open question. For example, some claim provisions may be typical for the debate as a whole, and for example, related to a desired structure, layout, style of citations, or choice of words for the main concepts. Others may depend on the parent claim, which is supported or opposed by the claim in question, and effects whether further clarifications or edits improving the relevance are needed. And potentially, even general domain knowledge may be proven useful. The final challenge we look at is Topical and User Bias. Collaborative revision histories contain noise and it could be due to accidental mistakes or biases of users and moderators. Not only can debate topics be highly controversial and require specific knowledge and expertise, but also certain argument quality dimensions, such as for example effectiveness, depend on the social and cultural context of the writer and audience, which makes determining the quality of the text even more difficult. To find out how we tackle each of the four challenges, we invite you to read our paper, where we present a detailed analysis of the strengths and weaknesses of strategies tackling each challenge and a systematic comparison of approaches for the introduced tasks. Based on our experiments, we can conclude that revision-based data can be employed effectively for the given tasks. Moreover, modeling the distance between two claimed versions is beneficial for detecting suboptimal claims. And finally, the impact of contextual information is dependent on both the task and the quality issues a text is suffering from. For further details and findings, please refer to our paper and thank you for your attention.'}\n",
      "{'id': '230', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '231', 'prompt_en': 'Answer the following question concisely given the English content: What is NACHOS?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist NACHOS?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cos'è NACHOS?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 什么是 NACHOS？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '232', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '233', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '234', 'prompt_en': 'Answer the following question concisely given the English content: How much does the prompting strategy impact the results?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welchen Einfluss hat die Prompt-Strategie auf die Ergebnisse?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che misura la strategia del prompting influisce sui risultati?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 提示策略对结果有多大影响？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '235', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '236', 'prompt_en': 'Answer the following question concisely given the English content: What are the 5 expert-written instructions?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lauten die 5 Anweisungen der Expert*innen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le 5 istruzioni scritte da esperti?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 5 个由专家编写的指令是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '237', 'prompt_en': 'Answer the following question concisely given the English content: What do the authors propose to test the models on using information from multiple sources?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie schlagen die Autoren vor, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa propongono gli autori per testare i modelli sull'utilizzo di informazioni provenienti da più fonti?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者建议如何使用来自多种来源的信息来测试模型？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '238', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/VXYpTPydNl.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/VXYpTPydNl.mp4', 'text': \"Hello, welcome to this video. My name is Yebowen Hu from the University of Central Florida. In this video, I'm going to present a new benchmark dataset, MeetingBank. Have you ever found yourself in a meeting desperately trying to note down every key point? In our fast-paced world, meetings are happening every day for different purposes, which result in an emergent need of various datasets to develop summarization technologies for different reading domains. To create this dataset, we addressed two major challenges. First, high-quality meeting summaries or scores. Second, it's not an easy task to locate trustworthy resources for public meetings. Ultimately, we managed to create a repository of City Council meetings, MeetingBank. All data we collected will be released in this dataset, including meeting transcripts, reference summary, and other URLs, which contain a variety of useful resources. First, allow me to guide you through the process of our data collection. As a start, we are using Speechmatics API to convert the audio data to transcripts. Then open the meeting website. This example is from the Boston City Council. We can first identify the type and data of the meeting from the information provided below the video. This information will be converted into a MeetingID that uniquely represents that particular meeting. We use this selected ItemID to locate the corresponding reference summaries from the meeting minutes. Then we use the same ItemID to obtain the corresponding meeting segments, including the start and end time. As the final step, we align the timestamps to get the second transcript and pair it with the previous extracted summary. Our dataset includes a total number of 1,366 City Council meetings and nearly 7,000 instances. In our dataset statistics, from left to right, we counted a number of meetings, meeting duration, number of tokens per meeting, number of speakers per meeting for each city, and the year period of meeting collected in our datasets. We also provide a number of summarization instances gathered for each city, as well as the average number of sentences and tokens in both source and summary texts. Average statistics for both meeting-level and secondary levels are posted across all cities. For data analysis, we measure the level of abstraction in meeting summaries by two common measures, coverage and density. The coverage score matches the percentage of some summary words that appear in source transcripts. The density score evaluates how much a summary can be characterized as a set of extracted references. As shown in this here, the coverage score of most City Council meeting summaries are between 0.7 to 0.9, indicating the summaries often include verbatim points over abstraction. Analysis of density scores reveals Seattle and Boston with the highest scores, while Denver scores lowest. This can indicate that a high degree of editing is performed. For model evaluation, we evaluated top-tier summarization system on the test set of MeetingBank. Our extractive summarizers include Oracle, LEAD, LexRank, and TextRank, and we're using the five best-performing neural abstractive summarizers for our task. This includes BART-Large, Pagasus, Longformer, DialogLM, and HMNet. We evaluated BART-Large with and without fine-tuning our dataset and compared the results to other models. We leverage our advanced capability of large language model GPT-3. Specifically, we conducted zero-shot summarization prompting with Davinci-003 on our test set. By comparing these ten different systems, we have several interesting observations. First, for extractive system, Extr-Oracle yields a high ROUGE-2 score, which indicates that the content of reference summaries mostly comes from the source transcripts, and an extractive summarization system could be promising. Unsurprisingly, the DialogLM, which is designed for long dialogue summaries, yields the highest ROUGE-2 score among abstractive models. Finally, GPT-3 does not perform well according to automatic metrics. But here we have interesting findings in the human assessment. To get an overall evaluation, we conduct experiments with not only the traditional metrics but also use new metrics like BERTScore and MoverScore. Further, we also use question/answering-based metrics. Ultimately, we carried out the human evaluation, in which we asked human annotators to assess the quality of each system-generated summary based on five criteria: informativeness, factuality, fluency, coherence, and redundancy. We have randomly selected 200 instances, and each includes source segments, transcripts, and summaries generated by several models. We recruited three experienced evaluators in the United States to perform 5-point Likert scaling. Due to the time constraint, I want to present one interesting finding: that GPT-3 achieves the highest overall scores. It shows exceptional performance in terms of fluency and coherence. However, these results are less impressive in terms of informativeness and factuality. These findings suggest that meeting summarization solutions should continue to focus on capturing the main discussion points, and a new method of automatic evaluation metrics should be developed to better align with human preference. In conclusion, our primary contribution is the creation of MeetingBank. This is a benchmark dataset constructed by segmenting meetings and pairing these segments with expert-written summaries. This dataset serves not only as a useful tool for researchers to design advanced meeting summarizers, but also an interesting dataset which provides intriguing insights into the decision-making process of City Council. At the end of this video, I encourage all of you to make use of this resource. Feel free to download it and play with it. Looking forward to further discussion with you in July. Thank you.\"}\n",
      "{'id': '241', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JJuRCeungt.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JJuRCeungt.mp4', 'text': 'Hi everyone. I\\'m Ethan, and today I\\'m going to discuss our paper, \"Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments\". This was a joint work with Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech. There have been many proposed approaches for automatically detecting misinformation on social media platforms. However, all of these approaches generally fall short on two key marks. Firstly, these systems are often unrealistically evaluated. For example, datasets used for the evaluation of these systems are often retrospectively constructed, instead of using live data. Relatedly, there is the possibility of leaked counter-evidence, which a recent work found was a problem with many of these systems. For example, for this evidence-based fact-checking approach, we see that while in this first case, counter evidence could clearly be found on a well-known data source like Wikipedia. For the second, more realistic case of misinformation, counter-evidence could only exist after the claim had been debunked publicly, at which case the system is not useful. Particularly, solving this deficiency is critical given the recent rise of misinformation in order to detect rumorous claims early, before they have the chance to spread. Secondly, these methods are often not human-centric. Specifically, they are not representative of the real scale or noisiness of these platforms, which necessitate the involvement of human content moderators. These methods tend to either completely cut out humans from the misinformation detection process or relegate them to the final determination step, instead of seeking input from them throughout the intersection process. We propose an evaluation framework for the development of systems that address these deficiencies. Such systems are end-to-end as they go from the noisiness of raw tweets on Twitter to actionable outputs used by humans. They have well-integrated human feedback as humans are involved at various stages of the process. The system itself is meant to be a system rather than authoritative. We also can concretely implement and evaluate our novel workflow for COVID-19 treatment misinformation. Our concretely instantiated system has two main components. The first component deals with the detection of misleading claims. Here, the system takes raw tweets as input from the wild and outputs discovered check-worthy claims. First, keyword filtering is used to filter relevant tweets. A T5 model is then trained for question answering and is used for claim extraction. Specifically, the model is trained to answer the question \"What is the mentioned COVID-19 cure?\" given the context of a particular tweet. These treatments are used to form claims of the type; for example, Ivermectin is effective in screening COVID-19. The claims are then ranked by trendiness, which is essentially a statistical popularity on a given day using Fisher\\'s Exact Test, before they\\'re provided to a human for verification. The second component focuses on policy violation verification, but the goal is to use the verified list of misinformed claims in the first stage to flag social media policy violations. A BERT-based stance classification model is used to determine the author\\'s stance in the tweet towards the unapproved treatment. For example, in the first tweet, the author clearly believes that Ivermectin can be used to effectively treat COVID-19. Such supporting stance tweets are then flagged for human review. Now we move on to the evaluation of our human-in-the-loop workflow. Through some of the discussion of loop counter-evidence, early detection is an important task to get right, as humans can act more effectively to stem the spread of misinformation when they know about rumors earlier. In our case, we operationalize early detection as the detection of an unapproved treatment before its first appearance in the debunking news article, which is discovered and annotated by humans. In this figure, you can see some selected examples of unapproved treatments that are detected by our system before they were debunked in the news. We believe that our definition of early detection captures the real utility of early detection systems. We also evaluated the efficacy of the policy violation verification portion of our workflow. Specifically, in our evaluation, humans assign a Likert scale value as to if a tweet is a violation of Twitter\\'s policies surrounding COVID-19 misinformation. The histogram of scores are shown below, where scores of four or five indicate most likely are clearly violating Twitter\\'s policies. At a high level, we find that our system has a position of 65% with reguards to policy violation detection. Additionally, to understand the human workload of such systems, the computer metric for the number of policy violations that can be confirmed per human hours worked, which includes both the claim verification and policy violation verification steps. For our instantiated system, we find that 124.2 policy violations can be detected per human hour worked. To conclude, our framework more realistically captures the complex interplay between systems and human content moderators in a realistic end-to-end setting. We also hope that our work motivates the development of future human-in-the-loop misinformation detection systems, which can now be evaluated consistently with the methods that we present in this paper. Finally, our work valuably provides outsiders an out-of-industry look at the development and evaluation of misinformation detection systems. Thank you so much for listening, and we hope to answer any questions you may have at the conference.'}\n",
      "{'id': '242', 'prompt_en': 'Answer the following question concisely given the English content: What are common evaluation methods for dialogue systems?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind gängige Bewertungsmethoden für Dialogsysteme?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono i metodi di valutazione comuni per i sistemi di dialogo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 对话系统的常用评估方法是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '243', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '244', 'prompt_en': 'Answer the following question concisely given the English content: In the example with Servin and Kea, what background knowledge is needed?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Hintergrundwissen wird im Beispiel mit Servin und Kea benötigt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Nell'esempio con Servin e Kea, quali conoscenze di base sono necessarie?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些背景知识？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '245', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MntHtJyGXr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MntHtJyGXr.mp4', 'text': 'Hi. I\\'m Lining Zhang. Today I\\'m going to present our work \"A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization\". Below are our co-authors The picture in the middle shows a two-step pipeline for finding high-agreement Amazon Mechanical Turk, called MTurk Workers. The motivation of the pipeline is that automatic metrics are sometimes problematic, and the best practices for recruitment on MTurk are also poorly understood. So we are going to present this work as shown in the outline here, starting with the \"Qualification Settings\", then the two steps of \"Qualification Task\" and \"Endurance Task\". We are also going to include the Reference-based task and contents about the Baseline and CloudResearch MTurk Workers, as well as the analysis of correctness across annotation sources. Finally, we are also going to include the conclusion and limitations. So for the MTurk qualification settings, pre-task qualifications on workers can be set, such as location, number of Human Intelligence Tasks (which we call HITs), and HIT Approval Rate. For the first stage of the qualification task, it tests the annotator\\'s ability to evaluate multiple dimensions correctly. It includes both the training parts and the qualification part. The qualification questions include three documents, one with an attention check, and each with one summary to evaluate six dimensions. Based on the result, we categorize workers into four types: gold, silver, bronze, and block, but only the gold and silver workers can pass this task. This results in 26 MTurk workers, 8 gold, 18 silver, which is 13% of 200 participants qualified for the following task. For the second stage of endurance tasks, it tests the annotator\\'s capacity for handling heavy workload. It includes 10 HITs, one document, and four summaries each in terms of the saliency dimension. This results in 12 MTurk workers, 4 gold and 8 silver, which is 6% of 200 participants who passed this task. They can achieve high agreement in terms of IAA, which is inter-annotator agreement, than experts. The figure on the right shows the Cohen\\'s Kappa across groups, and the best Krippendorff\\'s Alpha is 0.443. For the reference-based task, it\\'s designed to test the general performance on the true annotation task. It includes 30 HITs, one reference, and four candidate summaries each to check the information coverage in two directions. Our pipeline workers\\' results show that 8 out of 12 MTurk workers finished all HITs, and the figure on the right shows the Cohen\\'s Kappa between different workers, nd the Krippendorff\\'s Alpha is 0.534. As for the Baseline MTurk workers, we try different approaches, and the best one is achieved by the statistical filter called MACE. This threshold of 0.5 Krippendorff\\'s Alpha is 0.380, but with incomplete HIT coverage and fewer workers per HIT. We also include the CloudResearch MTurk workers, which are recruited from the platform to recruit high-quality annotators. And the Krippendorff\\'s Alpha is 0.513, but with lower task acceptance rate. In addition, we also perform the analysis of correctness across annotation sources, as shown in the heat map, on 50 random samples from the reference-based task. We find that Pipeline and CloudResearch workers had a significant Spearman\\'s correlation. However, Pipeline may not guarantee the training of correctness. Real GPT models correlated well with expert judgments. In summary, pre-task filtering of our pipeline can avoid a waste of time and resources and achieve high agreement at a lower cost. It can also have similar quality to CloudResearch. For the conclusion, our pipeline results in 4 gold and 8 silver workers, which is 6% out of 200 participants. It also serves as the best practice for high-agreement annotations at large scale and lower cost, and can avoid resource waste on discarded annotations. In the future, we are going to investigate ways to hire high-quality workers, both in terms of high agreement and correctness. And we are going to try multiple applications for tasks, languages, and platforms. There are also some limitations for this work. First, only English summarization on MTurk platform is tested. Second, the designed questions are not \"panacea\" solutions. Third, there is no guarantee for the training of correctness. Finally, we want to thank Google for the experiment fundings. And thanks for listening.'}\n",
      "{'id': '246', 'prompt_en': 'Answer the following question concisely given the English content: Is the code available, and if yes, where?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Ist der Code verfügbar, und wenn ja, wo?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Il codice è disponibile? In caso positivo, dove?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 代码是否公开？如果公开，可在哪里获取？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here\\'s an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you\\'re interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.'}\n",
      "{'id': '247', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/FHHyKHgcbZ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/FHHyKHgcbZ.mp4', 'text': 'Hello. This is Jiho Kim from KAIST AI, and I will present our paper titled \"FACTKG: Fact Verification via Reasoning on Knowledge Graphs\". Do you know of existing fact verification datasets? Their datasets, such as FEVER and VitaminC, which use Wikipedia text, or TabFact and InfoTabs, use tables as evidence. However, there was no dataset that utilized knowledge graphs as evidence with natural language claims. So we propose a new task, Knowledge Graph-Based Fact Verification. KG is a valuable knowledge source due to the two reasons. First, we can do a reliable fact verification. In text- or table-based cases, even if evidence is presented, additional interpretation is required to reason the claim. However, in KG-based, as you can see, the evidence is intuitive, and it is possible to connect directly to the claim. Therefore, reliable reasoning is possible. Also, it can be used practically. For example, modern dialogue systems communicate with internal knowledge graphs and use or update information. At this time, it can be used to check the consistency between the user\\'s words and the knowledge graph. As such, it can be used in all tasks that require a consistency check between KG and natural language. To this end, we introduce a new dataset, FactKG, fact verification via reasoning on knowledge graphs. The knowledge graph used is DBpedia, and claims exist in two styles, written and colloquial, for more practical use. There are two labels: SUPPORTED and REFUTED. And the task consists of retrieving the evidence from DBpedia and verifying the claim using the evidence. Also, it includes five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. There are five types of reasoning we used. First, one-hop claims can be represented by one triple. To verify one of the claims, we should check that the two entities appearing in the claim are connected to one relation. For example, in this case, the claim is supported when AIDAStella and Meyer Werft are connected by the relationship builder. Second, conjunction claims can be represented by multiple triples, and they include multiple one-hop claims. To verify these claims, all one-hop claims should be checked. For example, two claims, AIDA Cruise line operated the AIDAStella, and ADAStella was built by Meyer Werft, should be verified. Third, existence claims can be represented by one triple. To verify these claims, we should check that the entity in the claim is connected to the specific relation. In this example, we should check whether the entity \"Meyer Werft\" has the relation \"parent company\". To verify these [FOR:UNK 3:41] claims, multi-hop inference is required because some entities do not appear in the claim as they are. In this claim, we should check the path from the AIDAStella to Papenburg. For negation, even after finding graph evidence, it is required that verify one more inference for negation. Our dataset includes claims in colloquial style as well as written style for practical use. Two methods were used for this. First, the colloquial style transfer model (proposed Kim et al.) is used. This is an example. Second, presupposition templates were created and used. Presupposition is what it usually says, assuming that it is true or false when we speak. We use this concept, and these are the examples. This is the statistics of our dataset. Finally, we\\'ve constructed some baselines in two ways. Claim Only baselines use only the claims to verify, without graph evidence. And we utilize the GEAR model to verify the claim using correct evidence. And as a result, all of the baselines outperform the majority class baseline, which is 51%. And the GEAR model that uses graph evidence outperforms all other baselines. This is the summary of our paper. Thank you for listening. You can download our dataset, and feel free to contact me.'}\n",
      "{'id': '248', 'prompt_en': 'Answer the following question concisely given the English content: Are the annotators for NLPositionality balanced in regard to each demographic, i.e., country, gender, etc.?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Sind die Annotatoren für NLPositionality in Bezug auf jede demographische Gruppe, d. h. Land, Geschlecht usw., ausgewogen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico, ad esempio Paese, genere, ecc.?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： NLPositionality 的注释者在各个人口统计学特征（即国家/地区、性别等）方面是否均衡？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '249', 'prompt_en': 'Answer the following question concisely given the English content: How were sentences perturbed in the acceptable domain?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurden Sätze innerhalb der akzeptablen Domain durcheinander gebracht?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo sono state perturbate le frasi nel dominio accettabile?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何在可接受的域中扰乱句子？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '250', 'prompt_en': 'Answer the following question concisely given the English content: What does it mean to have a dimensional evaluation?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was bedeutet eine dimensionale Bewertung?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa significa avere una valutazione dimensionale?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 进行维度评估意味着什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '251', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '252', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/gmButXKCEL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/gmButXKCEL.mp4', 'text': 'Welcome to our presentation. My name is Sai Kiran Tanikella. I am a master\\'s student at IIT Kanpur. I\\'m excited to present our work \"U-CREAT: Unsupervised Case Retrieval using Events extrAcTion\". This is a joint work along with Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. Legal professionals, such as lawyers and judges, have traditionally relied on their experience to cite relevant past precedents, known as cited documents. However, with the increasing volume of cases, it becomes very challenging. This is where the task of Prior Case Retrieval comes into play. Given a legal query document, the Prior Case Retrieval Task involves retrieving relevant candidates from a candidate pool. These retrieved candidates should be both relevant to the query document and are cited within it. Here is an example showing query and candidate documents. Please note that relevance in the legal domain is mainly about similar factual situations. In this work, we have made two key contributions to the field of Prior Case Retrieval. These contributions are the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR Dataset, which stands for Indian Legal Prior Case Retrieval Dataset, is a new benchmark for PCR tasks. It is a collection of 7,070 legal cases with 6.775 average citations per query document. This provides a comprehensive test bed for assessing the performance of PCR algorithms. Here is a comparison between COLIEE’21 and IL-PCR datasets. COLIEE’21 is an existing dataset for Canadian legal documents. Note that, on average, IL-PCR has a larger pool of cases containing longer documents, a larger vocabulary, and more number of citations. We have created the IL-PCR Dataset using publicly available data. For more details, please check out the paper. Our next contribution, the U-CREAT pipeline, leverages the unsupervised learning techniques and introduces an event-based approach for PCR tasks. It demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. In the context of our work, event extraction plays a crucial role. If we consider a case document to be narrative about how things developed, it can be represented as a collection of events. To extract events, we utilize a dependency parsing technique using spacing. In this example sentence, we can see the dependency graph along with entity tags and parts-of-speech tags. By focusing on verb category, we examine the left/right children for different subjects and objects. We form the subject-verb-object triplets based on these relationships. Here each triplet is called an event. In the U-CREAT pipeline, the query document and candidate documents are sequentially given as input into the event extraction block for further processing. The event extraction block consists of three steps: pre-processing, dependency parsing, and post-processing. For each document, we obtain the extracted events. Then we compute an interaction matrix between the query and candidate events. The green blocks represent the common events. Further, the interaction matrix is used in different retrieval models to obtain a ranking order of the candidates. Here is an example of how the matching events are obtained. In our work, we conducted experiments using a diverse range of models to validate and compare their performance on the PCR task. These models are categorized into three groups: the count-based models, which are applied on the word level; the transformer-based models; and finally, the event-based models, which provide various approaches for improving PCR performance. Let us examine each grouping individually. We present the best-performing models within the grouping. The scores for the baseline methods are shown in the top row for reference. In count- and transformer-based models, we experimented with both pre-trained and fine-tuned transform models including BERT, DistilBERT, and DistilRoBERTA. The performance of transformer-based models falls drastically compared to baseline methods like BM25. We also introduced two legal transformer models specifically trained on Indian legal text, InCaseLawBERT and InLegalBERT. However, their performance in the prior case retrieval task was found to be significantly lower compared to the previous transformer models. This highlights the complexities and nuances of the legal domain and the need for the tailored approaches. Finally, we experimented with event-based models. In the Atomic Events model, each event is treated as an atomic unit. That means if two events differ at even one token, they are treated as different. In the Non-Atomic Events model, each word in an event is considered as a separate unit. For the Event Filtered Documents model, we filter the original corpus so that it contains only the sentences which produce matching events with other documents. Note that all event-based models perform significantly above the baseline. The Event Filtered Docs is the best-performing model. For more experiments and results, please check out the paper. In terms of performance, the method using Event Filtered Documents outperforms all other methods with a significant boost. This graph compares the performance of various event-based methods with the baseline. From this plot, we can observe that event-based models have lower inference times and higher F1 scores compared to the other techniques. Here is a performance comparison on the COLIEE data set. U-CREAT outperforms the existing approaches, including the recent supervised approach by the MTFT-BERT team. To the best of our knowledge, this approach is the current state-of-the-art method for the COLIEE’21 document retrieval task. In conclusion, with these significant contributions, U-CREAT opens up avenues for further exploration and development in the field of prior case retrieval. Please check out our paper for more details. That\\'s all from our side. Thanks for watching.'}\n",
      "{'id': '253', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UIRGWoJlxm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UIRGWoJlxm.mp4', 'text': 'Hello everyone. My name is Mario Ezra Aragón, and I\\'m going to present our work named \"DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media\". This is a group effort of researchers from Mexico and Spain. First, I want to start with the definition of a mental disorder, that is, a psychological syndrome which is associated with distress and disability that affect your thinking, feeling, mood, and behavior. There are different types of mental disorders, for example, major depression, PTSD, bulimia, and anorexia, among others. Nowadays, social media content is massive and provides an opportunity to do research on how people undergo difficulties. Many people use online platforms to publicly share their daily routines and important events, while others take advantage of the anonymity of these spaces to explicitly discuss mental health issues and seek help. In this work, we aim to contribute to the detection of mental health disorders by automatically analyzing their social media posts. This type of analysis is expected to support a new technology able to warn about the onset of mental disorders and provide supporting evidence. Why use domain adaptation? Sometimes we have insufficient annotated data and want to improve the performance of a model on a target domain. For this, we can use the knowledge learned by a model from another related or similar domain. Take, for example, BERT. BERT is a language model trained from the general data of Wikipedia and Google Books, and we want to train it to a more specific language of Reddit and mental health. Doing this adaptation, we can adjust the vocabulary of the semantic understanding of the model and learn the domain-specific task. Here we have the general diagram of our proposed approach. First, we start with the base language model and then integrate information from Reddit and mental health. We also incorporate the knowledge from a lexicon to guide the masking process. The main idea of our proposed approach is to first learn the social media language and then specialize in the mental disorder domain. Using the guided masking, we want the model to try to focus on important words during the training process. Here we have the general results using the eRisk datasets. We plot the precision and recall of our model and the baselines. Our model tends to locate in the main diagonal of the region, indicating its good balance, while other methods have a high precision of recall but score low in the other dimension. Let us illustrate the behavior of the learned model and the kind of textual segment it tends to pay more attention to. First, we analyze the most likely words the models generate when given a sentence with masked words. As sentence, we use an example from Beck\\'s Depression Inventory. This clinical tool, which consists of 21 items, aims to identify and measure the severity of typical symptoms of depression. For example, it measures the mood, pessimism, sense of failure, dissatisfaction, and guilt, among others. In the following table, we can see some examples of these sentences and the answers returned by BERT and DisorBERT. With DisorBERT, the answers tend to have a more negative meaning or psychological orientation compared to BERT. Take, for example, the sentence \"I used to be able to cry,\" where we mask the word \"cry\" and DisorBERT predicts words like focus, talk, breath, sleep, and eat. These words are related to common problems that are associated with mental disorders and cause an interference in the thinking and behavior of the affected person. Here we also present all the words predicted by both models, weighted by their frequency. These figures result from applying the two models to the entire set of 21 BDI items. Similar to what happened before, BERT tends to generate more general words, while our model tends to be biased toward words related to mental disorders. Finally, we want to visualize the most important sequences of the text by obtaining the most relevant words and sentences. For this, we use a visualization tool that provides an interactive head view in the form of a graph. We select a depression user with the highest score in the BDI questionnaire and compute the attention scores of the user\\'s post. We can observe that the most prominent words are related to \"anxious\" and \"medication\", topics that are highly relevant to depression. As conclusion and future work, the combined effect of double domain adaptation and guided masking is effective at capturing signs of mental disorders in social media interactions. Our approach also obtained better results than those achieved by MentalBERT, a model trained with a large amount of data. The evaluation shows a solid balance between finding users and labeling them correctly. In future work, we want to explore the application of different lexical resources as well as using clinical data. Thank you for your attention. If you have any questions, please feel free to ask me.'}\n",
      "{'id': '254', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/sZtUHdoYXQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/sZtUHdoYXQ.mp4', 'text': 'Hello everyone. Today I\\'m going to present our research work, \"Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction\". I\\'m Sun Qi from Nanjing University of Science and Technology. The document-level relation extraction is aimed to extract relations among entities in a document. It can be seen as this figure. Previous methods rely on the large-scale human-annotated corpora, which is time-consuming and labor-intensive. So recent work leveraged distantly supervised data to pretrain the document-level relation extraction models for better performance. As we know, these data contain various noise levels. Current efforts to alleviate the noise problem by using pseudo labels. However, these kinds of methods still persist the risk of noise induction by false-positive pseudo labels, as shown in this figure. When we rely just on the pseudo label, we will obtain an extra false relation, \"composer,\" and lose the correct relation, \"place of birth\". So how to mitigate the noise caused by the pseudo labels is still a challenge. In this paper, we propose a document-level relation distant extraction framework with uncertainty-guided label denoising to improve the label quality of DS data. This is the overview of our framework. We first train a pre-denoising DocRE model with both DS and human-annotated data to generate the pseudo labels. Since false pseudo labels are inevitable, we introduce uncertainty estimation to determine whether model prediction can be trusted or not. Considering there might be multiple relations between an entity pair, we propose an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations. We also designed a re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost the performance. Uncertainty estimation is an important technology for misclassification detection, out-of-distribution instance detection, and active learning. In order to model the uncertainty in pre-denoising DocRE model, we introduce the Monte Carlo dropout technology in the DocRE task. This method requires multiple stochastic forward-pass predictions with activated dropout to capture the model uncertainty. Previous works based on MC dropout calculate the uncertainty score of the model prediction as this formulation. However, the previous method is not suitable for the overlapping relation problem, as shown in the left figure. There are two different types of relations between this entity pair. It is hard to separate the false positive pseudo label, \"composer\", and the correct positive pseudo label, \"director\" by the previous uncertainty estimate methods. To solve this issue, we modify the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. The calculation can be seen in this formulation. We observe that the distribution of uncertainty score for each relation class is different. Moreover, it can be observed that frequent classes usually contain lower average uncertainty than the long-tail class. So we propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. The calculation can be seen in this formulation. Then we replace the original DS label with the pseudo label that contains a lower uncertainty score than its class uncertainty thresholds. In order to take full advantage of the DS data for further boosting the performance of DocRE model, we design a multi-phase training strategy to iteratively re-label the DS data, which is shown in this algorithm. We compare our framework with several strong baselines onto public datasets. As shown in this table, our framework outperforms the previous baselines on both two datasets. In conclusion, the main contribution of our work are summarized as those four points. The first one is our framework with uncertainty guided label denoising, which greatly improves the label quality of DS data. The second one is the instance-level uncertainty estimation method for overlapping relations. The third one is the iterative re-label strategy with dynamic class uncertainty thresholds for the long-tail problem. The last one is the great performance improvements.'}\n",
      "{'id': '255', 'prompt_en': 'Answer the following question concisely given the English content: In which cases, if any, is the form of the prompting important?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: In welchen Fällen, sofern zutreffend, ist die Form des Prompts wichtig?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In quali casi, se presenti, la forma del prompting si rivela importante?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在哪些情况下（如有），提示的形式很重要？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '256', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '257', 'prompt_en': 'Answer the following question concisely given the English content: Which dialog models did the authors evaluate?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Dialogmodelle haben die Autoren evaluiert?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali modelli di dialogo hanno valutato gli autori?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者评估了哪些对话模型？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '258', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/elHYcrZOkR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/elHYcrZOkR.mp4', 'text': 'Hi everyone. I\\'m Chiang Cheng-Han. Thank you for watching this video. In this video, I\\'m going to talk about our new work, \"Can Large Language Models Be an Alternative to Human Evaluation?\" In this work, we propose to use large language models to evaluate the quality of text in natural language processing. So we just give the large language models the instructions and use the instructions to instruct the models to evaluate the samples, and we give the large language models the samples to be rated. Hopefully, if the large language models can really understand the instructions, they can provide the ratings through the output. And there are some related works on this topic. For example, G-Eval also uses large language models to evaluate the quality of samples in other natural language processing tasks. So you might think that using the large language model for evaluation is not very novel, and it\\'s a natural and widely adopted method. But in fact, when we submitted this paper to ACL, there are no prior works that explore the idea of large language model evaluation. So, our idea was quite novel at the time of submission. And the motivation of this work is, in the past, we often use human evaluations to evaluate the quality of samples. And just like what we do in large language model evolution, we give the human evaluators the instructions, and we give them the samples that need to be rated, and the humans can rate the samples based on the instructions. But in fact, human evaluation is very unstable, and the process of human evaluation is very hard to reproduce. So we ask ourselves a question: Is there any alternative to human evaluation that can achieve the same goal as in human evaluation, but do not have the drawbacks of human evaluation? So we turn to large language models. Large language models, as their name indicates, are the language models that have a lot of number of parameters, and there are some large language models that have been shown to be able to follow natural language text instructions. So we think, \"Why don\\'t we just use natural language instructions to instruct the large language models to evaluate the samples in natural language processing?\" And since the large language models can understand the instructions, they may be able to perform large language model evaluation just like humans can perform human evaluation. So, in order to verify that large language model evaluation is really useful and it can provide meaningful rating results, we conduct the following experiment. We use large language model evaluation to rate the stories generated by either GPT-2 or written by humans. And we ask the large language models to rate the stories based on four attributes: grammar, coherence, likability, and relevance. So we just provide large language models with instructions and the inputs, as shown in this figure. And large language models will provide their ratings through the output, and we parse the output to get the ratings from the large language model. And in order to verify that large language models really generate meaningful results, we need some kind of ground-truth ratings to be compared with. And for the ground-truth rating, we use human evaluation results. And in this human evaluation, we also use the same instructions and same stories as in large language model evaluation, and we hire English teachers to read the stories. The reason that we hire English teachers in this task is because English teachers are familiar with scoring their students\\' essays, so they can be considered as experts in this task. In this experiment, we use four different large language models: T0. InstructGPT (and there are two kinds of InstructGPT, curie and davinci), and ChatGPT. And the overall result is that human raters, the English teachers, prefer human-written stories than GPT-2-written stories. But some smaller large language models do not show meaningful preference toward human-written stories. But still, luckily, we find that there are two large language models, Davinci and ChatGPT, that show a clear preference toward human-written text, just like English teachers. So there are indeed some large language models that can be used as an alternative to human evaluation in this task. So until now, I have introduced the main idea and the main supporting experiments in this paper. And you may have more questions. For example, you may want to know if large language models and human evaluators agree on the individual ratings of each story. Or you might want to know, \"What if we change the wordings in the instructions?\" Or what if we change how we sample the responses from the large language models? Will these factors affect the result of large language model evaluations? Or you might want to know, \"What is the benefits and the cost of using the large language model evaluation compared to human evaluation?\" And last, you may also want to know the results of large language model evaluation on other tasks. And all these questions are answered in our paper. So if you are interested in this topic, please read our paper, or you can come to our poster stand at ACL. That\\'s all for this video, and thank you for listening.'}\n",
      "{'id': '259', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '260', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four dataset [INAUDIBLE 4:39] PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.\"}\n",
      "{'id': '261', 'prompt_en': 'Answer the following question concisely given the English content: What are the ideal qualities of a good planner?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind die idealen Eigenschaften eines guten Planers?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le qualità ideali di un buon pianificatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 优秀规划器的理想品质是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '262', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '263', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/CEQmOPEoZZ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/CEQmOPEoZZ.mp4', 'text': 'Hi. Today I\\'m going to present our work, \"Mitigating Label Biases for In-context Learning\". In-context learning is a popular paradigm for utilizing large language models. However, such in-context learning ability is known to be unstable due to various design choices, such as the choice and order of in-context examples. Prior work shows that search instability of in-context learning arises from that, and these various design choices introduce biases to the models’ predictions. However, there hasn\\'t been any systematic discussion to categorize existing findings on bias problems in in-context learning and to detect new types of biases and, more importantly, to mitigate the effect of different type of biases. So in this work, we aim to address these problems within the context of text classifications. We start with a typology of label biases and based on which we\\'re able to identify a new important type of bias, which is domain-label bias. And finally, we proposed a novel calibration method to handle all types of biases. So for any classification task, we consider the following setups where we have a context that consists of a sequence of labeled examples. We have the text we want to classify, and also we have some label names we want the model to predict. And this can be summarized into the following equations. We see that there are three different components and we can define three different type of label boxes accordingly. So, first we have vanilla-label bias, which captures the models’ uncontextual preference for label names. And we have the context-label bias, which captures the effects from the context. And finally, we conceptualize a new type of bias, which we call domain-label bias, that captures the effect of the task corpus on the model\\'s predictions. So to confirm that the task corpus can actually bias the model\\'s predictions, we conduct the following experiments. We\\'ve given the model some random words and ask the model\\'s preference on each of the label names. We find that on many of the datasets, simply seeing random in-domain words that are sampled from the task corpus can severely bias the model\\'s predictions, but seeing random English words do not show such preference. And on many other tasks, seeing random English words or random in-domain words does not show such a big difference. We found the model performed quite differently on these two different types of tasks. First, on the task with small domain-label bias, the in-context learning performs actually quite well, and given some advanced calibration methods, the model can even perform better. But on the task with large-domain label bias, it\\'s quite a different story. We see that on this task, language models can barely outperform a chance-level baseline, and this is even true with some prior calibration methods. To handle domain-label bias and the other types of biases, we propose domain-context calibration. It follows prior calibration work to mitigate the effect of all different label biases in a holistic fashion. Specifically, it uses some content-free text to estimate the model\\'s bias on each of the label names and then use this estimated bias to calibrate the model\\'s original predictions. Prior calibration methods use some single predefined content-free tokens, such as \"not available\"e as the content-free text, which has been shown to be able to mitigate the effect of vanilla-label bias and the context-label bias. And our domain-context categorization, on the other hand, uses random in-domain words sampled from the task corpus as the content-free text. This, on the one hand, still is a text that is nearly content-free due to the randomness but also takes into account the domain-label bias by using random in-domain words. And to test the effectiveness of our domain-context calibration, we conduct the experiment using different models on a wide range of datasets and using different random seeds. We first observe that domain-context calibration improves significantly the average performance of in-context learning on this dataset. And then if we take a closer look by categorizing all of these datasets according to their domain-label bias level, we observe a larger improvement of using domain-context calibration on this task with larger domain-label bias. And if we take a further closer look to see the actual predict distribution, we see that after domain-context calibration, the model has better decision boundaries. And all of these findings holds for larger models like GPT-3. To understand why domain-context calibration is better than previous calibration attempts, we conduct comprehensive calibration studies. So first we observe an improvement by replacing the single pre-defined tokens, like \"not available\" to a random English word, showing that this pre-defined token can also be biased. And then, instead of using a single word, using more random words leads to further improvements, showing that it is sub-optimal to use only a single content-free token. And finally, by using random in-domain words rather than random English words, to take into account the domain-label bias, we\\'re able to achieve further large improvements. So to summarize, in this work, we propose a systematic investigation of the label bias problems of in-context learning. We start with the typology of label biases and based on which we are able to identify a new important source of biases in in-context learning. And finally, we propose the calibration method that is able to significantly improve the performance of in-context learning of those large language models. Check our paper for more details, and thank you.'}\n",
      "{'id': '264', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/CPfTgsOEan.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/CPfTgsOEan.mp4', 'text': 'Hi everyone. My name is Lin Wang, and I\\'m a graduate student at Zhejiang University, China. Today I will give a presentation for my paper titled \"TAVT: Towards Transferable Audio-Visual Text Generation\". Let\\'s get started. Currently, uni-model text generation tasks like machine translation and image caption have already flourished as a result of the large-scale pre-training and huge model capacity. However, for multimodal text generation tasks like audio-visual text generation, data annotation is more arduous and expensive. And existing works inevitably suffer severe degradation due to varying construction conditions in different domains. To break through the constraint, we propose a novel task named Transferable Audio-Visual Text Generation. The main challenge of this task is the multi-modal domain shifts like visual style, audio energy, and so on. Also, there are multimodal domain shifts that have different characteristics. We notice that in the content understanding for the same event, the visual content would shift significantly with the change of image style and shooting angle. But the change of audio content such as rhythm and energy hardly affects the understanding of the events. Based on these phenomena, we posit that a unified audio semantic space can be used to align visual concepts across domains. Next, I would like to present the overall framework. We aim to train a model that can learn and quickly adapt to new multimodal domains with limited labeled data. The framework that we present is modular and consists of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The first model is the audio-visual meta-mapper network, which can map different visual concepts across domains into a unified auditory semantic space, and as well as addressing shifts in the semantic distribution. We collect a lot of audio clips on the Flickr dataset and cluster those audio clips using k-means and use the resulting set of audiocast as a unified audio semantic space. Inspired by prompt learning, we introduce a set of learnable tokens. Those tokens are called visual prefix for the audio cluster. We generate a set of probability distribution in the audio space for reconstructing the audio using visual content as a query, and improve the semantics of those tokens by optimizing the occurrence of the restructuring audio and force the model to align the visual content with the audio space. Moving on to the second model, we used the transformer-based encoder and generator. In particular, we introduced an alpha to evaluate the contribution of different modalities to each word. At time step t, α-t is computed by measuring the relevance between the cross-attention of each modality and the previous words. Finally, we introduce the loss function and training detail of our framework. Although the reconstruction-based paradigm provides a constraint for AVMM, it cannot directly optimize the visual-audio alignment scores. Therefore, we propose a Dual Counterfactual Contrastive Learning (DCLL) which constructs fine-grained supervision signals from counterfactual results to directly optimize the visual-textual alignment without relying on the quality of the randomly-selected negative samples. And here is the meta-training details, similar as in MAML; to manage the model, we randomly select K – 1 specific domains in D as a support set and the remaining domain as a query set. In the meta-test stage, we consider a new domain target, which also has the support set for fast adaptation by fine-tuning the model meta-parameters, theta, to a given domain and query set Dq to evaluate the model on this domain. In the experimental section, to fully evaluate our proposed approach, we build two benchmarks based on MSVD and MSR-VTT, including cross-datasets and cross-domain settings. Next, I would like to present to you the main results of our experiments. To the best of our knowledge, there is no work investigating transferable audio-visual text generation. So we first choose the SOTA approach, including the RNN-based and the transformer-based models. For a fair comparison, source models are all trained on the Dmeta-train with the same meta-learning framework as TAVT and tested on the target domain. As it can be observed, our methods outperformed all compared models on all metrics by a large margin on both cross-datasets and cross-domain settings. For some low-resource domains with only a few labeled data, such as \"Kids\" and \"Beauty\", other methods suffer from severe performance degradation, while TAVT still performs well. Additionally, we conducted ablation experiments to analyze the impact of audio features on expanded performance Ok, that\\'s all. Thank you.'}\n",
      "{'id': '265', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '266', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '267', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '268', 'prompt_en': 'Answer the following question concisely given the English content: Which are the most common errors of PaLM?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches sind die häufigsten Fehler von PaLM?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono gli errori più comuni di PaLM?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： PaLM 最常见的错误是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It\\'s trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it\\'s important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings. It\\'s crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It\\'s the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it\\'s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that\\'s it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.'}\n",
      "{'id': '270', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '271', 'prompt_en': 'Answer the following question concisely given the English content: What does CFT stand for in this paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wofür steht CFT in dieser Arbeit?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa significa CFT in questo articolo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在本文中，CFT 代表什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '272', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '274', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '275', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '276', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/gvfdxsExlF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/gvfdxsExlF.mp4', 'text': 'Hi everyone. This is Ananya and Vignesh, presenting our work on \"IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages\". For the translation task, there are several evaluation metrics proposed for evaluating to-English translations. Also, there are many studies that perform meta-evaluation of these metrics by analyzing their correlation with human scores or discussing the advantages and shortcomings of each of them. But what about translations in the other direction? Evaluation of translations in the other direction is understudied. But it is important to study evaluation metrics for other languages instead of naively adopting the metrics proposed for English. Other languages have their own grammar rules, some novel aspects like shared and borrowed vocab, dialectic differences in certain settings, different sentence structure, and varying amounts of resources, among other differences. Our aim is to fill this gap. So in this work, we focus on Indian languages. More specifically, we study five languages belonging to two different language families. We have Tamil and Malayalam, which are Dravidian languages, and Hindi, Marathi, and Gujarati, which are Indo Aryan languages. So from the Flores dataset, we select 200 sentences randomly. To generate multiple candidate translations for each of these source sentences in English, we feed it to seven different translation models or APIs. So using seven such translation systems, we get a total of 1,400 candidate translations for each language. Now, across the five languages, we get a total of 7,000 samples. For collecting human annotations on these 7,000 samples, we want a rich level of detail to be captured in the annotation of each translated output. So we employ bilingual expert annotators to carefully evaluate the outputs by showing them the source sentence along with the candidate. We ask the annotators to mark each error along with its type and severity. We also asked the annotators to provide an overall score for the output. The various error types are borrowed from the MQM framework. Error types can be broadly classified into accuracy or meaning-based errors, fluency errors, and some special category errors. The annotators were also asked to indicate a specific subcategory of error. You can find more details on these in our paper. This is what an example annotation looks like. Here we have the output from Google API. It has three errors highlighted by the annotators. By changing the color of the text for the incorrect bits, the corresponding type of error for each marking is indicated. Also, the corresponding severity for each error is indicated. This figure shows the total number of errors per model. Recent MT models like NLLB or Indic Trans have fewer errors compared to relatively older models like CVIT. This table shows the average score of each system computed as the mean of human scores obtained on all outputs from that system. The best-performing models in descending order are Indic Trans, NLLB, Google API, Bing API, mT5, CVID, and mBART. The correlations Pearson and Kendall-tau between MQM-based scores and metric scores for 1,400 segments per languages are shown. We observe that out of the overlap-based metrics, chrF has the highest correlation across all languages, but overlap-based metrics are the worst-performing overall. Among the embedding-based metrics, LabSE embedding sees better correlations than other counterparts. The correlation improves further when we use BERTscore with embeddings obtained from different multilingual models. The results are mixed, with MuRIL showing the better correlations on average. Finally, COMET-metric variants have the highest overall correlations for all languages. The correlations of some metrics are good, but we observe a skewed range of scores. Many metrics tend to provide scores in a narrow range and do not utilize their full scoring range. In contrast, the human scores for Tamil cover the entire scale. Most metrics exhibit a skewed spread, including SacreBLEU, which has a scoring range of zero to 100, but predominantly yields lower scores. This makes it challenging to interpret the metric scores effectively. Splitting the dataset based on the error types shows a more nuanced picture. We select annotated segments containing errors of only one category, either from fluency or accuracy. This gives us two MQM data subsets, one for fluency errors and the other one for accuracy errors. This figure contains the correlation values of various metrics on fluency and accuracy subsets for all languages. Almost all metrics show a higher correlation with human scores when only accuracy errors are annotated. Having analyzed various metrics, we fine-tuned the best-performing metric, COMET, using our MQM dataset. The table here compares the correlation values of our fine-tuned IndicCOMET variants with the COMET baselines. We observe that IndicCOMET MQM outperforms the COMET baselines on three out of five languages and shows higher correlations than COMET MQM across all languages. In order to test the zero-shot ability of IndicCOMET on other unseen languages, we fine-tune on only four languages and test on the unseen one. The table contains the comparison between the best-performing IndicCOMET variant—that is, IndicCOMET MQM— and COMET baselines. We observed that an IndicCOMET outperforms both the COMET baselines, so the majority of languages. We then evaluate robustness scores on the ACES Translation Accuracy Challenge Sets, and we see that IndicCOMET MQM has a correlation score of 0.36 and is more robust than the COMET counterpart, which has a score of 0.272. So thank you, and please feel free to use our publicly available dataset, and have a good day.'}\n",
      "{'id': '277', 'prompt_en': 'Answer the following question concisely given the English content: If the new method has a name, state that name, otherwise state that it does not have a name.', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wenn die neue Methode einen Namen hat, geben Sie ihn an, andernfalls geben Sie an, dass sie keinen Namen hat.', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Se il nuovo metodo ha un nome, lo indichi, altrimenti dica che ne è privo.', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如果新方法有名称，请注明该名称，如果没有，请注明它没有名称。', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '278', 'prompt_en': 'Answer the following question concisely given the English content: What was the author\\'s description of the \"marked words\" method?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie beschreiben die Autoren die Methode der „markierten Wörter“?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è stata la descrizione dell\\'autore del metodo in relazione alle \"parole contrassegnate\"?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者如何描述“显性词汇”(marked words) 方法？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '279', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '280', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/LVIpHbZSwf.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/LVIpHbZSwf.mp4', 'text': 'Hi everyone. I\\'m Shi Tao. Today it\\'s my great honor to share my work \"MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations\", First, I would like to briefly introduce the task of emotion regulation in conversations. The goal of emotion regulation in conversations is to predict the emotion label of each utterance in the dialogue. Each utterance has its corresponding textual, audio, and visual modalities. To solve the problem of ERC, a lot of methods have been proposed. Most existing ERC methods focus on modeling speaker and contextual information, while there still exist several unsolved challenges. First, the complementarity of multimodal information has not been well exploited. Most existing ERC methods focus on the textual modality of utterances or simply utilize feature concatenation as the multimodal fusion mechanism, which is inadequate. Second, current state-of-the-art methods have unsatisfactory performances in minority emotion classes. And finally, the difficulty of distinguishing between semantically similar emotions. To address the above problems, in this paper, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO. The overall framework of MultiEMO is illustrated in Figure 3, which consists of four key components: unimodel feature extraction, context modeling, multimodal fusion, and emotion classification. The main contributions of this work can be summarized as follows. First, we propose a novel visual feature extractor named VisExtNet. Second, we design a multimodal fusion model called MultiAttn based on bidirectional multi-head cross-attention layers. Third, we introduce a Sample-Weighted Focal Contrast loss to address the difficulty of classifying minority and semantically similar emotion classes. Finally, we conduct extensive experiments on MELD and IEMOCAP and achieve state-of-the-art performances on both datasets. Next, I\\'ll introduce each contribution in detail. First, existing ERC visual feature extractors encode both facial expressions of interlocutors and scene-related information of each utterance clip. However, we argue that this approach is problematic because visual surrounding information is redundant for two reasons. First, there are no explicit correlations between scene information and the emotion of the speaker. Second, the scene normally remains unchanged throughout the conversation. Therefore, capturing simulating information is not necessary, and may lead to a wrong understanding of the speaker\\'s emotion due to the influence of irrelevant information. To address this problem, we propose a novel visual feature extractor named VisExtNet, which is made up of an MTCNN and VGGFace2 pre-trained ResNet-101, which captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. Our second contribution is MultiAttn. Existing literature fails to effectively integrate multimodal information. To address this problem, we propose a novel multimodal fusion network named MultiAttn, which is made up of three components: MultiAttn-text, MultiAttn-audio, and MultiAttn-visual, each of which aims to integrate one modality with complementary information from the other two modalities through stacked bidirectional multi-head cross-attention layers. Next, I\\'ll illustrate how multimodal fusion works through MultiAttn-text. MultiAttn-text first learns cross-modal correlations between textual and audio modalities by treating the textual modality as Query and audio modality as Key and Value for the bidirectional multi-head cross-attention operation. Second, the learned output from the first stage is utilized as a new Query while the visual modality is regarded as Key and Value for another bidirectional multi-head cross-attention layer to fuse the textual modality with visual cues. Finally, a feed-forward network is adopted which operates as a key-value memory. In addition, we employ residual connection and layer globalization over the output of each stage. Our third contribution is Sample-Weighted Focal Contrastive Loss, which assigns higher importance to hard-to-classify minority classes, and makes sample pairs with different emotion labels mutually exclusive with each other to maximize inter-class distances, such that semantically similar emotions can better distinguished. Here, alpha is a simple weight parameter that controls the degree of focus on minority classes, and gamma is a focusing parameter which forces the model to focus on how to classify examples. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on two ERC benchmark datasets, MELD and IEMOCAP, with significant improvements in minority and semantically similar emotions, as illustrated here in Table 1 and Table 2. Here, Figure 7 depicts a visualization of the heatmaps of a prone-to-misclassification utterance in MELD, which demonstrates the superiority of MultiEMO in difficult scenarios while the emotional tendencies from different modalities are asynchronized. MultiEMO will also have some limitations. First, VisExtNet does not distinguish between speakers and irrelevant people in the scene. Second, the SWFC loss requires a large batch size on MELD. And finally, the performance of MultiEMO in minority emotions are still worse than majority classes. That\\'s all. Thank you so much for listening.'}\n",
      "{'id': '281', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '282', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/LxUqhgGELt.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/LxUqhgGELt.mp4', 'text': 'Hello everyone. I\\'m Xuekai Zhu, and today I\\'m excited to present our new work at ACL 2023, \"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing\". This work addresses an important task in natural language generation, non-parallel text style transfer. Until now, most studies have focused on the token level or sentence level, such as sentence sentiment transfer or formality text transfer. Our research takes a significant step forward by story-level style transfer, and at the discourse level, which is crucial for imitating author style. And we can do this in the story level. Let\\'s see the main challenge in our task. Long text usually involves many complicated author linguistic preferences, such as discourse structures. The primary challenge lies in imitating the author\\'s linguistic choices at the discourse level, as you see in Table 1, in the red contents, such as narrative techniques, and moreover, also styles tend to be highly associated with specific writing topics. This makes it difficult to transfer this style-specific content to another style, like the missing content in orange in Table 1. And this is the two main challenges in our text. And as our solution to alleviate these issues, we propose our generation model named StoryTrans. StoryTrans learns discourse representations from the source texts and combines this with learnable style embeddings to generate texts in the target styles. We have also designed a new training objective to reduce the stylistic features from the discourse representations, pulling the representations derived from the different texts closer in the latent space, and moreover, to enhance the content preservation, we separate the generation into two stages. First, we transfer the source text with the style-specific content keywords masked, and then generate the whole text by incorporating these keywords explicitly. As for the training framework, we separated the two stages of the training. For the first stage, we use an advisory training framework. We employ the self-reconstruction loss to recover the input, and then the disentanglement loss is conducted on the sentence embeddings, which aims to disentangle the style and the contents of the sentence-level imitations. Sentence order loss aims to capture the sentence-level dependency, and finally, the style classifier loss tries to produce the style signals for the whole system. And as for the second stage, this stage is unrelated to the style transfer, which aims to fill the correct style-specific contents and remove the mask token. And finally, we can get the transfer tasks. And we can see the evaluation and datasets. We collected new datasets in Chinese and English for these new tasks and we conducted extensive experiments to transfer the fairytales or everyday stories to typical author styles. Both the automatic evaluation results and the manual evaluations confirm the efficiency of our model and show that StoryTrans outperforms strong baselines in terms of style control and content preservation. Furthermore, style visualization indicates the transfer test by StoryTrans are also aligning with the golden text in the style feature space. And finally, we can see some cases where StyleLM inserts many unrelated sentences, and StoryTrans can supplement several short phrases or plots to enrich the whole storyline and maintain the main contents. Furthermore, StoryTrans can rewrite most sentences with the target style and maintain the source semantics. And this is all of our work, and finally, our data and code included in this repo. If you have any questions, please don\\'t hesitate to email me.'}\n",
      "{'id': '283', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the first mentioned symmetrical dependency structure? Answer the one including the city name', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lautet der Name der zuerst erwähnten symmetrischen Abhängigkeitsstruktur? Beantworten Sie die Frage mit dem Namen der Stadt', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della prima struttura di dipendenza simmetrica menzionata? Risponda menzionando quella che include il nome della città', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '284', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/FpAwPfITCT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/FpAwPfITCT.mp4', 'text': 'Hello everyone. I\\'m Peng Tianshuo from Wuhan University. Today I will present my long paper for ACL\\'s Main Conference 4,915 titled \"FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction\". The current span-based UIE models involves identifying and labeling the span boundaries of the targets in the text, which overrelies on boundary positions of the annotated span. However, there is ambiguity in labeling the golden span boundary. That is, different annotation spans can be considered reasonable. So we proposed that the span boundary learned by the module should be fuzzy instead of precise. Besides, there is a mismatch between transformer feature extraction and information extraction. Basic Transformers focus on global features, which ignores the prior hypothesis that span has limited length. So we proposed that the attention used for span extraction decision should be adaptive rather than static in order to model the furthest span boundary, which represents the target boundary as a continuous distribution of correct probability in a specific range, where R-min and R-max represent the start and end of the fuzzy boundary. And the function Q represents the correctness of the current position. Through the sampling function shared in the slide, we convert the continuous boundary distribution into a group of discrete values for calculation of fuzzy span loss. The boundary distribution predicted by the module will calculate Binary Cross Entropy with golden boundary as BCE loss and adding KL-divergence between predicted boundary with fuzzy span boundary and supplementary information. To get the model in obtaining a more reasonable attention distribution for span extraction, we proposed a fuzzy span attention as a mask function to trim attention distribution. The image and formula of the mask function G are shown in the slide, where the fuzzy span is reflected in two aspects. On the one hand, by introducing an optimizable parameter delta to adjust the length of the full attention range, the attention span of the module is dynamically changing. On the other hand, the attention distribution on the attention span boundary linearly decays, rather than truncates. The overall structure of the module is presented on the slide, where the fuzzy span attention layer is only added on the top level to guide the model\\'s decision process without affecting the text encoding capability. To demonstrate the capability of FSUIE, we conduct experiments on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction. As for the results of named entity recognition, by introducing FSL and FSA, our FSUIE-base achieved significant performance improvement compared to UIE-base without a fuzzy span mechanism. On small-scale data size, the model is easier to learn universal attention spans, resulting in a more significant improvement. As a result, on relationship extraction, FSUIE achieves new sota results on datasets ACE2004, 2005, and ADE. FSUIE uses one unified structure to extract relationship elements, achieving better information extraction ability with simple structure. Besides, FSUIE shows stronger generalization capabilities for domain-specific information. As for results on ASTE tasks, FSUIE also achieves sotaresults on 14lap, 15res, and 16res of AST-V2 dataset, and demonstrates competitive performance on 14res datasets. The results of the ablation study shows that FSA improves convergence speed by guiding the module to obtain a reasonable attention distribution. FSL enables the module to fully utilize annotation information and obtain greater information extraction capability. The combined effect of the two will produce a greater enhancement. We also visualized the attention distribution of a fuzzy span attention layer. Results show that the module focused on semantic information within a limited range of preceding tokens. This meets our expectations. In conclusion, in this work, we first proposed a novel fuzzy span loss that alleviates the model\\'s reliance on span boundaries, and then we proposed efficient fuzzy span attention to adaptively adjusting the attention span of the model. And the FSUIE we proposed achieves excellent results in a wide range of IE tasks. Thank you for your listening.'}\n",
      "{'id': '285', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wUvRuLqqnX.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wUvRuLqqnX.mp4', 'text': 'Hello everyone. I\\'m Mingqi Gao from Peking University. I\\'m glad to share our work, \"Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework\" . This video focuses on the key points of our work. As we all know, summaries generated by models and even some reference summaries still contain factual errors, and there are two main types of solutions. The first is to introduce factuality-related objectives in the training or inference process to make the summarization models more faithful and expect it will generate more factually correct summaries. The second is the design of a Factual Error Correction model (abbreviated as FEC), which is independent of the summarization model. It takes the source document and model-generated summary as input and outputs a corrected summary. To the best of our knowledge, there has been no work on factual errors for dialogue summarization. Considering the importance of factual issues in dialogue summarization, we would like to try to correct factual errors in dialogue summaries. However, after carefully examining and considering the motivations and practices of previous FEC studies, we argue that there are flaws in the way FEC models are evaluated, which may have diverted the FEC for summarization from its original purpose. Let\\'s see how the current FEC models have been evaluated. Factuality metrics such as FactCC and DAE are used. It takes a source document and a summary as input and outputs a score. It is expected that the average scores of the corrected summaries are higher than the original. The higher the score, the better the FEC model. There are two flaws. First, factuality metrics give an overall score, which is so vague, and factuality metrics may not be reliable on their own. Second, this evaluation blurs the line between the two types of solutions. The FEC model can ignore the content of the original summary and directly generate a different but more factually correct summary. There may be no error correction at all. We argue that it is necessary to introduce manually annotated reference corrections to address both issues. Factual error correction for summarization has these basic requirements: to correct factual errors in the original summary by as few substitution, insertion, and deletion operations as possible to obtain a fluent and non-redundant summary. This can be reflected in the manual annotation. The introduction of reference corrections, on the one hand, provides more valuable data for the training of FEC models compared with pseudo data. On the other hand, and more importantly, it creates the condition for a more comprehensive and accurate evaluation of the performance of FEC models. To automatically classify factual errors for factual error correction, we propose a new taxonomy of factual errors. We point out that there are two classifications of factual errors of different perspectives: content-based and form-based. Content-based categories are assigned according to the part of speech and dependencies, and form-based categories are assigned according to whether it is an addition, deletion, and substitution operation. We build our evaluation framework on the basis of ERRANT, an evaluation metric for grammar error correction. It mainly consists of three steps: alignment, classification, and comparison. We experiment with some FEC models in different training modes to explore some factors of interest. With our proposed evaluation framework, we have the following key findings. Training FEC models with reference summaries from dialogue summarization datasets yields the best results of unreliable factuality metrics. There is an urgent need to change the evaluation methods for FEC models. Introducing human-corrected summaries during the training of FEC models for dialogue summarization can improve their performance. Combining human-annotated data with synthetic data is a promising direction. And current FEC models struggle to correct factual errors like addition and cannot address attribute errors, modality errors, link errors, etc. Thanks for listening.'}\n",
      "{'id': '286', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '287', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '288', 'prompt_en': 'Answer the following question concisely given the English content: What datasets can be used to test syntactic phenomena?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Datensätze können zum Testen syntaktischer Phänomene verwendet werden?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali insiemi di dati possono essere utilizzati per testare i fenomeni sintattici?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些数据集可用于测试句法现象？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '289', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '290', 'prompt_en': 'Answer the following question concisely given the English content: What are the abbreviations of the five methods for the first research question?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lauten die Abkürzungen der fünf Methoden für die erste Forschungsfrage?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le abbreviazioni dei cinque metodi per la prima domanda di ricerca?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 第一个研究问题的五种方法的缩写是什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '291', 'prompt_en': 'Answer the following question concisely given the English content: On which tasks is the model evaluated?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Anhand welcher Aufgaben wird das Modell evaluiert?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su quali attività viene valutato il modello?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该模型在哪些任务上进行了评估？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '292', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '293', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi! I\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding. We\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we\\'ve used. When we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here\\'s for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement. We\\'ve also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.'}\n",
      "{'id': '294', 'prompt_en': 'Answer the following question concisely given the English content: On which data is CamemBERT initially trained?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Mit welchen Daten wurde CamemBERT ursprünglich trainiert?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Su quali dati viene inizialmente addestrato CamemBERT?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： CamemBERT 最初是在哪些数据上训练的？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '295', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '296', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JyeewMtgdX.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JyeewMtgdX.mp4', 'text': 'Hi. I am Valerio Basile, and in this video I am going to present a work which is the fruit of a collaboration between the University of Turin and Amazon Alexa. Natural Language Understanding — and natural language processing in general — is based in large part on supervised machine learning, or the so-called data-driven approaches. And in order to be able to develop these approaches, we need a lot of data, large sets of manually annotated data which encodes some kind of human knowledge, which the annotators put into the data by their process of annotation. Typically, there is a series of works which have been showing that the assumption that there is one single truth — that\\'s called the ground truth, and the annotation is to converge towards it — is showing some limits. We chose to focus on irony in particular, which is a highly latent and pragmatic phenomenon in natural language. Irony detection is already a very difficult task for modern natural language processing models, but we want to investigate this problem further. Rather than training models that are able to say something like a binary label — like this sentence is ironic or is not ironic — we would like to have models that have a more informative output. So, in order to study these problems, we developed a corpus called EPIC, which stands for English Perspectivist Irony Corpus. We collected data from different sources — from social media, Reddit, and Twitter — and the data spans a long time window of 1½ years, and we collected about 300 short conversations made of pairs of text, one following the other. And we repeated this for both sources and for five varieties of English, which are listed here. We use the crowdsourcing platform Prolific to have people annotate this data. We selected about 15 annotators for each of the English language varieties, for a total of 74 annotators in total. We gave each of them 200 texts, or 200 short conversations. And we also put extra questions to use as an attention check for quality control. So on average, we got 5 annotations for each short conversation. This is how the annotation interface looks like. It\\'s very simple. It resembles a chat or a text interface. They see a text, a message, and its reply, and then there\\'s just the question there below, which asks \"Is the reply ironic?\" with respect to the context, and the annotator can choose \"Ironic\" or \"Not ironic\". We observed some differences between several groups along different dimensions, depending on how we divide the datasets. But whether we divide the annotators by gender, by age group, nationality, and so on, we can notice that the distribution of the inter-annotator agreement that is depicted in this violin plot is always kind of different in every case. We try to model these differences and build different models, which we call perspective-aware models, basically training different models by fine-tuning a pre-trained language model on splits of the datasets, where the splits are in terms of different annotators While in terms of raw performance, we didn\\'t notice particular trends here (of course, the results are different, but there is no upward or downward trends), by using perspective-aware models versus gold standard aggregated models, we notice a significant difference in the confidence that the perspective-aware models show. And in particular, we can see how the perspectivist models are, on average, less uncertain, more confident of their predictions. Finally, we went again into the data and we try to look at what may be the causes of the differences in the annotations, and we found something peculiar: that is, that in the case of age, it is generations that are close to each other that seem to be more in disagreement toward their perception of irony. And similar things happen in the geographical distribution of the annotators, where we found that the highest variations in response is given between the two models trained on labels given by annotators from the United Kingdom and Ireland. This is it for this short presentation, but we will be happy to answer all your questions and have further discussions at the poster session.'}\n",
      "{'id': '297', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DIvTWvVcoq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DIvTWvVcoq.mp4', 'text': 'Today I\\'ll be talking about our work, \"From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models\". This is an example of a speech given by Senator Josh Hawley a few years ago, where he\\'s complaining about the cosmopolitan elite agenda and experiment. While a lot of people will think that he\\'s complaining about maybe urban, liberal, worldly people, some will interpret this as a screed against Jewish people. \"Cosmopolitan\" is an example of a dogwhistle, terms that send one message to an outgroup and a second (often taboo, controversial, or inflammatory) message to an in-group. So in this case, the in-group knows \"cosmopolitan\" means Jewish, but he has plausible deniability because he never actually says \"Jewish\". So \"cosmopolitan\" is an example of a dogwhistle that is anti-Semitic. And how that works is that the speaker communicates both this message and a persona at the same time. While the outgroup audience can only interpret the message, the ingroup interprets both the message and the persona to arrive at this coded meaning. And understanding dogwhistles is super important for NLP and linguistics because it really questions our idea of meaning, and the meaning of dogwhistles is super context-dependent. But it\\'s also an important mechanism of political influence and persuasion and enables hateful and abusive rhetoric while evading content moderation online. But studying them is super hard because dogwhistles are most successful when the outgroup is unaware, and researchers were usually in the outgroup. In this project, we develop a typology and glossary with rich contextual information, perform a case study of historical U.S. political speeches, and evaluate dogwhistle recognition in language models. Finally, we do a case study of toxicity detection to show one way in which dogwhistles can evade content moderation. So to start, we have a glossary of over 340 terms and symbols, especially for racist, transphobic, and anti-Semitic dogwhistles. These are collected from a wide range of academic, Wikipedia, blog, and other sources. And they are all in English and mostly US-centric. Here\\'s an example of an entry for sex-based rights, which is a transphobic dogwhistle. We have all these different categories that are part of our typology, which I will talk about in a second, as well as an explanation of how this is a dogwhistle, with a real-world example. So, dogwhistles, we characterize them by register, type, and persona. Registers: informal or formal. Persona is, for example, anti-Semitic or transphobic, so what the speaker is communicating. And then type is a little more complicated, and we refer to the paper to understand all of these different categories. Basically, it\\'s whether a dogwhistle is really adding an additional implicature to the sentence, or if the goal is really just to covertly signal this persona. Then we do a case study of historical U.S. political speeches, and we find the pattern where just the frequency of speeches containing racial dogwhistles and the U.S. Congressional Record patterns quite closely with what we know about the Republican Southern Strategy, where dogwhistles were used a lot more since the Civil Rights era, since politicians were no longer given license to be explicitly racist. We also see that they are more associated with conservatism over time, which we discussed more in the paper. And then we looked at language models and we do a couple of different experiments. First, we look at surfacing dogwhistles with language models, where we prompt GPT-3 to give us examples of dogwhistles. And we do this with many different configurations of prompts and how specific groups are being asked for or not. So see the paper for that as well. And we do see that GPT-3 can surface many of the dogwhistles in our glossary, especially those that are part of the formal register. But we do see also that this performance varies a lot. It does very poorly with the more social media use, informal dogwhistles and also for transphobic dogwhistles especially. We also see if GPT-3 can identify the covert meanings given the actual dogwhistle term itself from our glossary, and we see that it does okay, but again with a lot of variation, and it also depends a lot on the prompting strategies. So here for example, we see that adding a dogwhistle definition and a secret cue — so if we ask the model what it secretly means — helps performance a lot. Finally, we show how dogwhistles can evade content moderation by looking at toxicity detection with Prospective API and hateful template sentences from HateCheck. So the goal here is, \"Do automated toxicity detection scores change when standard group labels or slurs are simply replaced with dogwhistles?\" And here are some examples. And the answer is yes, hateful sentences are rated to be less toxic even in the exact same sentence when slurs and standard group labels are swapped with dogwhistles. So just to wrap up, in this project, we develop a typology of dogwhistles and a glossary with rich contextual information, including information about each dogwhistle\\'s persona, register, and type as well as real-world examples. We also conduct this case study of the frequency of dogwhistles and historical U.S. political speeches. Then we evaluate dogwhistle recognition and language models, specifically GPT-3. And then we do a case study of Prospective API with hateful sentences to show how dogwhistles may evade content moderation online. Thank you.'}\n",
      "{'id': '298', 'prompt_en': 'Answer the following question concisely given the English content: Which findings led to the conclusion that the temporal drift is the main cause of performance loss?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Ergebnisse führten zu der Schlussfolgerung, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali risultati hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '299', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/fAizSsCfol.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/fAizSsCfol.mp4', 'text': '\"Hi everyone. My name is Michalis Korakakis. Today we are here to talk about \"\"Improving the robustness of NLI models with minimax training\"\". This is joint work with Andreas Vlachos at the University of Cambridge. NLI models have achieved state-of-the-art results across a block of benchmarks. However, despite rapid progress, recent work has demonstrated that the success of NLI models is partly due to learning and using shortcuts. Shortcuts are spurious correlations between the input attributes and labels introduced during the dataset creation process. For example, high word-overlap between the premise and the hypothesis in the MNLI dataset is strongly correlated with the entailment label. Consequently, NLI models that exploit shortcuts perform well on in-distribution samples, but are brittle when tested on out-of-distribution adversarial test sets where such spurious correlations do not hold. Prior work in shortcut mitigation typically assumes access to an auxiliary model designed to rely on shortcuts for predictions. For instance, the auxiliary model can learn to exploit shortcuts by being trained only on a small number of examples or by leveraging an auxiliary with reduced learning capabilities. The output of the auxiliary is then used to re-weight training instances for the learner model. Existing shortcut mitigation methods may require knowing this in advance. This assumes domain- and dataset-specific knowledge, which is not always available and thus limits the potential of shortcut mitigation. Furthermore, current shortcut mitigation methods often assume that the learner will naturally exploit the same types of shortcuts as the auxiliary. In practice, the behavior of the learner diverges from that of the auxiliary. For example, the auxiliary may down-weight instances that are useful for training the learner or provide inaccurate uncertainty estimations that may hinder the learner\\'s out-of-distribution generalization capabilities. Finally, current shortcut mitigation methods require using a pre-trained language model as the auxiliary, which incurs additional computational overhead. Motivated by these limitations, in this work, we propose a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance. The key insight behind our training method is that NLI models suffer from poor performance on under-represented \"\"hard\"\" training instances with patterns that could contradict the shortcuts in the dominant \"\"easy\"\" examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution examples. Crucially, the loss of hard examples decreases considerably more slowly than the average loss throughout training. Therefore, our aim is to obtain an example weight distribution that places emphasis on the under-represented hard examples. To compute the weight distribution with both a minimax training objective between a learner and auxiliary, the learner tried to minimize the loss of the NLI task, whereas the task of the auxiliary is to maximize the learner\\'s loss by generating example weights such that the learner is incentivized to concentrate on ranges of the input space where it incurs high losses. Thus, the learner would prioritize learning from under-represented hard examples that counteract the uses of shortcuts present in the dominant easy examples. Both models are optimized in an alternating fashion using any standard optimization algorithm, such as stochastic gradient descent. At test time, the learner can make predictions without relying on the auxiliary. Our method does not make any assumptions about the type of shortcuts contained in a dataset. It relies on the learner\\'s own training dynamics to generate example weights. And finally, we use a feed-forward network to model the auxiliary. We evaluate our proposed method in three commonly used analytic datasets, MNLI, FEVER, and QQP, and the corresponding out-of-distribution adversarial test sets, HANS Symmetric, and PAWS. Here, we observe that compared to an ERM training model as well as the best-performing shortcut mitigation method in each dataset, the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy. Finally, in our paper, we also examine whether the performance improvements transfer in larger models, synthetic shortcuts, and out-of-domain test sets. What is the effect of pre-training the learner? How small the auxiliary needs to be. And finally, we conduct a qualitative evaluation of the learned example weight distribution. If you find this work interesting, we would love to chat with you during our poster session. Thank you for your time.\"'}\n",
      "{'id': '300', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wDNGJCFIKb.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wDNGJCFIKb.mp4', 'text': 'Hi. My name is Belinda, and the work I\\'ll be presenting today introduces a task called interactive dictation and makes initial steps towards solving this task. This was work done at Semantic Machines in collaboration with Jason Eisner, Adam Pauls, and Sam Thomson. So what is interactive dictation? At a high level, interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. In this example, a user starts by dictating, \"\"Just wanted to ask about the event on the 23rd.\"\" This is transcribed verbatim into the text box. However, in the middle of speaking, the user realizes they made a mistake and corrects themselves, saying, \"\"on Friday the 23rd.\"\" Ideally, the system can pick up that this was a speech correction and replace the correct span with a new utterance. Next, the user continues transcription, saying \"\"Is the event still on?\"\" which gets transcribed into the text box. Finally, the user can issue a verbal command like \"\"Replace \\'the event’ in the last sentence with \\'it\\'.\"\" The system can identify the correct occurrence of \"\"the event\"\" to replace with \"\"it.\"\" While speech-to-text systems are starting to proliferate, most of them support only dictation and do not support invoking edits through vocal commands. There are a few softwares that do recognize vocal edit commands, such as Nuance Dragon NaturallySpeaking, and the Microsoft Word Dictate function. However, these systems can be unintuitive because they require memorizing a fixed set of template commands. We know that a more natural and intuitive interface is possible when dictating to a human assistant. Even without agreed-upon trigger words or commands, humans can generally tell when you\\'re commanding versus when you\\'re dictating, and what command you\\'re invoking. Thus, distinct from prior work, the interactive dictation task is characterized by the following key features. First, flexible interleaving of dictation and editing, not separated by a trigger word. Second, using intuitive and open-ended natural language utterances to specify edits. In summary, our contribution is threefold. First, we introduce and formalize a new task, interactive dictation. Second, we design a data collection interface and build a dataset for this task. And finally, we create a baseline system for this task. To begin, we formalize the task of interactive dictation as a four-step procedure. In the first step, an ASR recognition module parses raw audio into a speech transcript. Next, the speech transcript is segmented into separate dictation and command utterances. Third, each command is extracted and normalized. The ASR miss detections and speech errors are fixed. Finally, each dictation and command utterance is executed in sequence until we arrive at the final document state. Note that in a real system, this all happens in real time as the user is speaking. Since this is a new task, we need to collect our own data, for which we design a new interface. So I\\'m going to transcribe the following email. I\\'m going to start by clicking \"\"Begin Transcription\"\"... \"\"Hey, I\\'m really sorry, but I can\\'t make it today.! Note that this issued a dictation or insert text segment, and whatever I said got transcribed both in this ASR field and also into the document state. Now I\\'m going to press the ’ctrl’ button on my keyboard to issue a command: \"\"Change the comma after ’Hey’ into an exclamation point.\"\" Ok, note that I just issued a command, and unlike dictations, commands don\\'t automatically show up in the document state. I must demonstrate the change using mouse and keyboard. Moreover, for commands, I can actually go into the ASR and fix it up. So I can keep doing this, issuing commands and dictations in sequence, until I\\'ve replicated this email. Using this annotation interface, we collect the dataset. More details about how these trajectories were collected can be found in the paper. Finally, we build a baseline system that performs each of these four steps. We train a separate model to perform each of these steps. You can see the paper for more details, but in particular for the interpretation model, we experiment with two different architectures, T5 and GPT-3, and two different types of outputs. We either have the model predict programs that can be executed into the next state, or we have it directly predict the next state. First, for the segmentation model, we see that it\\'s both fairly accurate and efficient. Next, we evaluate the ASR repair and interpretation models jointly using exact match of the predicted end-state against the goal end-state. We find that there is generally a trade-off between runtime and accuracy, and that generally GPT-3 models are more accurate but also much slower. Furthermore, for GPT-3 models, predicting state directly is much more accurate than predicting intermediate programs. For T5 model, this distinction is much less pronounced, and predicting programs allows us to significantly improve efficiency with minimal impact on accuracy. As you can see, however, there\\'s clearly much more room for progress here, and we welcome more work on this task. To facilitate future work, we have released code at the following site. Please also check out the paper for more details.'}\n",
      "{'id': '301', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '302', 'prompt_en': 'Answer the following question concisely given the English content: Why is it necessary to permute the tokens for the output sequence?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Warum ist es notwendig, die Token für die Ausgabesequenz zu permutieren?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Perché è necessario permutare i token per la sequenza di output?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 为什么有必要对输出序列中的词元进行排列？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '303', 'prompt_en': 'Answer the following question concisely given the English content: Why did the authors recommend that model owners should increase transparency about bias mitigation methods?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Gründe geben die Autoren für die Empfehlung an, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Perché gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 为什么作者建议模型所有者应提高偏见缓解方法的透明度？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '304', 'prompt_en': 'Answer the following question concisely given the English content: What are minimal-pair unacceptable inputs?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind inakzeptable Minimalpaareingaben?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa sono gli input inaccettabili di coppia minima?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 什么是最小对不可接受输入？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '305', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '306', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/oNVsjhIdMY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/oNVsjhIdMY.mp4', 'text': 'Hello everyone. I am Sebastian Schuster, and together with Najoung Kim, I\\'m going to give you a short overview of our work on Entity Tracking in Language Models. For an agent to understand a discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So, for example, in the context of a recipe — such as here —an agent has to understand that \"Put the eggs, sugar, and flour in a bowl\" results in all of these three entities ending up in a bowl. And if the discourse continues with \"Mix to form a light batter\", then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding longer discourses, but there haven\\'t really been any systematic investigations into what pre-trained language models can actually perform such tasks. And so the overarching research question we\\'re trying to answer in this paper is to what extent large language models can track entities. Now, given that we don\\'t know the exact contents of the pre-training data of many language models, and considering several properties about how discourses work, there are actually several challenges with designing a task to evaluate entity state tracking abilities. First, some entity states will be common in the pre-training data, and therefore, the model may predict the correct state without actually having any entity tracking abilities. For example, eggs often end up in bowls or babies often end up in cribs. So we want to make sure that the distribution of patterns in the pre-training data cannot give away the entity states in the evaluation data. Then second, sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse, and the model may seem to be able to perform entity tracking while in fact it just learns simple heuristic associations between words and entity states. For example, that the word \"empty\" is always associated with an entity being empty. And then third, if one uses fine-tuning or in-context demonstrations — which is often necessary to probe the model — then the model may memorize entity state sequences, or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task, we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Najoung will tell you a bit more about how we set up this task. Hello. My name is Najoung Kim, and I\\'ll be talking about the task design and the experimental results. So to find the entity tracking abilities, we designed the following task involving boxes and objects. And in our setup, the input to the model starts with a description of the initial contents of each box, as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now, given just this initial description, the task is pretty trivial: the model can just copy the relevant information from the description. But in our task, we also include multiple state-changing operations like moving objects or adding objects to a box. So for these, the model would have to combine the initial description with the operations to make the correct prediction. For example, Box 1 now contains the car and the watch after moving the watch from Box 3 to 1. And additionally, we implemented various measures to prevent the model from using heuristics, as Sebastian discussed earlier on. So please check out our paper for how we did this. We tested the setup with Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning. And what we\\'re showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. And on the left panel, we have the data points where the probed entity state is different from the state in the initial description, whereas on the right panel, we have cases where the state is the same as in the initial description. So for these ones, the model can simply copy. And our experiments show that most models simply repeat the initial state, as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-003 exhibits non-trivial tracking, which is the pink line here in the left panel. And all other models performed below a strong random baseline obtained by random simulation, which is the blue line. So what gives rise to this difference between models? Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. And this suggests that pre-training on code is what\\'s responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5-base can learn to perform entity tracking if you directly fine-tune the models. But on the other hand, randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision, suggesting that pre-training is again important here. However, as we discuss in more detail in the paper, it remains unclear whether the state tracking abilities we observe generalize beyond our set-up in this case. Thanks for listening, and we have a lot more results and analysis, including GPT-4 experiments, in our paper, so please check out arxiv. And if you have any questions or comments about our work, either find us in person at ACL, or you can reach out to us over email or on Twitter. Thank you.'}\n",
      "{'id': '307', 'prompt_en': 'Answer the following question concisely given the English content: What evaluation metrics did the authors use?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Bewertungsmetriken haben die Autoren verwendet?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali metriche di valutazione hanno utilizzato gli autori?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者使用了哪些评估指标？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '308', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.\"}\n",
      "{'id': '309', 'prompt_en': 'Answer the following question concisely given the English content: Which metric was used for measuring inter-annotator agreement?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Metrik wurde verwendet, um die Übereinstimmung zwischen den Kommentatoren zu messen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale metrica è stata utilizzata per misurare l'accordo tra annotatori?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 使用了哪个指标来衡量注释者之间的一致性？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '310', 'prompt_en': 'Answer the following question concisely given the English content: Which domain was chosen to add completely unrelated sentences to the unacceptable and acceptable queries?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Domain wurde gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale dominio è stato scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.\"}\n",
      "{'id': '311', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '312', 'prompt_en': 'Answer the following question concisely given the English content: How does MultiInstruct differ from other benchmarks?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was unterscheidet MultiInstruct von anderen Benchmarks?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: In che modo MultiInstruct differisce dagli altri parametri di riferimento?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： MultiInstruct 与其他基准有何不同？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '313', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '314', 'prompt_en': 'Answer the following question concisely given the English content: What is the definition of binary coordination?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lautet die Definition der binären Koordination?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è la definizione di coordinazione binaria?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 二进制协调的定义是什么？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '315', 'prompt_en': 'Answer the following question concisely given the English content: How long, on average, were the prompts used in this study?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lange waren die in dieser Studie verwendeten Prompts im Durchschnitt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Per quanto tempo, in media, sono stati utilizzati i prompt in questo studio?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在本研究中，提示语的平均长度是多少？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'NA'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '316', 'prompt_en': 'Answer the following question concisely given the English content: What are the implications of the findings on the smaller T5 model?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Auswirkungen haben die Ergebnisse auf das kleinere T5-Modell?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le implicazioni dei risultati sul modello T5 più piccolo?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这些发现对较小的 T5 模型有什么影响？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '317', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/eBtxiqqWFT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/eBtxiqqWFT.mp4', 'text': 'Hello everyone. I\\'m Peng Li from Fudan University. I\\'m very delighted to present our work titled \"CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors\". Information extraction is a classic task in natural language processing. It refers to extracting structured information from unstructured text. Common information extraction tasks include named entity recognition and relation extraction (RE), and so on. Here is an example of named entity recognition. For the input task, \"Steve became CEO of Apple in 1998.\" The model needs to recognize that Steve is a person\\'s name and Apple is an organization\\'s name. For previous information extraction models, we see using pre-trained language models like T5 and GPT-3. They operate in a text-to-text manner during the pre-training stage. However, during the inference phase, the structured output of information extraction is linearized into a plan sequence. The problem with these approaches is that while it learns the inputs reformatted between the inference and pre-training stages, the outputs are not learned. One output is plain text while the other is a structured output, making it challenging for the model to generate the correct structures. This often requires a large amount of structured training data and special decoding strategies to mitigate this issue. To address the problem of mismatched outputs, we propose CodeIE, transforming the text-to-structured information extraction task into a structure-to-structure code generation task and use code large language models like Codex to perform it. This way, we can easily convert text to a structured format during the input stage and ensure aligned structures in the output stage. Specifically, for the Named Entity Recognition Task, we have designed the following prompt. First, we define a function for named entity recognition that takes an input text as input. We add a comment and extract named entities from the input text to this function. Then, we set the input task to the actual input text and initialize an entity list called Entity List. We provide a comment saying extracted named entities to trigger the subsequent content. With few-shot in-context demonstrations, we can expect the model to output the following code, continuously extract the text and entity pairs, and append them to the entity list. For relation extraction, we have designed similar prompts. We evaluated our method on three recognition datasets and four relation extraction datasets. Our evaluated models include T5 model, the UIE model, the text-davinci-002 version of GPT-3 model, and code-davinci-002 version of Codex model. We compared the performance of two types of prompts, one using a traditional text-style prompt and the other using the code-style prompt described before. In the case of one to few-shot, we found that our proposed approach using code language models and code format prompts significantly and consistently outperformed the traditional baseline models, such as UIE and natural language large language models, like the GPT-3 model. We further conducted a detailed and in-depth analysis of this phenomenon. Firstly, we observed that the perplexity computed on text format inputs using models like T5 was generally higher than that of code format samples using models like CodeT5. And we can observe that transforming information extraction into a code generation task and using code pre-training language models better aligns with the information extraction task itself. Furthermore, we observed that when decoding with GPT-3 and text format prompts, there are many structural errors, whereas when using Codex and code format prompts, such errors were almost non-existent. We also analyzed that using GPT-3 for information extraction tasks, it outputs with labels that were not present in the predefined label set, such as currency, company, called organization, and so on. Additionally, we found that regardless of prompt formats, the Codex model outperformed the GPT-3 model in information extraction tasks overall. Furthermore, in testing the models, using code format prompts performed better than text format prompts especially in terms of recall. We hope this analysis can provide some inspiration to everyone. Lastly, thank you all. If you have any questions, please feel free to contact me. Our paper and code are made of publicly available.'}\n",
      "{'id': '319', 'prompt_en': 'Answer the following question concisely given the English content: Which learning strategies are investigated in the work?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Lernstrategien werden in der Arbeit untersucht?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali strategie di apprendimento vengono esaminate nel lavoro?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 论文研究了哪些学习策略？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn\\'t have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn\\'t scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '320', 'prompt_en': 'Answer the following question concisely given the English content: How big is the factor of overfitting due to test reuse specifically?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie groß ist der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanto è grande il fattore di overfitting dovuto al riutilizzo del test?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '321', 'prompt_en': 'Answer the following question concisely given the English content: How was the quality of the simplification evaluated?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie wurde die Qualität der Vereinfachung beurteilt?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Come è stata valutata la qualità della semplificazione?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 如何评估简化质量？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.\"}\n",
      "{'id': '322', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/iNcexwFvlA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/iNcexwFvlA.mp4', 'text': 'Hi everyone. I\\'m Enrico and I will be presenting at ACL 23 answering the question, \"What does a Text Classifier Learn about Morality?\" First off, let me explain to you what is morality. Human morality is what helps us distinguish right from wrong; it\\'s our internal compass that helps us determine whether an action or a concept is morally right or morally wrong. And morality is at the base of our societies, obviously, and it is essential that language models can understand and recognize morality in our language in text. The understanding of morality in text has already been approached in the NLP community, but it is often and usually being treated as a singular scale between immoral and moral, where a concept, a sentence can be labeled somewhere between immoral and moral for the judgment of morality. However, we know that morality is very subjective. Different people would label the same concept differently in the scale. Take, for example, divisive concepts such as abortion or LGBTQ rights. Some of us would label, for example, abortion to be immoral and some would label it to be moral. Simply taking an average between these or taking a majority aggregation would actually hide the real truth, that it\\'s a pluralist, different way of interpreting morality. And teaching language models just the average is very dangerous. To this extent, there are social theories that we can apply to the understanding of human morality. In particular, a very popular and established theory is the so-called Moral Foundation Theory. According to this theory, there are five different ways in which we humans perceive morality, as much as there\\'s five different taste buds in our tongue, and each action, each concept, tickles a different moral aspect, a different moral foundation. And each of us, each human, prioritizes these foundations in different ways. For example, for some of us, fairness is very important, and for others, authority is much more important. The different ways in which we prioritize these foundations, determine the way in which we judge the morality of a concept or of an action. The Moral Foundation Theory has already been used in natural language processing. There\\'s been a surge of papers in the last couple of years, including papers from ourselves, that attempt to understand and to classify morality in text. And it\\'s been shown that language models can somewhat understand morality in text. So what we aim to do in this paper is to try to understand what they learn. We apply explainable AI techniques to these language models trained to understand morality in text. In particular, we focus on understanding how morality is expressed differently across different domains. We use a data set called Moral Foundation Twitter Corpus, composed of 35,000 tweets collected in seven different domains. Domains, for example, corresponding to the hashtag #AllLives Matter or the hashtag #BlackLivesMatter, and we try to see whether language models can understand that morality is expressed differently in different domains. We all know, of course, intuitively that the way in which morality is expressed, say in ALM or in BLM, is very different. But can language models understand these fine-grained differences? We have proposed a method, we have made a number of experiments to answer these questions, so I\\'m going to just give you one small sneak preview of that. Take the example of the difference indeed between ALM and BLM, All Lives Matter and Black Lives Matter. These two domains have a similar rhetoric because they cover similar topics, but they have a significantly different rhetoric for the moral element of subversion, which means rebellion to authority. We know intuitively, and we found that language models recognize that in ALM\\'s version is associated with words such as overthrow, mayhem, and subversion is frowned upon, whereas in BLM, subversion is somewhat encouraged. So, language models do recognize that morality can be expressed differently. Of course, there\\'s different levels of understanding that we explore in the paper, and these essentially warn us of the fact that, of course, morality is expressed differently in different domains, and using just one single model for many different domains can actually lead to misunderstandings of morality in a very dangerous way. I hope to see you at ACL in Toronto. See you there. Bye.'}\n",
      "{'id': '323', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rfJEBqlZck.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rfJEBqlZck.mp4', 'text': 'Hello everyone. I am Yujie Wang from Shanxi University, China. The title of my paper is \"Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA\". Commonsense QA is a challenging task that requires machines to answer questions that rely on common knowledge to test their language understanding abilities. This requires machines to retrieve relevant knowledge from external sources. Recently, Holmes thinks that knowledge is stored in both language models and knowledge bases. Many works combine these two types of knowledge to solve Commonsense QA with good readout. These works retrieve relevant knowledge from the knowledge base through entity matching, building a subgraph, and then using the language models and GNNs to infer answers. However, they introduce some noisy entities during subgraph retrieval, such as \"Top\", \"Bank\", and \"Cat\" here in the subgraph, which are largely unattached to the current question. Moreover, they encode the subgraph and text in isolation, leading to limited interaction between the two modalities. And the encoding process ignores the semantic relationship between entities. Based on the above problems, we propose DHLK. First, we build an HKG based on multiple knowledge bases run through a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. Finally, we use the language model to encode and fuse the two modalities. First, we remove the subwords that make up the phrase entity by using the dictionary vocabulary. At the same time, we retrieve the paraphrase of key entities in WordNet and Wiktionary and connect them as additional nodes to the subgraph, forming the HKG. Then we use RoBERTa and Mask Self-Attention to encode and fuse QA contexts and the entities, building the HKG. Meanwhile, we dynamically remove entities with weaker relevance to the QA context based on the attention weights of RoBERTa, as follows in the second subgraph, like \"wood\". For the initial entity and relation embeddings, we get the embeddings of entities and relations by mean pooling. Since HKG is composed of multiple triplets, we introduce TransE to optimize the entity and relationship embeddings in HKG. Unlike other works that use GNN to model subgraphs, we use Relation Mask Self-Attention to model our subgraphs. Inspired by RGAT, we introduce relationships in Mask Self-Attention, creating RMSA. We update the entity and relation embeddings of the HKG by iterating through L layers of RMSA. Finally, we get the graph embedding of the HKG by applying max-pooling to the question key entities. Next, we incorporate the HKG path information into the QA context and get the embedding representation of the QA context after path enhancement. For our final answer prediction, we input the HKG graph for embedding, the paths and QA context embedded, and the QA context embedded into the MLP to get the answer probability. We conduct experiments on CommonsenseQA and OpenBookQA using external knowledge bases: ConceptNet, WordNet, and Wiktionary. Meanwhile, we extract key entities in the QA context based on KeyBERT and retrieval of knowledge paths within two hops in ConceptNet. Here we report the results and leaderboards for CommonsenseQA and OpenBookQA. Compared with other LM and HKG methods, our method gets good results.'}\n",
      "{'id': '324', 'prompt_en': 'Answer the following question concisely given the English content: Do language models have different political biases?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Haben Sprachmodelle unterschiedliche politische Vorurteile?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: I modelli linguistici presentano bias politici diversi?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 语言模型是否有不同的政治偏见？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '326', 'prompt_en': 'Answer the following question concisely given the English content: What is cognitive dissonance?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist kognitive Dissonanz?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cos'è la dissonanza cognitiva?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 什么是认知失调？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '327', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/XNNpgzwgBl.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/XNNpgzwgBl.mp4', 'text': 'Hello everyone. I\\'m Xiao Xu, a third-year PhD student from Harbin Institute of Technology. I\\'m honored to present our work to you at ACL 2023. Thank you for your interest in our work, \"ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning\". This work was done during my internship in the MSRIC group. And I would like to thank Intel Cognitive Computing Group for their support and discussions. Let\\'s get started. The goal of Vision-Language learning is to train a smart AI system that can understand both image and text. Visual Question Answering is one of the most famous VL tasks, which needs to answer a question based on the input image. Since 2019, with the help of large-scale self-supervised pre-training on image-text pairs, transformer-based vision language models have achieved remarkable progress. From the perspective of model architecture, recent vision-language works can be unified as a two-tower architecture which consists of a textual encoder, a visual encoder, and a cross-modal encoder. If we step into the unimodal encoders of the two-tower architecture, such as METER, we can find that they feed only the last-layer unimodal representation directly into the top cross-modal encoder, ignoring the semantic knowledge at the different layers of the deep unimodal encoders. Different from the two-tower two-architecture, BridgeTower connects multiple top unimodal layers with each cross-modal layer in a layer-by-layer fashion to exploit unimodal\\'s magnetic knowledge at different levels. However, BridgeTower still suffers from two obvious limitations. First, its layer-by-layer utilization of different unimodal layer representations is ineffective. Each cross-modal layer can only utilize an artificially assigned unimodal layer representation, thus restricting the exploitation of different levels of unimodal semantic knowledge. Second, the number of cross-modal layouts is tied to the number of unimodal layer representations it used, thus limiting its scalability and capability. In this work, we build upon BridgeTower and advance it in two aspects. We propose ManagerTower, a novel VL modal architecture that each manager takes multiple unimodal representations as the insights of pre-trained unimodal experts at different levels. ManagerTower adaptively aggregates insights with managers in each cross-modal layer. Here is the detailed architecture of our ManagerTower. We use RoBERTa and CLIP-ViT base as unimodal encoders. It introduces managers in each cross-modal layer to gather and combine the insights of pre-trained unimodal experts at different levels. The managers can adaptively exploit different levels of unimodal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. It is important to note that one can apply any visual, textual, or cross-modal encoder in ManagerTower. With only four million images for visual language pre-training, ManagerTower achieves superior performances on various downstream tasks. Importantly, METER, BridgeTower, and ManagerTower use the same pre-training and fine-tune settings. ManagerTower significantly improves performance, especially 39.15% accuracy on Wikivideo test standard. This further demonstrates that, with all other factors fixed, compared to BridgeTower, which introduced bridges to METER, ManagerTower allows more effective exploitation of different levels of universal semantic knowledge via well-designed managers. Notably, ManagerTower not only outperforms many base-size models pre-trained on 4 million data, but it also surpasses some models trained with more data or parameters. Here we delve into managers by visualizing the average aggregation weight of textual or visual managers in each cross-modal layer over all samples in VQAv2 dataset. The x-axis is the index of the unimodal expert, and the legend shows the index of the cross-modal layer. We first take a look at static managers. No matter the textual role or visual role, similar progressive trends are shared in each cross-modal layer, which is inconsistent with the intuition that the need for unimodal semantic knowledge varies among cross-modal layers for adaptive measures we used in ManagerTower. Interestingly, the aggregation weight distributions generated by adaptive managers are completely different from those generated by static managers, as shown above. And there are two distinct trends. Vertically, there is a significant difference between textual and visual managers. Horizontally, whether textual or visual managers, they exhibit diverse aggregation weight distributions in different cross-modal layers. This provides strong evidence that adaptive managers can adaptively exploit different levels of unimodal semantic knowledge for comprehensive cross-modal representation learning. Paper, code, and modals are available on Archive and Github. We hope our work can be useful to you. Thank you.'}\n",
      "{'id': '328', 'prompt_en': 'Answer the following question concisely given the English content: Which language model is the most liberal?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welches Sprachmodell steht am meisten links?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quale modello linguistico è il più liberale?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 哪种语言模型最倾向于自由派？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington. Today I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It\\'s like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it\\'s kind of like the electric trolley problem. Ok, great. I think that\\'s pretty much all I have for today. Thank you for your time.'}\n",
      "{'id': '329', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DIsuMwxeqw.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DIsuMwxeqw.mp4', 'text': 'Hello everyone. I\\'m Minghang Zheng from Peking University. It\\'s my great honor to present our work, \"Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization This work was done in cooperation with Shaogang Gong, Hailin Jin, Yuxin Peng, and Yang Liu. In this work, we focus on zero-shot video sentence localization. Video sentence localization aims to find the most relevant segments with a given natural language query for long videos, and has broad applications in video retrieval, summarization, and other fields. This task takes a video and natural language query as input, and requires the model to output the start and end times of the video segment that are most relevant to the given query. However, many of these methods require a large number of manual annotations for training, which is costly and inefficient. Details on the settings outlined in this paper enable us to train video sentence localization models without any manual annotation. Existing zero-shot methods mainly use the following process. First, generate pseudo-events based on the video, then generate pseudo-queries based on the pseudo-events. Finally, train a video sentence localization model using these pseudo labels. They have three main drawbacks. Firstly, the pseudo-queries are usually too simple. For example, some methods combine detected nouns and verbs in the events to generate the queries, which has a large gap between the real queries. Secondly, as shown in the figure, their approach can only show a high relevance between videos within the event and query, but can not guarantee that the query and video outside the events is irrelevant, which lead to unalignment between the pseudo-queries and pseudo-events. Finally, they directly use these pseudo labels to train the model, ignoring the risk of label noise. Thus, we propose the noise-resistant Structured Pseudo-Label generation method. As shown in the figure, we first use a pre-trained image caption model to generate more complex free-form pseudo-queries. Then we use a pre-trained model to measure the relevance between individual frames and pseudo-queries, and generate pseudo-events which guarantee high relevance between the video inside the event and the query, and low relevance between the video outside the event and the query. Finally, we reduce the weight of noisy samples and create noisy labels to reduce the influence of label noise. First, we densely sample video frames and then use the image text pre-trained BLIP model to generate pseudo-queries based on the video frames. In this step, we use an image caption model instead of a video caption model. This is because image-text pre-trained models have large training data and better zero-shot generalization ability. Meanwhile, because image-text pre-trained models do not consider temporal information in the videos, in the second step we need to model the temporal structure of events to generate pseudo-events. Then we generate pseudo-events based on the event temporal structure. We require high relevance between videos within the events and queries, and low relevance between videos outside the events and query. Therefore, for each pseudo-query, we first calculate the similarity between video frame features and query text features and find the quality of an event as the average similarity within the event minus the average similarity outside the event. Finally, we enumerate all possible pseudo-events using a sliding window and select the one with the highest event quality . For example, as shown in the figure, we choose the proposal from 3.7 second to 12.1 second because it has the greatest difference in similarity within and outside the event. As we have a large number of suitable queries for each video, some of these queries may have low quality and high overlap. Therefore, we only keep the top K pseudo-queries with a higher event quality and eliminate pseudo-query-event pairs with high event overlap. Firstly, we use pseudo labels to train the video sentence localization model and reduce the influence of label noise. On the one hand, we estimate the label noise based on the model\\'s predicted confidence and the IoU between the prediction and the pseudo-label. The lower the confidence and the lower the IoU, the higher probability of label error. We weight these samples using the weights shown in the formula to reduce the contribution of noisy samples. On the other hand, if the most predicted confidence is high and has a high IoU with the pseudo-label, we train the prediction as a new pseudo-label for the next round of model training. We use both strategies in our model training. We perform experiments on two datasets, ActivityNet Captions and Charades-STA. The evaluation metrics R@M represents the percentage of predicted moments that have the IoU value larger than M, and mIoU represents the average Intersection over Union. These figures show the comparison between our methods and existing methods. We use SPL to represent our method. Compared with other zero-shot methods, we outperform other methods on most metrics. More experiments can be found in our paper. In conclusion, we propose a zero-shot video sentence localization method based on structured pseudo-label generation which is robust to label noise. We generate free-form pseudo-queries and generate pseudo-events based on event temporal structure, and reduce the influence of noise in pseudo-labels by sample re-weight and label refinement. We achieve best zero-shot performance on two datasets. Thanks for listening. Our code is available by scanning this QR code.'}\n",
      "{'id': '330', 'prompt_en': 'Answer the following question concisely given the English content: Does cumulative training perform better than iterative when doing active learning?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Ist kumulatives Training besser als iteratives Training für aktives Lernen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 在主动学习时，累积训练是否比迭代训练更有效？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.'}\n",
      "{'id': '331', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '332', 'prompt_en': 'Answer the following question concisely given the English content: Where was the data taken from in the MuDa benchmark?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Woher stammen die Daten für die MuDa-Benchmark?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Da dove sono stati tratti i dati nel parametro di riferimento MuDa?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： MuDa 基准中的数据是从哪里获得的？', 'metadata': {'qa_origin': 'Abstract', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '333', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vHpvYiXxJs.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vHpvYiXxJs.mp4', 'text': 'Hi everyone. I\\'m Wenhao from Nanjing University. It is a great honor to be here to introduce our work, \"INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation\". Before introducing our work, I would like to acknowledge collaborators. They are Jingjing Xu from Shanghai AI Lab, Shujian Huang and Jiajun Chen from Nanjing University, and Lingpeng Kong from the University of Hong Kong. In this work, we focus on neural machine translation. We know that the target of NMT is to learn a generalized representation space to adapt to diverse scenarios. However, neural networks often induce a non-smooth representation space, which limits its generalization ability. Specifically, in the representation space of the NMT model, we observe that low-frequency tokens disperse sparsely, as illustrated below. Due to the sparsity, many \"holes\" could be formed; in these holes, the semantic meaning can be poorly defined. As a result, an NMT model performs poorly in these areas. To enhance NMT models\\' generalization and performance, kNN-MT is proposed as a solution. Its core idea is to smooth predictions according to nearest neighbors in the representation space. To do so, it requires a training corpus to build a key value data store to save representations and their corresponding target tokens. At each decoding step, the NMT model will query the data store to retrieve nearest entries and refine the prediction probability according to the retrieval result. Although effective, this approach has two significant drawbacks. Retrieving neighbors from a large datastore at each decoding step is time-consuming, and once the datastore is constructed, representations cannot be easily updated. To overcome these drawbacks, we propose the framework INK, to INject kNN Knowledge into an MT. Our INK training loop has two steps. At first, kNN knowledge is extracted from the datastore to guide the adapter to adjust representation. Then updated representations are used to refresh the datastore asynchronously. This trend loop will run until convergence. Specifically, we adjust the representation by allowing three kinds of representation using KL-divergence. First, we align contextualized representation and token embeddings to keep semantic meaning. Then we align contextualized representations and kNN token embeddings to enrich semantic meanings. And we also align contextualized representations of the same target token to address the sparsely dispersing problem. Overall, we optimize the adapter with the combined learning objective and run this training loop until convergence. In the end of the training loop, we can drop the datastore aside. In our experiments, we chose the winner model of WMT’19 German-English news translation task as the off-the-shelf NMT model. We conduct experiments on the full benchmark dataset and find that even for the WMT winner model, its representation space can still be greatly improved. In our experiments, we explore the following three questions. The first research question is, \"Can we smooth the representation space with a small adapter and drop the datastore aside during inference?\" The second research question is, \"How much improvement can be brought by using kNN knowledge to adjust the representation distribution?\" The third research question is, \"Will using an adapter and datastore together bring further improvements’\" As shown in the table, the INK system outperforms the state-of-the-art kNN-MT system and achieves the best performance after smoothing the representation space. Compared with using an adapter baseline, we can see that refining representation according to kNN knowledge brings larger performance improvement. To better the show the effect of the INK framework, we use adapters of different sizes. In general, the INK system locates on the top right of each figure, which means that INK achieves higher BLEU scores with less memory space. Besides, we also find that jointly applying adapter and datastore can further smooth predictions, which indicates that the representation space of the NMT model is not fully refined by the adapter. If a more effective framework can be designed, the benefit of smoother representation space will be further revealed. To conclude, we propose a novel training framework in this paper. In our framework, we devise, inject, and refine a training loop to iteratively refine the representation space of the NMT model according to kNN knowledge. Experimental results show that the INK system achieves an average gain of 1.99 COMET score and 1.0 BLEU score, compared with the state-of-the-art kNN-MT systems. Our INK system also achieves better translation performance, with less memory space and faster inference speed.'}\n",
      "{'id': '334', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It\\'s okay the way instead of \"it\", we have this long NP. But it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That\\'s why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It\\'s absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there\\'s no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I\\'ll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.'}\n",
      "{'id': '335', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '336', 'prompt_en': 'Answer the following question concisely given the English content: What is cross-lingual transfer?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was ist sprachübergreifender Transfer?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cos'è il trasferimento interlinguistico?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 什么是跨语言转移？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University. Today I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they\\'re only evaluated on certain neural models. For example, there\\'s only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we\\'ll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.'}\n",
      "{'id': '337', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wfwueTMlOd.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wfwueTMlOd.mp4', 'text': 'Hello everyone. It\\'s my pleasure to present to you our work, \"Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning\". In this speech, I will provide an overview of our research and highlight its key contributions. It\\'s well known that out-of-vocabulary words (or OOV words) are difficult to represent while critical to the performance of embedding-based downstream models. So, how can we specifically handle OOV words? When we encounter an OOV word, we first observe its word formation and then associate it with some relevant words to further infer its meaning. It\\'s quite reasonable since most language vocabularies are derived from the creation of new words on the basis of old ones. Drawing inspiration from human study habits, we develop a newer approach that leverages word formation and association to infer the meaning of OOV words. We introduce a Word Relationship Graph that imitates the lexical rules of word formation and association. When the OOV word appears, we tokenize it into wordpieces and associate it naturally with other relevant words, forming a two-level graph around the OOV word. In our approach, each word or wordpiece acts as a node in the group, and its corresponding word embedding serves as the node attribution. We preserve all the nodes in the first layer to retain complete wordpiece information. As for the second layer, we sample a fixed number of nodes for training to mitigate noise from the wordpiece noise with numerous neighbors. Before we employ a graph neural network to process the word relationship graph, we need to address the issue of assigning node attributes to the OOV nodes. To overcome this challenge, we utilize a self-attention network that assigns attributes based on the characters of the OOV words. To extract the most important information and reduce the impact of noise neighbor nodes, we apply two levels of Graph Attention Network that we concatenate, and fuse the initial input with the hidden embedding of each layer, resulting in a node-level representation. To capture the whole graph information and summarize the word formation, we incorporate a readout block layer that\\'s a graph-level representation. Given that a word\\'s formation comprises its subunits and the relationships between them, a simple one-layer Graph Convolutional Network can suffice for our purpose. Furthermore, we aim to mimic the vector space of the background embedding model, so we apply contrastive learning in the loss function with NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself. And a pair for further graph-level embedding with their background embedding is selected to encourage proximity between them while pushing them farther apart from other samples in a path. Through extensive experiments, we have demonstrated that our model of performance is superior to baselines in both intrinsic and extrinsic tasks, which proves the effectiveness of learning OOV words by word formation. Moreover, our model can bring some profits to both static and contextual models in downstream tasks. Finally, we want to discuss the possibility of adding languages to our model. Agglutinative languages, which form words by stringing morphemes together directly, are well-suited for our model. On the other hand, fusional languages, which form words through linking morphemes, present more challenges. Nevertheless, our model performs well with English by reasonable word segmentation. In conclusion, the graph in our model can handle various complex word formations. We believe that the application of our model to other languages will largely depend on the rationality of word decomposition. Thank you for listening.'}\n",
      "{'id': '338', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/GBkOsXHhfn.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/GBkOsXHhfn.mp4', 'text': 'Good day everyone. My name is Bingsheng, and I want to express my gratitude for your interest in our research. Today, I will be presenting our work titled \"Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations\" on behalf of our research group. It is a collaborative work of researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. We will briefly present our motivation, discuss related works, and primarily focus on the contributions, which are divided into three sections: a unified structure, preliminary experiments, and an evaluation of five data sets and two models comparing our proposed metrics with an established metric. Many researchers have sought assistance from humans, including coworkers and experts, to annotate both labels and corresponding natural language explanations so as to train these models to generate human-understandable explanations and boost the models\\' prediction performance and reasoning ability. Before bluntly using these conditions as the gold standard, we need to address the vital question, \"How do we evaluate the quality of a human annotated explanation?\" Unlike labels, such explanations can be subjective and task-dependent. Let\\'s take a look at a diagram from the paper on using QA, comparing the human explanations from ECQA and CoS-E on the same commonsense QA instance. The one from CoS-E is shorter and contains less information, and you may think the one from ECQA is better. However, the one from CoS-E is not wrong, and it is difficult to systematically compare these two explanations. Traditional metrics such as BLEU and ROUGE treat human annotations as the gold standard and focus on word similarity. The simulatability score, on the other hand, measures the baselines performance change when explanations are presented or absent. However, it does not consider the task differences and neglects the difference utility of explanations during fine-tuning and inference stages. In our work, we selected five popular large-scale datasets for various tasks. These datasets include CoS-E and ECQA for the commonsense QA task, e-SNLI for natural language inference, and ComVE for evaluating commonsense validations. First of all, we introduce a template-based unified data format that converts various tasks into a unified multiple-choice task. This structure includes baseline setting without explanation and infusion setting where explanation serves as additional input to sequence-to-sequence models. Here we show how different tasks and formats can seamlessly apply to unified structure by substituting the corresponding questions and choices. We conduct an in-depth experiment to analyze the utility of explanations. We randomly sample nine subsets ranging from 10% to full training data, train models with baseline and infusion settings, perform inference on both settings, and repeat the whole process three times to obtain an average score. We plot the results onto diagrams, one for CoS-E and the other for ECQA, and the lines denote different fine-tuning and inference settings. Due to the time limit, we\\'ll go over some of the observations, but you can find details in the paper. First, the fine-tuning process is not teaching the model with new knowledge conveyed in the explanations. Second, fine-tuning with infusion actually teaches the model to rely on the explanation part of the input to predict. CoS-E explanations are less helpful than ECQA ones on baseline models, which aligns with previous works and emphasizes the task-dependent nature of explanations. Last but not least, we find that fine-tuning a model, even with a small amount of data that incorporates explanations, can lead to substantial improvement. Based on our observations, we propose a novel evaluation metric called TREU, which extends the simulatability score. Compared with the simulatability score, the TREU score additionally evaluates the helpfulness of explanations at fine tuning, where two models are fine-tuned with baseline infusion settings to compare their performance differences. We evaluate human explanations across the five aforementioned datasets using unified structure with both our TREU metric and simulatability score on two models, T5 and BART. Analyzing the scores for CoS-E, our results support the intuition that human-annotated explanations can still benefit model predictions even if they were considered low quality by humans in previous literature. Our metrics can reflect this observation better than the simulatability score. By examining the dataset ordering in both tables, which is based on our TREU scores, we observed that our metric consistently ranks dataset qualities on both T5 and BART. In contrast, simulatability score falls short in evaluating ComVE and e-SNLI. We further observed the TREU scores for the entailment category are positive, while for neutral and contradiction classes lead to negative values in e-SNLI. This suggests that the helpfulness of human explanations to models heavily depends on the task and explanation format, such as negation connotation in e-SNLI and ComVE, and counterfactual writing styles for neutral and contradiction classes. Our hypothesis is supported by recent works discussed in our paper. To summarize the contributions in our work, in terms of the desiderata for faithful explanation evaluations, we propose a unified data structure to preliminary experiments analyzing factors contributing to explanation utility and the proposal of a metric with an evaluation of five datasets with two models. Our evaluation demonstrates that our metric outperforms simulatability scores for this purpose. We emphasize that our work lays the foundation for high-quality human collaboration in annotation jobs, and we recommend researchers perform similar quality checks in the future. For more detailed findings, please refer to our paper. Thank you for your attention listening to my presentation.'}\n",
      "{'id': '339', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection. We can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.'}\n",
      "{'id': '340', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/jhNdXGyDEJ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/jhNdXGyDEJ.mp4', 'text': 'Hello everyone. I am Kuan-Hao Huang from UCLA. I\\'m presenting our work, \"ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation\". This is a joint work with Varun, I-Hung, Anoop, Kai-Wei, and Aram. Paraphrase generation is a long-standing and important task in the NLP domain. It benefits many other NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, we usually need a large scale of high-quality paraphrase data. However, for the existing human-annotated datasets like MRPC, PAN, and Quora they have high quality, but they are limited in scale. There are some automatically generated datasets, like back-translation — translating a sentence to another language and translating back. They can automatically generate a large scale of paraphrase datasets, but they are lack of syntactic diversity. Here is one example. We can see that the generative paraphrase has almost the same syntax to the original sentence. In this work, our goal is trying to construct a large-scale, syntactically diverse paraphrase dataset. Our key idea is to leverage AMR graphs. AMR (Abstract Meaning Representations) is a directed graph that captures the abstract meaning of a sentence. Each node represents a semantic concept in a sentence, and each edge represents a semantic relation between concepts. And we have focus, which is the root node, to represent the main assertion of the sentence. We therefore propose to use AMR back-translation to generate syntactic diverse paraphrases. First, we will use a pre-trained AMR parser to get AMR Graph of a source sentence. Then we will change the focus of the graph. We will randomly sample a node and set it as a new root node, then modify the corresponding edge and their edge labels. Then we will use the AMR graph-to-text generator to generate text from the modified graphs. For the generated text, because they share the same AMR graph structure, so they will have similar semantics. Also, because the text generator would emphasize focus at the beginning of a sentence, their syntax would be a little bit different. By using AMR back-translation, we can get our proposed dataset, ParaAMR. There are around 15 million source sentences in ParaAMR, and there are around 6.9 paraphrases per source sentence. And there\\'s some examples in ParaAMR. Compared to other datasets which use back-translation, we can see that ParaAMR usually generates more syntactic diverse paraphrases. We also present some quantitative analysis for ParaAMR. We have automatic scores, and we also have human evaluation scores. Those scores indicate that ParaAMR has similar semantic similarity scores to other datasets which use back-translation. But ParaAMR has higher syntactic diversity scores. That means ParaAMR is syntactically more diverse compared to existing datasets while preserving good semantic similarity. Then we demonstrate that ParaAMR can benefit several NLP applications. The first application is learning sentence embeddings. We learn sentence embeddings with different paraphrase datasets. And we found that the sentence embeddings learned from ParaAMR can perform better than other datasets in STS testing benchmark. For the second application, we consider syntactic control paraphrase generation. We show that by training with ParaAMR, we can get a paraphrase generator that has better syntactic control. Finally, we consider using paraphrase generator to generate paraphrases for data augmentation for few-shot learning. Since ParaAMR is more syntactically diverse, we observe that ParaAMR can get higher scores for few-shot learning. Here is the conclusion of our work. In this work, we propose ParaAMR, a large-scale, syntactically diverse paraphrase dataset which is constructed by AMR back-translation. And we show that ParaAMR benefits several NLP applications compared to existing paraphrase datasets. Our dataset is available at this link. Thank you.'}\n",
      "{'id': '341', 'prompt_en': 'Answer the following question concisely given the English content: Which latency measures do the authors use?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welche Latenzmessungen verwenden die Autoren?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: A quali misure di latenza fanno ricorso gli autori?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 作者使用了哪些延迟测量方法？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'A'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '342', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/THZqYqwyGP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/THZqYqwyGP.mp4', 'text': 'Hello everyone. My name is Gao Jingsheng. Today I\\'m going to present our paper, \"LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming\". This paper conducted by me, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI. Here is the outline of my presentation. First part is introduction. What is the Open Domain Dialogue? It means a type of conversational exchange between a human and an artificial intelligence system that can cover a range of topics and doesn\\'t have a specific goal, which relies on pre-trained models and larg-scale datasets. The existing large-scale corpora mainly consists of online chat conversations. We can actually divide them into text sources and video sources based on their sources. Currently, the large-scale pre-trained dialogue datasets are mostly text-sourced. Therefore, it is of great significance to construct a large-scale video-sourced dialogue dataset that is closer to real spoken conversation. Existing video-sourced dialogue datasets can be cut up into two groups. Those with scripted conditions as TV and movie scripts, and those without scripts, such as interview datasets. However, these datasets are limited in scale as they rely on manual annotations and instructions. To construct a large-scale dialogue dataset, the key lies in finding an effective matching mechanics that captures the reply-to relationships among speakers. Furthermore, in addition to general open-domain dialogue, personalized dialogue is crucial in developing applications such as virtual streamers and virtual employees. However, the current research on personalized dialogue faces challenges such as utilizing persona information to represent the characteristics, and the lack of session dialogue for each persona. Moreover, [INAUDIBLE 2:05] a multi-party dialogue scenario in which there are more than two interlocutors involved in the conversation. For research on multi-party conversation, the lack of large-scale Chinese multi-party dialogue has been a challenge, and our datasets can help address this issue. In total, the key barriers of existing dialogue datasets are summarized below. To address these barriers, we propose a large-scale personalized dialogue dataset, LiveChat, with a unique automatic dialogue-constructing method. We conduct sufficient experiments on two benchmark tasks: Response Modeling and Addressee Recognition. We further investigate transfer learning of generation models to LiveChat. The second part is our dataset, LiveChat, which is conducted in three steps. Firstly, we scratch origin streaming videos from Chinese TikTok, Douyin. Then we extract audio from videos and transcribe audio into utterances through ASR. Secondly, we collect audience comments and construct dialogues by our reply-to-whom matching method. Thirdly, we collect the persona information for personalized dialogue generation. The persona extraction of LiveChat can be categorized into two parts. The first one are basic profiles by manual labeling and scratching. The second one is [INAUDIBLE 3:38] profiles. We extract them by rules and trained persona classifiers. This is the comparision between our dataset and other existing open-domain dialogue datasets. We can see that our LiveChat is video-sourced with a larger scale. Besides LiveChat and videos [INAUDIBLE 4:01] personal annotations and the longest average sessions. The third part is our experiments. We train retrieval baselines for two tasks. The first task is response modelling. We can conclude that our extracted persona and longer average sessions are persona beneficial to the final result. Besides, both rules and classifier are important to persona extraction. The second task is Addressee Recognition, which shows that single-stream BERT outperforms double-stream BERT, although persona is beneficial to address recognition. Except the two prompt tasks, we also investigate the performance of the pre-trained dialogue models on our LiveChat. Firstly, the performance of BART is better than the other two. It confirms that the domain of our LiveChat is far away from the domains of existing dialogue datasets. The human of evaluation result of LLMs are better in terms of rich informativeness. We also have performed a series of experiments of in-context learning on different shots to study the influence of demonstrations. The performances keep growing as the shots gradually increase. However, when the number of demonstrations exceeds 8shots, the performances of the LLMs slightly decrease due to the random manual selection of demonstrations, which mainly introduce noise. In conclusion, we propose LiveChat, a Chinese video-sourced and personalized dialogue dataset. Experiment on two benchmark tasks show that selected persona profiles and the average sessions per persona are advantageous in learning the speaker’s personalized response. The comparisons between BART and other LLMs have unveiled the distinctiveness of our LiveChat. In the future, we will pay more attention to the efficient transfer learning of LLMs for LiveChat. My presentation is over. Thanks for listening.'}\n",
      "{'id': '344', 'prompt_en': 'Answer the following question concisely given the English content: What are the drawbacks of tree-based methods?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Was sind die Nachteile der baumbasierten Methoden?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono gli svantaggi dei metodi basati su alberi?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 基于树的方法有哪些缺点？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '345', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '346', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '348', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can\\'t make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.'}\n",
      "{'id': '350', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EsdybtoHYe.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EsdybtoHYe.mp4', 'text': 'Hello everyone, and welcome to the presentation of our paper, entitled \"What’s the Meaning of Superhuman Performance in Today’s NLU?\" I\\'m Simone Tedeschi, and this is a joint work with several renowned researchers spanning many institutions around the world. In the last five years, leaderboard-based evaluation has become the de facto standard in NLP, and consequently, the main objective became to reach the top spot in popular benchmarks. Not infrequently, it happens that systems achieve human-level or even superhuman performance in such benchmarks. And we call these benchmarks \"saturated benchmarks\". And these achievements quickly spread in the research community and outside, leading to us the conclusions such as that some tasks are now solved by these models. However, although we know that it\\'s easy, for example, for a calculator to outperform humans in arithmetic tasks, it\\'s still not clear what it means to outperform humans in tasks involving knowledge, reasoning, and inference. Additionally, we all know that these models are brittle in many ways. For example, they are not really able to generalize, they suffer adversarial attacks, they strongly rely on spurious patterns, they lack sensitivity to basic perturbations such as negation, and they are over-sensitive to perturbations that are not very important. So, in light of all these problems, in this paper, we investigate how reliably do the leaderboard scores compare models and humans. To answer this question, we analyze two of the most popular benchmarks in NLP and NLU, namely SuperGLUE and SQuAD. SuperGLUE is a well-known framework for evaluating systems of language understanding and consists of 10 tasks concerning commonsense reasoning, entailment, reading comprehension, and more. In some of these tasks, the human baseline is computed by the authors of the corresponding paper, while for the others, it\\'s computed by the SuperGLUE creators. In this slide, we can see the SuperGLUE leaderboard with humans highlighted in red and the best system highlighted in green. In particular, we can see that humans rank 8th, and they are outperformed by systems on 6 out of 10 tasks. Furthermore, the best system outperformed humans by 1.5 points on average, and in some tasks, like MultiRC, systems outperformed humans by a very large margin, namely, more than 10 exact match accuracy points. Similarly, on both versions of the SQuAD benchmark — that is, reading compression datasets focused on question answering — humans are largely outperformed by the systems ranking 16th and 13th on the two benchmarks, respectively. However, by manually inspecting these datasets, we discovered several sources of error that make the comparison between humans and systems unfair. The first glaring mistake is that systems and humans are evaluated on different sets. In particular, humans are almost always evaluated on a small or very small subset of the actual test set. For instance, on BoolQ, systems are evaluated on the full test set made of more than 3,000 instances, while humans are evaluated on a small subset of just 100 instances. Additionally, we also discovered several errors in the ground-truth answers. For instance, we can look at the first example in this slide, extracted from the Recognizing Textual Entailment data set. The premise says that in most Pacifici countries, there are very few women in parliament, while the hypothesis says that women are poorly represented in parliament. Of course, given the very specific premise, we don\\'t have enough evidence to claim that the very general hypothesis is entailed. So this is an error, and there are similar errors in Word-in-Context, Commitment Bank, and all other datasets. So, concerning the comparisons between humans and systems, here it is important to highlight that while systems can find spurious correlations between training and test instances benefiting from specific patterns of errors, humans cannot. Additionally, researchers in NLP often vaguely estimate human performance. Indeed, the term \"human baseline\" is often used, and it seems to imply that systems need to beat it. Specifically, simple aggregation methods such as average or majority voting are used, and instead it would be interesting to compare the scores of the best systems with that of the best possible humans, like it is done in other areas of artificial intelligence. However, even assuming that the score of the best human in the pool is reported, can we be sure that it would be comparable with that of the best possible human in general? And by analyzing the benchmarks, we discovered that pay rates varied considerably across the various tasks, and in some cases, these are very low, such as $3.6 per hour in ReCoRD, or even unknown. In particular, if humans are not adequately motivated, the resulting quality will be low. And we argue that data sets constructed under these conditions should not be used for that kind of human-to-system comparisons. Finally, along the same lines, we also discovered the details about the annotator pool are often omitted. Specifically, it\\'s often unknown how many annotators were hired, following what process, what is their cultural background, and so on. And without all this information, we argue that claims about superhuman performance are not scientifically meaningful. So, to summarize, in this presentation, we discussed the meaning of superhuman performance in NLU and explain why such claims are not yet grounded. Read our paper if you want to know more about the consequences of the identified issues. And in our paper we also provide recommendations to avoid repeating the same mistakes and construct more reliable benchmarks. Thanks for your attention.'}\n",
      "{'id': '351', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '352', 'prompt_en': 'Answer the following question concisely given the English content: What does ABC-Eval stand for?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wofür steht ABC-Eval?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Cosa significa ABC-Eval?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： ABC-Eval 代表什么？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.\"}\n",
      "{'id': '353', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EtEXMcVDjF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EtEXMcVDjF.mp4', 'text': 'Hello people from ACL 2023. Today, I\\'m going to introduce the paper \"Python Code Generation by Asking Clarification Questions\" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych. Motivation: Code generation and program synthesis (given natural language description) is a heated research topic. However, state-of-the-art methods failed to address an important challenge, and that challenge is input underspecification. In the motivating example, only the classifier—which is the regressor—is mentioned with multiple specifications missing. The issue is prevalent in real-world use cases, and it\\'s important to train code generation models. Interaction, for example, asking clarification questions, is a good paradigm to address this challenge. However, before addressing this challenge, we have to figure out two challenges. First, missing specifications can happen at various levels, and second, it\\'s not obvious how to identify if an NLD carries information about specifications at any level mentioned above or not. Therefore, in this paper, we introduce interactivity into code generation. We hypothesize that through interaction, we can gather more specifications to alleviate the problem of underspecification. We also propose the task of generating code by asking clarification questions. We focus on clarifying operation-level specifications. We propose a method to create CodeClarQA, which is a synthetic dataset with clarifications on key operations, and we propose a pipeline of code generation by asking clarification questions. Here\\'s a table of contents. Dataset creation. We identify key operations and corresponding documentation from the code. And then we represent them in latent space using their schemata, compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. Notice that we also hire annotators to annotate the validation set and the test set. We adopt templates to create CQAs for missing key operations. And that goes to two types of questions, which is yes-or-no questions or multiple-choice questions. Example of key operations. We\\'re using heuristics to extract key operations based on the code knowledge graph generated by Graph4Code, and we show the generated graph of the example. Each node is an operation where the key operations are marked in red and the rest in gray. Edges show the data flow. Here\\'s an example of identifying if a key operation is missing or aligned. Notice that a schema is defined as a set of important elements of a document, and every schema element is either in a verb/key-phrase/relation form, or in the form of only key-phrases. In this figure, schema element pairs with the highest similarity scores are highlighted if the operation is predicted aligned. Then let\\'s check out the results of identifying missing key operations. It seems like we did quite well using our methods, and we also noticed that MPNet has the best performance of identifying missing key operations among all these models. Then what about the errors? Let\\'s do some error analysis. The rare case of false positive predictions suggests that our method to generate CQAs effectively creates CQAs for missing key operations. We also noticed some common errors which reflects the challenges and potential directions to improve our method, including taxonomy, that aligned operations might require clarification to be distinguished from operations with similar names, and argument, where we use documentation of operations instead of using argument values as well. Then here\\'s the pipeline of the CQ-driven code generation. We have a Clarification Need Predictor, a Question Selector, and a Code Generator. And we have the module experiment results. We actually have two hypotheses: that first, our task is more challenging than existing CQ ranking tasks, which is supported by CQ ranking results. And we also have the hypothesis that clarifications help code generation, which is supported by the code generation results. We also test our pipeline. We see that model performances on all evaluation metrics, including with more high-ranked CQs being answered and included, increases. However, there\\'s an opposite trend of unanswered clarifications, and at the same time, our pipeline is still underperforming the model-only trainer NLDs and code, which is expected as we fine-tune the models on all CQAs and CQ ranking task is a challenge. Then, at the end, we do some analysis. The first thing is, \"Is clarified key operations the reason for better generated code?\" and seemingly, yes. We also gave some examples of predictions, and see that training with Oracle CQAs leads to predictions close to the ground truth with only minor differences. However, the task is challenging as the top five ranked CQs do not include CQs in the reference CQAs, leading to the pipeline prediction including a call of confusion matrix missing the classes mentioned here. So thank you for listening. Please check out our paper and code, and we are looking for your feedback.'}\n",
      "{'id': '354', 'prompt_en': 'Answer the following question concisely given the English content: Until which year is the performance delta between CoNLL-2003 and CoNLL++ higher than 5 percentage points?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Bis zu welchem Jahr ist das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ höher als 5 Prozentpunkte?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Fino a quale anno la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 直到哪一年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于 5 个百分点？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.\"}\n",
      "{'id': '356', 'prompt_en': 'Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Welcher Universität gehören die Autoren an?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quali sono le affiliazioni degli autori dell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi! My name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they\\'re not ordered. That\\'s why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That\\'s because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '357', 'prompt_en': 'Answer the following question concisely given the English content: What is the name of the speaker?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie heißt der/die Referent*in?', 'prompt_it': 'Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Qual è il nome della relatrice o del relatore?', 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University. I\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.'}\n",
      "{'id': '358', 'prompt_en': 'Answer the following question concisely given the English content: How many authors are involved in the paper?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie viele Autoren sind an der Arbeit beteiligt?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Quanti autori sono coinvolti nell'articolo?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？', 'metadata': {'qa_origin': 'General', 'qa_type': 'AV'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.'}\n",
      "{'id': '359', 'prompt_en': 'Answer the following question concisely given the English content: To which dedicated simulST architecture is the approach compared?', 'prompt_de': 'Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Mit welcher SimulST-Architektur wird der Ansatz verglichen?', 'prompt_it': \"Rispondi in modo conciso alla seguente domanda dato il contenuto inglese: Con quale architettura simulST dedicata viene confrontato l'approccio?\", 'prompt_zh': '根据所给的英文内容，简要回答以下问题： 该方法与哪种专用的 simulST 架构进行了比较？', 'metadata': {'qa_origin': 'Transcript', 'qa_type': 'V'}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.'}\n",
      "{'id': '360', 'prompt_en': 'Transcribe the English content.', 'prompt_de': None, 'prompt_it': None, 'prompt_zh': None, 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.\"}\n",
      "{'id': '361', 'prompt_en': 'Summarize the English content in an abstract of approximately 200 words.', 'prompt_de': 'Fasse den englischen Inhalt in einem Abstract mit ungefähr 200 Wörtern zusammen.', 'prompt_it': 'Riassumi il contenuto inglese in un abstract di circa 200 parole.', 'prompt_zh': '用400个字左右概括所给的英语内容。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/AXxQRZeHcK.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/AXxQRZeHcK.mp4', 'text': 'Hi. My name is Armineh Nourbakhsh. I\\'m a PhD student at the Language Technologies Institute at Carnegie Mellon University. I\\'m also a research director at the JP Morgan AI Research team. The work I\\'m presenting today is titled \"CounterComp\" and is focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. By multi-step quantitative reasoning, we\\'re specifically focused on the question answering task. So if you\\'re given a financial table such as the one displayed on the right-hand side of the slide, you\\'d be able to ask different questions about the data in this table. For instance, you could ask \"What was the net change in revenue from 2019 to 2020?\", and the answer would be a certain number that\\'s derived from executing one or more arithmetic operations. And this is what we mean by multi-step quantitative reasoning when the output includes multiple arithmetic operations. Unfortunately, state-of-the-art neural models don\\'t perform very well on these tasks, especially when the output has more than two steps. And the reason is because these models are memorizing spurious patterns. So, going back to the table, you might notice that there are certain tokens that are repeated in the various questions in the input; for instance, a token \"2019\". If the model sees this token repeatedly during training, it might start to mistakenly associate it with a very common operation in the output, such as the subtraction operation in this case. And we want the model to avoid that as much as possible. So ideally, we\\'d want a model that attends to appropriate tokens in the input when it\\'s generating certain operations in the output. And we could certainly add supervision signal that encourages the model to do so. But of course, adding additional supervision is always costly in terms of human effort, and we\\'d like to avoid that as much as possible. So we look at this challenge in a different light. We go back to the table that I showed you in the first slide and look at the questions again. If you pay attention to the examples provided here, you\\'ll notice that the components of the question that actually matter in terms of the operations and the operations that are used in the output are somewhat interchangeable, in the sense that, if you change those components, the output might also slightly change. As an example, in the first question here, if you change \"net change\" or switch \"net change\" to \"percent change\", effectively what you do is you add a division and multiplication to the output. This basically means that these components can be used to mine counterfactual scenarios from the input. And that is exactly what we do. Given a training sample, we treat it as an anchor, and we mine what we call positive and negative examples from the training set. A positive example would be an example where an intervention in the question would not yield any change in the output, and a negative example would be an example where the intervention in the question would yield a change in the output. And we use these triplets to add an auxiliary metric learning loss, to the training procedure. And that auxiliary metric learning loss has a dynamic margin that is actually measuring the extent of change or intervention in the questions between each pair, and using that to adjust the metric learning loss accordingly. We show that adding this auxiliary loss to three state-of-the-art baselines consistently improves their performance, especially when the number of reasoning steps grows beyond two. This is performance on in-distribution samples, meaning the model is trained on a dataset and tested on the same dataset. But more importantly, we also show that adding an auxiliary metric learning loss improves performance on out-of-distribution samples, either in cases where the model is trained in one dataset and tested on other datasets, or the model is trained on one dataset and tested on examples from the same dataset that were never seen during training, which is basically exactly what compositional generalization aims to obtain. We also show qualitatively that adding the CounterComp loss helps the model attend to more meaningful tokens during training, meaningful in the sense that they relate to more meaningful operational terms in the output. Here is the main references using these in this presentation. For more information, make sure to check out our poster, or if you have any questions, feel free to reach out to the contact listed here. I\\'d like to thank my co-authors, my advisor at CMU, and my co-advisor at JP Morgan, and I would like to thank you all.'}\n",
      "{'id': '3', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JhbtCwcsWY.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JhbtCwcsWY.mp4', 'text': \"Hi!\\nWelcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level.\\nMy name is Regina Stodden, and I will guide you through the first part of the presentation.\\nLet's first define text simplification.\\nText simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers.\\nTo train a text simplification model we require parallel pairs of text, for example of documents or sentences.\\nAnd the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.\\nTo simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words.\\nWe now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora.\\nSo for example, these corpora here are too small to train a text simplification model on.\\nThe other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments.\\nTherefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web.\\nDEPLAIN-apa is based on news texts.\\nIn DEPLAIN-apa, we aligned 483 documents all manually.\\nIt results in roughly 13,000 parallel sentence pairs.\\nFor DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods.\\nIn total we result in 30,450 sentence pairs.\\nWe analyzed our sentence pairs a little bit more, so for example, on the type of simplification.\\nAs you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts.\\nOn all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification.\\nFurthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations.\\nSo for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus.\\nOn the other hand, in the web corpus we have much more rephrasings.\\nSo let's now see what we can do with this corpus.\\nHello, I am Omar and now I will talk about the use cases for our data set DEPLAIN.\\nSo for the first use case, we can evaluate automatic alignment methods.\\nIn the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents.\\nBut in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level.\\nAnd now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods.\\nAnd we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper.\\nAt the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign.\\nAnd you can also find the code to run this method on your own documents in the paper.\\nThe second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text.\\nWe have fine-tuned two different models.\\nWe have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications.\\nYou can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.\\nWe concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future.\\nThank you so much for your attention and we hope to meet all of you during the conference.\\nThank you.\"}\n",
      "{'id': '14', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/MjDvRpkOFq.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/MjDvRpkOFq.mp4', 'text': 'Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination.\\nAs you may know, there are different dependency structures assumed by different theories and corpus approaches.\\nSo for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure.\\nSo in this case, Lisa.\\nA similar approach is assumed in Igor Mel\\'čuk\\'s meaning text theory, where again, the whole coordinate structure is headed by the first conjuct.\\nSo these two approaches are asymmetric.\\nRight.\\nThey single out one of the conjuncts.\\nNow those are asymmetric approaches to coordinate structures, such as the Prague approach.\\nThe conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction.\\nSo, we get some dependencies from end to all the conjuncts.\\nAnd finally, there\\'s also a multi-headed approach that\\'s used, for example, in the Hudson\\'s Word Grammar, where they say all conjuncts are heads of the coordinate structure.\\nSo we get dependencies from the governor.\\nHere loves to all conjuncts separately: Lisa, Bart, and Maggie.\\nNow the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two.\\nOK.\\nThe argument is based on the principle of dependency length minimization that I will explain on the basis of these examples.\\nSo in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away.\\nSo \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse.\\nRight?\\nBecause here between the verb and the direct object is an adjunct: \"yesterday\".\\nHowever, this effect may be ameliorated when the direct object is very heavy and very long.\\nBecause then it can be moved to the position after the adjunct.\\nThis is illustrated here.\\nSo both these sentences are fine.\\n\"Marge read this absolutely fascinating book about bees yesterday.\"\\nIt\\'s okay the way instead of \"it\", we have this long NP.\\nBut it\\'s also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\"\\nSo the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred.\\nSo these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures.\\nSo here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it\\'s 11.\\nWhen you swap these two constituents, the sum of these two dependencies becomes 6.\\nSo instead of 11, 6 is much shorter.\\nThat\\'s why this sounds quite okay.\\nRight?\\nIt violates one principle, but it satisfies another one.\\nOk.\\nSo what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn\\'t you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter.\\nSo, \"salt and pepper\" and not \"pepper and salt\", measured in syllables.\\nAnd, also the observation that was made in parsing that this tendency grows with length difference.\\nSo when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right?\\nSo the proportion is bigger of the left short conjunct.\\nBut what\\'s novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent.\\nRight?\\nSo the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left.\\nIt\\'s absent in the second example \"Homer came and sneezed.\"\\nHere we have coordination of two verbs and there\\'s no outsides, external governor.\\nIn such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts.\\nHowever, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears.\\nSo we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column.\\nSo I\\'ll concentrate on the right one.\\nWhat we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences.\\nBut when the governor is on the right this tendency disappears.\\nAnd we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two.\\nSo see the paper for the full arguments.\\nAnd talk to us about at the poster session.\\nThank you.'}\n",
      "{'id': '47', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ICWfTnUMio.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ICWfTnUMio.mp4', 'text': 'Hi, I\\'m Shangbin, PhD student in the University of Washington.\\nToday I\\'m presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\".\\nSo language models are trained on large scale web crawl data.\\nPolitical news media are well covered in their pretraining data.\\nAccording to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data.\\nThis has created a mixed blessing for language model applications.\\nSo on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas.\\nOn the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications.\\nTo this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases?\\nSecondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications?\\nSo specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test.\\nThis ensures us to do automatic evaluation well grounded in political science literature.\\nSo some preliminary results demonstrate that first, language models do have varying political leanings.\\nThey occupy all four quadrants on the political campus.\\nWe can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants.\\nSecondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data.\\nSo we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning.\\nBy further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift.\\nFor example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases.\\nAnd we also try to investigate whether language models can pick up the polarisation that\\'s prevalent in our modern society.\\nSo we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States.\\nWe separately pretrain language models on the two different temporal corpora.\\nWe can see that language models generally had a political leaning that is further away from the centre after 2017.\\nSo this indicates that language models can also pick up the polarisation in our society.\\nSo last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications.\\nSo we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern.\\nFor example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society.\\nAnd vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities.\\nSimilar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.\\nWe further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories.\\nThere are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models.\\nFor example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control.\\nSo this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings.\\nSo a little bit of discussion.\\nWe would also like to highlight that we expose the unique dilemma regarding language model political biases.\\nIt\\'s like between Scylla and Charybdis.\\nSo if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues.\\nIf we do try to sanitaze somehow, we would also risk censorship, or exclusion.\\nAnd it\\'s incredibly hard to determine what is actually neutral and should be retaining language monitoring data.\\nSo it\\'s kind of like the electric trolley problem.\\nOk, great.\\nI think that\\'s pretty much all I have for today.\\nThank you for your time.'}\n",
      "{'id': '96', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/DyXpuURBMP.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/DyXpuURBMP.mp4', 'text': \"Hi everyone.\\nI'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models.\\nThis work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap.\\nSo let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content.\\nYou might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones.\\nWhere prospective API is able to detect correctly toxic instances.\\nBut that's not really the case for Aditya Sharma.\\nWhere prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts.\\nThis is an example of a design bias where we see systematic performance differences of technology between populations.\\nDesign biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers.\\nPositionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.\\nThis is a concept widely used in critical studies, specifically in feminist and queer academic spaces.\\nAnd as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make.\\nAnd so one question that people might ask is, do datasets and models have positionality?\\nAnd we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others.\\nSo prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality.\\nHowever these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs.\\nSo to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models.\\nWe do this through our framework NLPositionality.\\nOur framework works in two main steps.\\nThe first step is to re annotate data sets with diverse annotators.\\nAnd we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared.\\nAnd so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data.\\nWe then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions.\\nOur frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator.\\nIn Live in the Wild is an online experimentation platform where we can recruit divers volunteers.\\nCompared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data.\\nWe host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is.\\nAfterwards to stay engaged in the study, they can compare their responses to an AI and others.\\nWe've then compared these, annotations with Social Chemistry, Delphi and GPT 4.\\nWe then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech.\\nWe then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4.\\nOur study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries.\\nSo now we're better equipped to answer who do NLP datasets and models align with the most.\\nWe find that there is positionality in NLP.\\nFor example, we find that data sets and models are most aligned to English speaking countries.\\nSo for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries.\\nWe find that Dynahate is also most aligned to English speaking countries.\\nWe also find most additional alignment with people who have a college education.\\nSo for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education.\\nHowever, when models and data sets are aligned to specific populations, some are inevitably left behind.\\nAn example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts.\\nWe find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well.\\nSo, given that there is positionality in NLP, what can we do about it?\\nSo we have a few recommendations for this.\\nFirst one is keep a record of all relevant design choices throughout the research process.\\nAnd the other is to do NLP research with the lens of perspectivism.\\nOur third recommendation is to build specialised datasets and models within 4 specific communities.\\nAnd a good example of this is the Masakhani initiative.\\nI mean, we want to emphasise that inclusive NLP isn't just making.\\nYou know, all technologies work for everyone.\\nAnd so that concludes our presentation.\\nBut if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper.\\nThank you.\"}\n",
      "{'id': '99', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ccpXHNfaoy.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ccpXHNfaoy.mp4', 'text': 'Hi, I\\'m Siyu Yuan from Fudan University.\\nI\\'m here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\".\\nIn everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts.\\nPrevious work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\".\\nAnd show that large language models can effectively decompose goals into steps.\\nHowever, previous work mainly focuses on planning for the abstract goals of stereotypical activities.\\nPlanning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied.\\nIn this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning.\\nAn abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.\\nA good planner should write scripts that are reasonable and faithful to constraints.\\nIn this paper, we first evaluate and improve the constrained language planning ability of large language models.\\nSince no dataset of specific goals exists to support our study, we have to acquire these goals first.\\nAs shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT.\\nWe sample 100 specific goals and evaluate the scripts generated from large language models.\\nThis table reports the overall accuracy of the results.\\nWe find that all language models achieve unsatisfactory results on planning for specific goals.\\nThen we conduct detailed analysis to investigate why learning models fail.\\nResults in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed.\\nWe dig into a more fine-grained topic categories of constraints defined in wikiHow.\\nThe heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories.\\nPrevious studies have shown that the output quality of language models falls in high variance, leading to bad performance.\\nThus, we adopt the idea of over-generate-then-filter to improve generation quality.\\nWe first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals.\\nThen, InstructGPT over-generates K scripts for specific goals.\\nNext, a filter model is developed to select the faithful scripts.\\nWe convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity.\\nIn addition, we reward the script that contains the keywords of the target constraint.\\nWe only keep the script if the target goal scores the highest in the goal set.\\nWith our method, InstructGPT can generate scripts of higher quality.\\nOur method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint.\\nSince large language models are costly to deploy, it\\'s essential to enable language planning ability of smaller and specialized models.\\nCreating the dataset is an essential step to this end.\\nHowever, previous studies do not enable planning for specific goals and manual dataset annotation is expensive.\\nThus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models.\\nWe appy our method for building a dataset of constrained language planning, named as CoScript.\\nIn total, we generate 55,000 specific goals with scripts.\\nTo ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples.\\nThis figure shows the constraint distribution of CoScript.\\nWe find CoScript shows high pluralism in the generated specific goals.\\nWith CoScript we can try smaller but specialized models for constrained language planning.\\nWe find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.\\nIn summary, we establish the constrained language planning problem.\\nWe evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models.\\nWe use large language models to generate a high-quality script dataset, CoScript, for constrained language planning.\\nWe hope the CoScript dataset can be a valuable resource to advance research on language planning.\\nThanks for your time.\\nPlease find more details of CoScript in our paper.'}\n",
      "{'id': '112', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/QTlIuodOsA.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/QTlIuodOsA.mp4', 'text': \"Hello everyone, my name is Shuheng.\\nToday I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023?\\nLet's get started.\\nOur paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task.\\nWe observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems.\\nFirstly, can these models generalise to modern data?\\nAnd when we develop new taggers, what is needed for good generalization?\\nAt the same time, if we do observe poor generalization, what causes the performance drop of these models?\\nTo investigate these problems, we developed the CoNLL++ Dataset.\\nThis is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines.\\nWe then fine-tuned over 20 models on CoNLL-2003.\\nWe evaluated them on both the CoNLL-03 test sets and the CoNLL++.\\nAnd last but not least, we calculated the percentage change in F1 to assess the generalization of each model.\\nSo what is needed for a good generalization?\\nThroughout experiments we found that there are three main ingredients that are needed.\\nThe first one is the model architecture.\\nThrough our experiments we found that the transformer models normally generalize better to new data.\\nThe second ingredient is the model size.\\nWe found that usually larger models lead to better generalization.\\nAnd last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task.\\nHere we also found that more fine tuning examples, actually also leads to better generalization.\\nTo our next question, what causes the performance drop of some models, We had two hypothesis.\\nThe first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set.\\nThe second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data.\\nFor data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one.\\nThis means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns.\\nAnd this shows us that adaptive overfitting in this case is not observed.\\nSo what about temporal drift then?\\nFor temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift.\\nOur conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples.\\nAnd these goes hand in hand, we can't just have one ingredient but throw out the others.\\nAt the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years.\\nSo going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023?\\nAnd we found that the answer is actually a resounding yes.\\nWe hope our paper calls for more research on how to improve generalizations of the models.\\nAnd lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me.\\nThank you so much.\"}\n",
      "{'id': '142', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/miPjvjWOvI.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/miPjvjWOvI.mp4', 'text': 'Hi!\\nI\\'m going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus.\\nMy name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis.\\nOur goal is to understand users’ language when they want to make a choice.\\nConsider this alternative question.\\n\"Did you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\"\\nHere, a user wants to select between one of these two songs.\\nThe most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\".\\nBut sometimes an indirect reference is more appropriate to have a more natural conversation.\\nThis could happen when the user cannot remember the name of the song.\\nOr the pronunciations are too similar to each other and hard to disambiguate.\\nOr when the user wants to specify a preference.\\nHere are some examples of indirect references for example, \"the newer one\" or \"the song that\\'s not energetic.\"\\nThis is an important problem in conversational systems and also for benchmarking LLMs\\' entity understanding.\\nWe\\'re not aware of a larger-scale public data set for the task, so we collect one using crowd annotation.\\nOur data set covers three different domains: music, books, and recipes.\\nOur data set collection methodology emphasizes informality using a cartoon completion setup.\\nThe cartoon has three speech bubbles.\\nIn the first bubble, Bob says, \"Remember that song we were listening to yesterday?\"\\nAnd with that, Bob sets the dialogue context.\\nIn the second speech bubble, Alice says, \"Do you mean \\'Easy on Me\\' or \\'I Gotta Feeling\\'?\"\\nWhich is the alternative question.\\nAnd in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\"\\nWe provide the first and second speech bubbles automatically, but the third one is filled in by the annotator.\\nThe first speech bubble is chosen from a few manual prompts per domain.\\nThe second one, which is the alternative question is generated as follows.\\nWe always use a simple template.\\nDo you mean A or B?\\nWhere A and B are samples from Wikipedia.\\nHere are the different sampling methods we\\'ve used.\\nWhen we move higher in the list, the entities become more similar to each other and it\\'s usually harder to make the disambiguation.\\nThe first one is uniform at random.\\nThe second one is when the entities have similar titles, for example, two books with the name \"The Return\".\\nThe third one is when they have similar descriptions on Wikipedia.\\nAnd finally when they have similar info boxes or attributes on Wikipedia.\\nFor example, the same genre or the same artist for a song.\\nWhen we show this alternative question to the annotators, they know the name of these entities, but they don\\'t necessarily know about the entities.\\nSo what we do is that we show some background knowledge about the two entities.\\nFor songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song.\\nHere\\'s for example, the Google search result for the song \"Easy on Me.\"\\nFor the recipes and books domain, we show some background text from Wikipedia.\\nFor recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like.\\nThen, we asked the annotators to pick one of these entities, for example, here\\'s the first one, and describe them using three to five indirect referring expressions.\\nFor example, the one with the piano music.\\nHere are some examples from our dataset.\\nFor example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on.\\nThe AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions.\\nResults with T5 XL model are summarized below.\\nIf the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it\\'s around 92 to 95%.\\nBut this is not realistic.\\nIf the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic.\\nFor example, when the language model retrieves the background knowledge.\\nIf the language model has access only to entity names, then the accuracy is only 60%, so there\\'s a lot of room for improvement.\\nWe\\'ve also shown that the models are domain-generalizable.\\nHere is a link to our dataset.\\nThanks.'}\n",
      "{'id': '148', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/OiqEWDVtWk.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/OiqEWDVtWk.mp4', 'text': 'Hi, I\\'m Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi.\\nWhat is simultaneous speech translation?\\nSimultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication.\\nAnd what are the problems of the current SimulST models?\\nSpecific architectures are usually trained, introducing additional modules to be optimized.\\nLong and complicated training procedures, for example, training involving different optimization objectives.\\nAnd training and maintaining several models to reach different latency regimes.\\nFor example, training a model with an average of one second latency and another one with two seconds latency, and so on.\\nSo what is our solution?\\nFirst, to use already existing offline ST models without re-training or adopting specific architecture for SimulST.\\nUse only one model for every latency regime and handle latency through specific parameters.\\nAnd leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output.\\nThat is the cross-attention mechanism, and you can see an example on the right.\\nOur solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to.\\nA word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable.\\nFor example, if we receive a speech chunk containing \"I\\'m going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we\\'ll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames.\\nThis means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk.\\nIf we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames.\\nThis means that these three words will be emitted.\\nIf we look at the main results of EDAtt, we\\'ll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model\\'s computational times to predict the output.\\nSo we want our curves to be as high as possible on this plot.\\nBut also we want that they are shifted on the left.\\nAnd we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement.\\nAnd we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation.\\nThese are all the results of the simultaneous speech translation strategy on German.\\nAnd we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left.\\nAnd we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy.\\nIf you want to discover more results, read our paper.\\nAnd we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work.\\nThanks for your attention.'}\n",
      "{'id': '151', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xhjBHGVOyQ.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xhjBHGVOyQ.mp4', 'text': \"Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning.\\nSo with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way.\\nRecently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions.\\nHowever, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out.\\nTherefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks.\\nAdditionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal.\\nThere exist more than 1600 language-only instruction tasks.\\nHowever, there is no large-scale publicly-available multi-modal instruction task.\\nTherefore, this motivates us to build a multi-modal instruction tuning dataset.\\nHere we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories.\\nThese tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions.\\nFor investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model.\\nOFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box.\\nHere we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types.\\nWe follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format.\\nIn which the input text, images, instructions and bounding boxes are represented in the same token space.\\nOk, now I'm going to talk about multi-modal instruction tuning.\\nSo for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task.\\nFor testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups.\\nWe use all the instances in the test split for each task.\\nIn addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP.\\nSo we use pre-trained OFA large model as a base model.\\nDuring training, we mix all the instances for all the tasks.\\nEach instance is randomly combined with one of its five instruction templates.\\nSo during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions.\\nIn each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments.\\nIf the task is a multi-model classification task, we report accuracy.\\nIf it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well.\\nWe also introduce an additional evaluation metric called sensitivity.\\nSo this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction.\\nHere is our main result.\\nAs we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks.\\nAlso, transfer learning from natural instruction dataset can benefit instruction tuning.\\nHere we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity.\\nSo we also did one experiment.\\nWe use one instruction versus 5 instruction.\\nAs we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot.\\nSo this shows the effect of different fine-tuning strategies on the model sensitivity.\\nAs we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model.\\nWe also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset.\\nSo overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits.\\nWe design a new metric called sensitivity.\\nSo one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them.\\nSo this is a QR code for our data and model.\\nThank you.\"}\n",
      "{'id': '159', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/vrydRuOXbT.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/vrydRuOXbT.mp4', 'text': \"Hi, everyone.\\nI'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper.\\nLanguage model acceptability judgments are not always robust to context.\\nThis is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.\\nSo in this work, we revisit the minimal pair paradigms.\\nSo the minimal pair paradigm basically evaluates language models on top of acceptability judgments.\\nWhich can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs.\\nAnd in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence.\\nAnd then the hope is that the model, basically, puts more probability to the acceptable sentence.\\nThe current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences.\\nThese days large language models are coming up with longer and longer context windows.\\nSo it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here.\\nWe're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences.\\nSo that is the approach.\\nSo what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets.\\nSo for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case.\\nAnd what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure.\\nWe extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query.\\nSo we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability.\\nAnd we can also do the same by choosing sentences from a different subset or a different data set.\\nSo that is what we call as the mismatch scenario.\\nSo here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with.\\nAnd we can do the same for unacceptability case.\\nFinally, we can choose sentences from a completely unrelated domain such as Wikipedia.\\nSo this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at.\\nSo how does the model do?\\nSo first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length.\\nWe increase the context length toward up to 1024 for to max out OPT and GPT 2 models.\\nAnd we saw here in the orange dotted line, the MPP judgments are relatively stable.\\nNow, what happens when we choose sentences from the same data set?\\nSo here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset.\\nAnd there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.\\nBut when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable.\\nNow this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window.\\nSo why does the match prefix affect the language model judgement so much?\\nSo we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input.\\nAnd after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print.\\nBasically, we find that the models are sensitive to the perturbed sentences in similar ways.\\nThat is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion.\\nSo, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences.\\nAnd the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window.\\nPlease read our paper for more details of our experiments.\\nThank you for listening.\"}\n",
      "{'id': '170', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/ERmKpJPPDc.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/ERmKpJPPDc.mp4', 'text': 'Hello everyone, my name is Yusen Zhang from the Penn State University.\\nToday I\\'m going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\".\\nSo, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus.\\nAnd Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations.\\nAs shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera.\\nExisting cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications.\\nFor instance, there are lots of coverage on certain natural languages.\\nBut Chinese is missing and lack of coverage on certain meaning representation.\\nThe Lambda calculus is missing, or they\\'re only evaluated on certain neural models.\\nFor example, there\\'s only one single model to evaluate them.\\nSo to this end we propose XSemPLR.\\nWe provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations.\\nIt contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families.\\nAnd to better evaluate our benchmark, we consider the six settings for training and evaluation.\\nThe first one is Translate-Test.\\nWe use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation.\\nAnd for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL.\\nAnd we\\'ll also test Monolingual Model.\\nIn this setting, the source language is the same as target language, for example German to German or English to English.\\nWe also test Monolingual Few-shot setting by training monolingual models with only 10% of training data.\\nAnd we test Multilingual Model which we train one multilingual model for all languages.\\nFor example, we put the German, English, Chinese queries together to train a multilingual model.\\nAnd during inference we can use this model to translate German queries or Chinese queries, et cetera.\\nAnd we also consider Cross-lingual Zero-shot and Few-shot transfer.\\nWe train on one source language and transfer to another language.\\nSo during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output.\\nAnd we also find many interesting results.\\nSo, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR.\\nAnd, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5.\\nWe found that Encoder-Decoder obtains the best performance on all nine datasets.\\nAnd we evaluate on mT5 and XLM-R + PTR on multilingual setting.\\nWe found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages.\\nWe found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets.\\nI think this is known as the \"Curse of Multilinguality\".\\nWe also compare the cross-language performance gap.\\nIn this figure, the blue line is Cross-lingual Few-shot transfer.\\nThe orange line is Cross-lingual Zero-shot transfer.\\nWhile the green line is the Monolingual Setting.\\nWe found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly.\\nWe also find some other interesting findings.\\nFor example, Encoder-Decoder outperforms previous work or achieves comparable results.\\nPretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.\\nTo sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.\\nWe conduct a comprehensive benchmark study on three representative types of multilingual language models.\\nAnd our results show many interesting findings.\\nAnd et cetera.\\nAnd welcome to visit our paper and code.\\nThanks for listening.'}\n",
      "{'id': '239', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/myfXyntFYL.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/myfXyntFYL.mp4', 'text': 'Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\"\\nThis is joint work with my colleagues from Google Translate.\\nPaLM is a 540 billion-parameter large language model presented last year in 2022.\\nIt\\'s trained on a large collection of text, comprising 780 billion tokens.\\nAt the time of publication, it achieved state-of-the-art in hundreds of NLP tasks.\\nIn this work, we present the first systematic study of large language model prompting for machine translation.\\nWe evaluated the transition capability of such models using the best practices of the MT community.\\nThis involves using the latest test sets to avoid an overlap of the test data with the training data of the language model.\\nAnd we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation.\\nWe use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results.\\nFinally, we provide some recommendations for prompt selection strategies.\\nThe prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence.\\nThe majority of sentences 516 out of 1,000.\\nThe difference observed is of more than one BLEURT points.\\nAnd this can go, in extreme cases, up to 40 BLEURT points.\\nSo, it\\'s important to select a good prompting strategy.\\nIn our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it\\'s in.\\nSo in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon.\\nWe saw that the actual form of the prompting doesn\\'t have a big influence in the case of several short promptings.\\nIt\\'s crucial for zero and one-shot prompting.\\nAnd when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting.\\nIt\\'s the examples that carry most of the weight.\\nThe summary of our experimental results is that the example quality is more important than the similarity to the source sentence.\\nSo it\\'s important to select the examples from high-quality translations.\\nIn particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data.\\nThe dev data is much more curated, and with higher quality than the training data, that it\\'s more noisy.\\nAnd their results so a better performance when using the dev data.\\nNevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations.\\nBut, PaLM comes pretty close to a commercial system.\\nIn our case, we chose to evaluate with Google Translate.\\nThe insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy.\\nSo, in particular, the most common errors are omission errors.\\nSo, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation.\\nHowever, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy.\\nAnd that\\'s it for this really short overview.\\nFor more details, please come to the full presentation of the paper.\\nThank you very much.'}\n",
      "{'id': '240', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rOwZgUjcwB.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rOwZgUjcwB.mp4', 'text': 'Hello, I am Dawei, a PhD student at Saarland University in Germany.\\nIn this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\"\\nThis is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.\\nI\\'d like to begin with a brief introduction to weak supervision and weakly supervised learning.\\nIn weak supervision, you do not manually label the data.\\nInstead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right.\\nWhen compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect.\\nIf we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.\\nIn weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.\\nIn recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets.\\nTechnically, this claim is not wrong, but there\\'s a catch, which is that people do assume that there\\'s an additional clean validation set available for model selection.\\nWe can\\'t stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning.\\nBut like an elephant in the room this necessity is often overlooked.\\nThe aforementioned doubt is asked to ask three research questions.\\nFirst, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead?\\nSecond, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need?\\nFinally, should we only use the clean samples for validation, or there are better ways to utilize them?\\nWe addressed these research questions in our work and our findings are as follows.\\nFirst, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly.\\nOtherwise, there is a large performance drop.\\nAs shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless.\\nThis indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked.\\nOur second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left.\\nTypically we only need 20 samples per class to attain high performance.\\nBut that\\'s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance.\\nThe right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only.\\nAs we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches.\\nFinally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples.\\nAs we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE.\\nHowever, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods.\\nSo in practice, there\\'s no reason to choose more complex WSL methods which require more computation time and disk space.\\nTo summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly.\\nTheir performance gain and practicality are heavily overestimated.\\nOur concrete recommendations for future work are as follows.\\nFirst, report the model selection criteria.\\nFor example, report if the model selection is done via clean validation samples.\\nSecond, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples.\\nThird, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL.\\nFinally, we have open-sourced our code.\\nYou can find it via the QR code on this slide.\\nPlease feel free to check it out.\\nThank you and enjoy the conference.'}\n",
      "{'id': '269', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/JRrbTnEZbF.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/JRrbTnEZbF.mp4', 'text': \"Hello, I'm James Finch.\\nAnd I'm Sarah Finch.\\nAnd today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI.\\nThis work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.\\nSo let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art.\\nThe common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.\\nThese approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects.\\nTherefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.\\nOne approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods.\\nHowever, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.\\nOur approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.\\nWe call this approach annotating behaviors in chat or ABC-Eval in short.\\nWe developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.\\nABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors.\\nFor example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy.\\nTo determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval.\\nFor comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons.\\nFor each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions.\\nFrom our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations.\\nIn addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis.\\nFor example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less.\\nFinally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.\\nYou can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.\\nOn the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information.\\nThese reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.\\nYou can see that in the results of our experiment that several challenges still remain and have been precisely quantified.\\nFor example, the bots we tested have common sense violations in around 20% of their responses.\\nThey produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time.\\nWith the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted.\\nHowever, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.\\nWe hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction.\\nAnd we look forward to seeing how conversational AI will advance in the coming months and years.\\nThank you for watching.\"}\n",
      "{'id': '273', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/csJIsDTYMW.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/csJIsDTYMW.mp4', 'text': 'Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context?\\nA Data-driven, Multilingual Exploration\".\\nThis work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.\\nSo a lot of translations depend on context.\\nFor example, how would we translate \"mole\" in this sentence?\\nWell, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy.\\nBut if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark.\\nSo, depending on context, the meaning of the word changes, and therefore its translation changes as well.\\nHowever, evaluating how well models can translate cases like this is pretty hard.\\nFirstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations.\\nAnd some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation.\\nIn this work, we try to answer these two questions.\\nFirst, when does translation require context?\\nAnd second, how well do models handle these cases?\\nTo answer the first question, we started by measuring how much a word depends on context during translation.\\nIn the previous work, we introduced CXMI as a measure for context usage by machine translation models.\\nAnd this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model.\\nIn this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level.\\nWe can think of words that have high P-CXMI as ones that require context for translation.\\nNow we analyze words with high P-CXMI to look for patterns between these words.\\nAnd we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages.\\nWe perform our analysis at three different levels.\\nFirst, we look at part-of-speech tags that have high mean P-CXMI.\\nAnd this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI.\\nAnd this can be explained because English doesn\\'t have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic.\\nAnd similarly, we find that certain languages also require context when we want to choose the appropriate verb form.\\nWe then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences.\\nAnd this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you\\'re using the same translation within the document.\\nAnd similarly, we find that context is important to translate in the right formality.\\nAnd finally, we look at different individual tokens that have high P-CXMI.\\nAnd this allows us to identify phenomena that cannot really be captured by the word itself, but that\\'s rather expressed in the sentence structure, such as ellipses resolution.\\nSo now we use our findings from our analysis to design a benchmark for document-level translation.\\nFor each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon.\\nAnd we called our tagger the Multilingual Discourse-Aware, or MuDA tagger.\\nWe can then also note that different languages have different proportions of these discourse phenomena.\\nWe then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified.\\nAnd finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation.\\nFirst of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance.\\nBut then if we use COMET, context-aware models perform best.\\nAnd if we use word f-measure, then models with and without context have comparable performance.\\nThis again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone.\\nNow, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion.\\nBut these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form.\\nSo this sort of suggests where we would need to see more progress for document-level translation.\\nWe also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation.\\nTo summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation.\\nThank you so much for your attention.\\nSee you in Toronto.'}\n",
      "{'id': '318', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/UOlPKyCVgg.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/UOlPKyCVgg.mp4', 'text': 'Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\"\\nIn this presentation, we first talk about language modeling in healthcare.\\nThen we will present the main contribution of our article.\\nWe introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web.\\nWe also introduced a comparison of models with multiple pre-training settings and data sources.\\nThen, we present our results on 11 biomedical and clinical downstream tasks in French.\\nAnd finally, we conclude about the experiments and give you more details about how to access those models.\\nSince its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more.\\nSince then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English.\\nSpecialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data.\\nHowever, French didn\\'t have any open source model for biomedical until now.\\nSo we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data.\\nTo answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse.\\nAfterwards, we ask ourselves how much data do we need to train a specialized model on French data?\\nIs it 4 gigabytes, 8 gigabytes, or more?\\nTo answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes.\\nIn addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy.\\nOne based on the weight of CamemBERT and trained on a 4 GB set of NACHOS.\\nAnother also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS.\\nIn total, we have seven models.\\nTo evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.\\nThese models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT.\\nThe evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained.\\nHowever, we can observe that data from heterogeneous sources appear to be more versatile.\\nWe also observe that using more data translated to better performance.\\nOverall, from-scratch pre-training seems to obtain higher performance on most of the tasks.\\nHowever, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch.\\nWhich is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues.\\nFinally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT.\\nWe are also observing that more specialized data is better, but it doesn\\'t scale well.\\nAll the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository.\\nSo thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.'}\n",
      "{'id': '325', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/wJAPXMIoIG.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/wJAPXMIoIG.mp4', 'text': 'Hi!\\nMy name is Matthias Lindemann, and today I\\'m going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\".\\nThis is joint work with my advisors Alexander Koller and Ivan Titov.\\nCompositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.\\nIn the context of semantic parsing, testing for compositional generalization might look like this.\\nAs usual, we have a training set of utterances.\\nIn this case, \"The girl slept.\"\\nAnd \"Mary knew that the girl slept.\"\\nThese utterances are paired with logical forms that represent core aspects of their meaning.\\nIn contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.\\nIn this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.\\nNaive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.\\nIn particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example.\\nA popular method to address this is to integrate trees into the models.\\nThe trees are intended to capture the compositional process that relates utterances with the logical forms.\\nThis works well, but trees are usually not given and need to be obtained somehow.\\nThis can be complicated and sometimes a computationally expensive process.\\nTypically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols.\\nObtaining trees may also involve specialized grammar-induction procedures.\\nIn this paper, we don\\'t use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output.\\nFor the first time, we show strong generalization to deeper recursion without relying on trees.\\nOur approach predicts the output from the input in two steps.\\nFirst, we tag each input token with an unordered multiset of tokens that will appear in the output.\\nAfter the first step, we have all the right tokens, but they\\'re not ordered.\\nThat\\'s why in the second step we use another model to predict a permutation to put them into the right order.\\nWe introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations.\\nThis makes our approach quite flexible and expressive.\\nConceptually, our permutation model works roughly like this.\\nWe go from left to right over the output and determine which multiset token to put in every position.\\nFor the first output position, we simply select one, as highlighted in red.\\nThen we jump to the next multiset token, to determine the second token in the output.\\nWe determine the third token in the output in a similar way by jumping to another multiset token.\\nWe continue this process until every token from the first stage has been visited exactly once.\\nTo give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark.\\nOur model outperforms the others by a large margin on generalization to deeper recursion.\\nSome other kinds of structural generalization remain very challenging, though.\\nIn our paper, we solve a couple of interesting technical challenges.\\nFirst of all, the alignment between input and output is not given in the training data.\\nAs a consequence, for a given token we don\\'t know which multiset it came from, which poses a challenge for training.\\nIn addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent.\\nWe address this by inducing the alignment as part of the training.\\nOur permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard.\\nThat\\'s because this is related to the \"Traveling Salesman\" problem.\\nWe approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations.\\nIf you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.'}\n",
      "{'id': '343', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/krJSAnVcGR.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/krJSAnVcGR.mp4', 'text': 'Hello everyone, I\\'m Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\"\\nThis work is a collaboration between McGill University, Mila, and Microsoft Research.\\nNatural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time.\\nRecent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task.\\nBut natural language understanding often requires knowledge that is also supplied at inference time.\\nFor example, in the sentence, \"John saw the newly elected president on TV.\"\\nPretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining.\\nTherefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge.\\nIn this work, we propose a diagnostic test suite for knowledge integration.\\nWe introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources.\\nWe evaluate the data set with human study participants and established coreference resolution models.\\nHere is an example from our data set.\\nServin is a judge.\\nKea is a Baker.\\nServin and Kea met at a park.\\nAfter a long day at work deciding cases in a law court, he was happy to relax.\\nThe task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin.\\nThe resolution of a given pronoun requires two types of information.\\nFirst, entity-specific knowledge such as \"Servin is a judge.\"\\nAnd second, background knowledge such as \"Judges decide cases in law courts.\"\\nGenerally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time.\\nWe vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources.\\nWe have defined three settings of KITMUS.\\nFirst, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time.\\nSecond, there\\'s a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time.\\nLastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time.\\nThis last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models.\\nFor example, because new occupations have developed since the time of pretraining.\\nHere\\'s an example of how we control the availability of facts in the true sources.\\nIn the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\"\\nIn the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context.\\nIn the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters.\\nWe evaluate the data set both with human study participants, and established coreference resolution models.\\nIn this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting.\\nWithout task-specific training on KITMUS, both models do not perform well.\\nWhen trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice.\\nThis suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed.\\nAdditional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time.\\nTo summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training.\\nHowever, with task-specific training, some models successfully integrate knowledge from multiple sources.\\nStill, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time.\\nIf you\\'re interested in more details, please see our paper and check out the data set and code on GitHub.\\nThanks for listening.'}\n",
      "{'id': '347', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/rxrToXvRyM.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/rxrToXvRyM.mp4', 'text': 'Hi, I\\'m Myra and today I\\'ll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\"\\nThis work is done in collaboration with Esin Durmus and Dan Jurafsky.\\nIn recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs.\\nHowever, these measures have various limitations.\\nThey usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don\\'t generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups.\\nFurthermore, most work in this space doesn\\'t account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm.\\nTo overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts.\\nSo we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman.\\nDescribe yourself.\".\\nAnd we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt.\\nSo here are some example generations from GPT-4.\\nImmediately we see that, while the outputs aren\\'t overtly negative or toxic in the traditional sense of these words, there are some interesting patterns.\\nThe Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region.\\nAnd both of the women of color personas make references to ancestry while the white man persona has nothing of the sort.\\nTo capture these patterns, our method has two parts.\\nThe first one is generating these personas.\\nOur prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes.\\nAnd also this enables direct comparison between our generated personas and the human written responses.\\nThe second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I\\'ll elaborate on shortly.\\nThe benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon.\\nSo the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked.\\nSo for instance, the word \"warrior\" is usually associated with men.\\nSo when people are describing a warrior who is a woman, they\\'ll usually actually specify \"woman warrior\" and mark the term with \"woman\".\\nAnd more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked.\\nSo in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group.\\nSo for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups.\\nNow for some results.\\nSo first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones.\\nHowever, when we actually look at the distribution of the words and lexicon, we find very different things.\\nSo, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\".\\nSo, really just only the positive or at least non-negative ones.\\nAnd in fact, this lexicon doesn\\'t really capture many of the harmful patterns that we saw in the earlier slides well at all.\\nSo instead to do that, we\\'ll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives.\\nIn our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns.\\nFirst, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\".\\nAnd these words define these groups only by their relationship to their identity and distinguish them as different from the white norm.\\nThis contributes to a long legacy of discrimination and othering for these groups.\\nFurthermore, there\\'s a lot of common tropes that are reflected in these words, especially for women of color.\\nSo for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism.\\nFor Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on.\\nAnd finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\".\\nThis connects to an archetype that people have called the \"Strong Black Women\" archetype.\\nAnd while it sounds positive at first glance, there\\'s been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.\\nSo rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms.\\nMore broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives.\\nSo based on these patterns, we conclude with three recommendations for model owners.\\nFirst, we should, as researchers, be addressing positive stereotypes and essentializing narratives.\\nWe should also be using an intersectional lens to study biases and harms because there\\'s a lot of things that might be overlooked if we don\\'t do that.\\nAnd finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don\\'t know if it\\'s because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns.\\nWe just really can\\'t make any assumptions or really study that further, without more transparency.\\nThank you so much for listening.\\nHave a good time at ACL.'}\n",
      "{'id': '349', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/EqmWoxNDIr.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/EqmWoxNDIr.mp4', 'text': \"Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China.\\nIt's my pleasure to give a short advertisement video of our paper.\\nAre you copying my model?\\nProtecting the copyright of large language models for embedding as services via backdoor watermark.\\nLet's first introduce the background about embedding as services.\\nCurrently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation.\\nEmbedding as services is one of the services built upon large language models to assist various, NLP tasks.\\nFor example, OpenAI offers a GPT based embedding API.\\nHowever, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services.\\nTherefore, it's necessary to protect the copyright of embedding as services.\\nTo protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark.\\nThe watermark method need to meet the following properties.\\nFirst the method should be applicable to embedding as services.\\nSecond, the watermark should not degrade the utility of the provided embeddings.\\nThird, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily.\\nFinally, the watermark needs to be transferable to the attacker's services during the model extraction process.\\nExisting works can be broadly classified into four categories.\\nHowever, this method either not applicable to embedding as services or lack of transferability.\\nTherefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services.\\nThen let me introduce the details of our embedding marker.\\nEmbedding marker contains two main steps.\\nWatermark injection and copyright verification.\\nBefore these main steps, we first select a trigger set.\\nThe trigger set is a group of words in a moderate frequency interval.\\nWe assume the provider can collect a general text corpus and count the word frequency with it.\\nIn watermark injection, we first define a target embedding.\\nWhen a user send a sentence to the provider service the provider counts the trigger number in the sentence.\\nThe provided embedding is a weight summation of the target embedding and the original embedding.\\nThe weight of the target embedding is proportional to the number of triggers in the sentence.\\nWhen a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding.\\nCopyright verification is to detect whether a model behind another service contains the word mark.\\nWe first construct a back door and a benign data set.\\nBack door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets.\\nThen the provider requests the embeddings from the stealer's service with the data set.\\nThe cosine and L2 similarity between the requested embedding and the target embedding are computed.\\nWe compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2.\\nMeanwhile, we also apply KS test and use its p-value as the third metric.\\nWe conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam.\\nWe assume the provider apply wiki text data set to count word frequency.\\nThe results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks.\\nWe also validate the covertness of the provided embedding by visualising the embedding of sentences on four datasets via PCA.\\nThe legend of the figures means the number of triggers in each sentence.\\nAs shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings.\\nThat's all.\\nThank you.\\nWelcome to discuss with us.\"}\n",
      "{'id': '355', 'prompt_en': None, 'prompt_de': 'Übersetze den englischem Inhalt nach Deutsch.', 'prompt_it': 'Traduci il contenuto inglese in italiano.', 'prompt_zh': '将英文内容翻译成中文。', 'metadata': {'qa_origin': None, 'qa_type': None}, 'audio': 'MCIF_DATA/LONG_AUDIOS/xiSxNRoOzm.wav', 'video': 'MCIF_DATA/LONG_VIDEOS/xiSxNRoOzm.mp4', 'text': 'Hello, my name is Vasudha and I\\'m a Computer Science PhD candidate at Stony Brook University.\\nI would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\"\\nWe begin by defining cognitive dissonance and why it is an important problem to study in language.\\nSimply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\".\\nThis belief and action are inconsistent, and they are in dissonance.\\nFurther mentioning that \"I don\\'t think I could keep my job without them\" justifies the second occurrence.\\nAnd they have a consonance relationship.\\nWhile dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations.\\nSo why does this matter?\\nStudying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population.\\nHigh cognitive dissonance is also related to anxiety disorders and can help understand people\\'s mental health better.\\nStudying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.\\nFinally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better.\\nTo the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations.\\nWe used dissonance-first approach, as seen in the flow chart here.\\nTweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper.\\nAs can be seen here, dissonance was only found in 3.5% of the annotated pairs.\\nOn collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance.\\nTo no surprise, the classifier performed not much better than chance.\\nGiven the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity.\\nTo alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection.\\nSince the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks.\\nWe transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here.\\nWe find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62.\\nFurther, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance.\\nThus, this is the model that we use to cold start the active learning.\\nNext, we determine the best method to update a model with new data from each round of active learning and annotations.\\n\"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected.\\nOver the different strategies, we found that Cumulative performed equal or better than Iterative across the board.\\nNext, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare.\\nWe compare this to the other state-of-the-art AL strategies that are commonly used in the community.\\nWe find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small.\\nNote that the performance is significantly lower for random.\\nOn further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far.\\nWe also check the feasibility of each strategy for annotation quality and costs to annotators.\\nWe find that PRC has the highest percentage of dissonance and works best for rare class.\\nHowever, the annotators also find the examples difficult.\\nIn summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly.\\nWe also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update.\\nThese are the links to our core data set and our paper.\\nFeel free to get in touch with us if you have any questions.\\nThank you.'}\n"
     ]
    }
   ],
   "source": [
    "for row in ds['test']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bd7a84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences: 320\n",
      "French sentences: 320\n",
      " 1. English: Answer the following question concisely given the English content: What are the main data sources for language models? French: 根据所给的英文内容，简要回答以下问题： 语言模型的主要数据来源是什么？\n",
      " 2. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      " 3. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      " 4. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      " 5. English: Answer the following question concisely given the English content: Which model did they use to obtain the 82%-87% accuracy? French: 根据所给的英文内容，简要回答以下问题： 他们使用哪种模型获得 82%-87% 的准确率？\n",
      " 6. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      " 7. English: Answer the following question concisely given the English content: Do CoNLL-2003 taggers still work? French: 根据所给的英文内容，简要回答以下问题： CoNLL-2003 标注器是否仍然有效？\n",
      " 8. English: Answer the following question concisely given the English content: What is the novelty of the proposed human evaluation method? French: 根据所给的英文内容，简要回答以下问题： 提出的人工评估方法有何新颖之处？\n",
      " 9. English: Answer the following question concisely given the English content: What does the success of the existing weakly supervised approach heavily rely on? French: 根据所给的英文内容，简要回答以下问题： 现有弱监督方法的成功在很大程度上依赖于什么？\n",
      "10. English: Answer the following question concisely given the English content: What kind of advances can be done to improve the score? French: 根据所给的英文内容，简要回答以下问题： 可以采取哪些措施来提高分数？\n",
      "11. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "12. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "13. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "14. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "15. English: Answer the following question concisely given the English content: Which domains are simplified more? French: 根据所给的英文内容，简要回答以下问题： 哪些领域的简化程度更大？\n",
      "16. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "17. English: Answer the following question concisely given the English content: What is the example of the preference for shorter left conjuncts? French: 根据所给的英文内容，简要回答以下问题： 偏好较短左并列词的示例是什么？\n",
      "18. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "19. English: Answer the following question concisely given the English content: Can I use the models for my research? French: 根据所给的英文内容，简要回答以下问题： 我可以将这些模型用于我的研究吗？\n",
      "20. English: Answer the following question concisely given the English content: DEplain-web contains documents from the web. Which type of content is inside DEplain-apa? French: 根据所给的英文内容，简要回答以下问题： DEplain-web 包含来自网络的文档。DEplain-apa 中包含哪种类型的内容？\n",
      "21. English: Answer the following question concisely given the English content: Which factors lead to good generalization? French: 根据所给的英文内容，简要回答以下问题： 哪些因素有助于良好的泛化？\n",
      "22. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "23. English: Answer the following question concisely given the English content: How was the tendency for left conjuncts to be shorter measured? French: 根据所给的英文内容，简要回答以下问题： 如何衡量左并列词是否更短？\n",
      "24. English: Answer the following question concisely given the English content: How were the experiments designed to study the effect of the governor’s position? French: 根据所给的英文内容，简要回答以下问题： 如何设计实验来研究支配词位置的影响？\n",
      "25. English: Answer the following question concisely given the English content: How well does a baseline classifier work training on imbalanced data? French: 根据所给的英文内容，简要回答以下问题： 基线分类器在不平衡数据上的训练效果如何？\n",
      "26. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "27. English: Answer the following question concisely given the English content: What are the characters' names in the example conversation? French: 根据所给的英文内容，简要回答以下问题： 示例对话中的角色名字是什么？\n",
      "28. English: Answer the following question concisely given the English content: On which discourse phenomena do context-aware MT models improve over context-agnostic ones? French: 根据所给的英文内容，简要回答以下问题： 在哪些话语现象上，语境感知 MT 模型比语境无关模型更有优势？\n",
      "29. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "30. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "31. English: Answer the following question concisely given the English content: How exactly does the introduced framework quantify the positionality? French: 根据所给的英文内容，简要回答以下问题： 引入的框架如何量化立场？\n",
      "32. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "33. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "34. English: Answer the following question concisely given the English content: What was the finding of the previous study where human subjects were given the same persona prompts? French: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "35. English: Answer the following question concisely given the English content: Which sources of data were used in this study? French: 根据所给的英文内容，简要回答以下问题： 此研究使用了哪些数据来源？\n",
      "36. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "37. English: Answer the following question concisely given the English content: What are some closely related tasks for cognitive dissonance? French: 根据所给的英文内容，简要回答以下问题： 与认知失调密切相关的任务有哪些？\n",
      "38. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "39. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "40. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "41. English: Answer the following question concisely given the English content: How does the introduced framework differ from the previous works? French: 根据所给的英文内容，简要回答以下问题： 引入的框架与以前的研究有何不同？\n",
      "42. English: Answer the following question concisely given the English content: Which of the three compared setups overlaps the most with the lexicon of stereotypes? French: 根据所给的英文内容，简要回答以下问题： 在三个比较设置中，哪一个与刻板词汇的重叠最多？\n",
      "43. English: Answer the following question concisely given the English content: Which commercial systems were compared? French: 根据所给的英文内容，简要回答以下问题： 比较了哪些商业系统？\n",
      "44. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "45. English: Answer the following question concisely given the English content: Up to how many tokens context length were MPP evaluations performed? French: 根据所给的英文内容，简要回答以下问题： MPP 评估最多涵盖了多少个词元的上下文长度？\n",
      "46. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "47. English: Answer the following question concisely given the English content: Which domains did they include in their dataset? French: 根据所给的英文内容，简要回答以下问题： 他们的数据集中包含哪些领域？\n",
      "48. English: Answer the following question concisely given the English content: What is the definition of positionality in general? French: 根据所给的英文内容，简要回答以下问题： 一般来说，positionality（立场）的定义是什么？\n",
      "49. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "50. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "51. English: Answer the following question concisely given the English content: Does EDAtt adapt an existing offline ST model? French: 根据所给的英文内容，简要回答以下问题： EDAtt 是否适应了现有的离线 ST 模型？\n",
      "52. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "53. English: Answer the following question concisely given the English content: Does the tested model work on the test suite? French: 根据所给的英文内容，简要回答以下问题： 被测模型是否能在测试套件上运行？\n",
      "54. English: Answer the following question concisely given the English content: Which three variants of KITMUS are there? French: 根据所给的英文内容，简要回答以下问题： KITMUS 有哪三个变体？\n",
      "55. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "56. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "57. English: Answer the following question concisely given the English content: What is the last research question? French: 根据所给的英文内容，简要回答以下问题： 最后一个研究问题是什么？\n",
      "58. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "59. English: Answer the following question concisely given the English content: How does the metric sensitivity work? French: 根据所给的英文内容，简要回答以下问题： 指标灵敏度是如何工作的？\n",
      "60. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "61. English: Answer the following question concisely given the English content: Does greater sensitivity indicate improved model performance, or does it suggest the opposite? French: 根据所给的英文内容，简要回答以下问题： 更高的灵敏度是否表示模型性能得到了提高，还是表明相反？\n",
      "62. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "63. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "64. English: Answer the following question concisely given the English content: What kind of linguistic context do models receive during pretraining? French: 根据所给的英文内容，简要回答以下问题： 在预训练期间，模型会接收什么样的语言上下文？\n",
      "65. English: Answer the following question concisely given the English content: How many clean validation samples are typically needed for good performance in WSL? French: 根据所给的英文内容，简要回答以下问题： 在 WSL 中，通常需要多少个干净的验证样本才能获得良好的表现？\n",
      "66. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "67. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "68. English: Answer the following question concisely given the English content: Why is there a need to develop new methods for measuring media biases? French: 根据所给的英文内容，简要回答以下问题： 为什么需要开发新的方法来衡量媒体偏见？\n",
      "69. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "70. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "71. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "72. English: Answer the following question concisely given the English content: What does the political bias propagation pipeline look like? French: 根据所给的英文内容，简要回答以下问题： 政治偏见传播流程是怎样的？\n",
      "73. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "74. English: Answer the following question concisely given the English content: Does the simplification process differ for DEplain-apa and web? French: 根据所给的英文内容，简要回答以下问题： DEplain-apa 和网站的简化过程是否有所不同？\n",
      "75. English: Answer the following question concisely given the English content: Is Coscript publicly available? French: 根据所给的英文内容，简要回答以下问题： Coscript 是否公开可用？\n",
      "76. English: Answer the following question concisely given the English content: How exactly is the watermark inserted into the text? French: 根据所给的英文内容，简要回答以下问题： 水印是如何插入到文本中的？\n",
      "77. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "78. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "79. English: Answer the following question concisely given the English content: Can encoder-decoder models such as mt5 improve by training on a mixture of languages? French: 根据所给的英文内容，简要回答以下问题： 像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进吗？\n",
      "80. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "81. English: Answer the following question concisely given the English content: What is an example of constrained language planning? French: 根据所给的英文内容，简要回答以下问题： 受限语言规划的一个示例是什么？\n",
      "82. English: Answer the following question concisely given the English content: How do they make sure of the covertness of their method? French: 根据所给的英文内容，简要回答以下问题： 他们如何确保其方法的隐蔽性？\n",
      "83. English: Answer the following question concisely given the English content: How does the work use existing PLMs to build a new one? French: 根据所给的英文内容，简要回答以下问题： 研究如何使用现有的 PLM 来构建新的 PLM？\n",
      "84. English: Answer the following question concisely given the English content: Which country is GPT-4 the least aligned with? French: 根据所给的英文内容，简要回答以下问题： GPT-4 与哪个国家/地区的立场最不一致？\n",
      "85. English: Answer the following question concisely given the English content: On which example sentence does the speaker show how the model leverages knowledge learned though the attention mechanism? French: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "86. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "87. English: Answer the following question concisely given the English content: How does the amount of tasks impact the model performance? French: 根据所给的英文内容，简要回答以下问题： 任务的数量如何影响模型的性能？\n",
      "88. English: Answer the following question concisely given the English content: Name three treeless baselines that the authors compare their method with. French: 根据所给的英文内容，简要回答以下问题： 请说出作者用来比较其方法的三个无树基线。\n",
      "89. English: Answer the following question concisely given the English content: In what relation are the two co-authors with the first author? French: 根据所给的英文内容，简要回答以下问题： 两位合著者与第一作者有什么关系？\n",
      "90. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "91. English: Answer the following question concisely given the English content: Who is the first author of PaLM? French: 根据所给的英文内容，简要回答以下问题： PaLM 的第一作者是谁？\n",
      "92. English: Answer the following question concisely given the English content: How many problems of SimulST does the speaker mention? French: 根据所给的英文内容，简要回答以下问题： 演讲者提到了 SimulST 的几个问题？\n",
      "93. English: Answer the following question concisely given the English content: What is an effective way to mitigate social and political biases in datasets when training NLP models? French: 根据所给的英文内容，简要回答以下问题： 在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "94. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "95. English: Answer the following question concisely given the English content: How good is the fluency of PaLM? French: 根据所给的英文内容，简要回答以下问题： PaLM 的流畅度如何？\n",
      "96. English: Answer the following question concisely given the English content: What are the important properties of a watermarking method? French: 根据所给的英文内容，简要回答以下问题： 水印方法的重要属性是什么？\n",
      "97. English: Answer the following question concisely given the English content: Which are the 14 different languages into which the English TED talks have been translated? French: 根据所给的英文内容，简要回答以下问题： TED 英语演讲已被翻译成哪 14 种不同的语言？\n",
      "98. English: Answer the following question concisely given the English content: How many instances are sampled from one dataset for reannotating? French: 根据所给的英文内容，简要回答以下问题： 从一个数据集中抽取多少个实例用于重新注释？\n",
      "99. English: Answer the following question concisely given the English content: Which distance metrics are used for measuring the difference between benign and backdoor datasets? French: 根据所给的英文内容，简要回答以下问题： 哪些距离度量用于衡量良性和后门数据集之间的差异？\n",
      "100. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "101. English: Answer the following question concisely given the English content: How were the multilingual encoder-based models used for this task? French: 根据所给的英文内容，简要回答以下问题： 如何将基于编码器的多语言模型用于这项任务？\n",
      "102. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "103. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "104. English: Answer the following question concisely given the English content: How do the authors decide what moderate-frequency words are? French: 根据所给的英文内容，简要回答以下问题： 作者如何确定中等频率的单词？\n",
      "105. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "106. English: Answer the following question concisely given the English content: What speech segment size does the approach use? French: 根据所给的英文内容，简要回答以下问题： 该方法使用的语音片段大小是多少？\n",
      "107. English: Answer the following question concisely given the English content: In the example with Servin and Kea, what entity-specific knowledge is needed? French: 根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些特定于实体的知识？\n",
      "108. English: Answer the following question concisely given the English content: What is the most important factor between the example quality and the similarity to the source sentence? French: 根据所给的英文内容，简要回答以下问题： 示例质量和与源句子的相似度相比，哪个因素更为重要？\n",
      "109. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "110. English: Answer the following question concisely given the English content: Which language models does the paper focus on in the extended experiments? French: 根据所给的英文内容，简要回答以下问题： 在扩展实验中，论文侧重于哪些语言模型？\n",
      "111. English: Answer the following question concisely given the English content: Does the model use attention scores from a specific layer or combine the scores from several layers? French: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "112. English: Answer the following question concisely given the English content: What are the examples of direct inference? French: 根据所给的英文内容，简要回答以下问题： 直接推断的示例有哪些？\n",
      "113. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "114. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "115. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "116. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "117. English: Answer the following question concisely given the English content: Was translating the natural language query using a machine translation model before semantic parsing considered as a baseline? French: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "118. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "119. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "120. English: Answer the following question concisely given the English content: What example did the authors give as a marked group? French: 根据所给的英文内容，简要回答以下问题： 作者给出的“显性群体”(marked group) 的示例是什么？\n",
      "121. English: Answer the following question concisely given the English content: Which model architectures do not generalize well? French: 根据所给的英文内容，简要回答以下问题： 哪些模型架构泛化能力较差？\n",
      "122. English: Answer the following question concisely given the English content: What are the names of the testing datasets? French: 根据所给的英文内容，简要回答以下问题： 测试数据集的名称是什么？\n",
      "123. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "124. English: Answer the following question concisely given the English content: Does the author work with multiple modalities or only text? French: 根据所给的英文内容，简要回答以下问题： 作者是否采用了多种模态，还是仅使用文本？\n",
      "125. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "126. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "127. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "128. English: Answer the following question concisely given the English content: What do the authors claim is an understudied area in NLU? French: 根据所给的英文内容，简要回答以下问题： 作者认为哪些是 NLU 中研究不足的领域？\n",
      "129. English: Answer the following question concisely given the English content: What are the names of the speakers? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "130. English: Answer the following question concisely given the English content: Did Coscript undergo any quality checks? French: 根据所给的英文内容，简要回答以下问题： Coscript 是否经过了质量检查？\n",
      "131. English: Answer the following question concisely given the English content: What are the limits of existing resources for on context-dependent translation? French: 根据所给的英文内容，简要回答以下问题： 对于依赖上下文的翻译，现有的资源有哪些局限性？\n",
      "132. English: Answer the following question concisely given the English content: To which existing SimulST policies is the approach compared? French: 根据所给的英文内容，简要回答以下问题： 该方法与哪些现有的 SimulST 策略进行了比较？\n",
      "133. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "134. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "135. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "136. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "137. English: Answer the following question concisely given the English content: Is the dataset publicly available? French: 根据所给的英文内容，简要回答以下问题： 数据集是否公开？\n",
      "138. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "139. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "140. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "141. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "142. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "143. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "144. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "145. English: Answer the following question concisely given the English content: What type of tokens does the first step of the method map the input tokens to? French: 根据所给的英文内容，简要回答以下问题： 该方法的第一步将输入词元映射到什么类型的词元？\n",
      "146. English: Answer the following question concisely given the English content: How many scripts are represented in Coscript? French: 根据所给的英文内容，简要回答以下问题： Coscript 中包含了多少个脚本？\n",
      "147. English: Answer the following question concisely given the English content: What is the best alignment method for DEplain? French: 根据所给的英文内容，简要回答以下问题： DEplain 的最佳对齐方法是什么？\n",
      "148. English: Answer the following question concisely given the English content: What is the benefit of weakly supervised learning? French: 根据所给的英文内容，简要回答以下问题： 弱监督学习有什么好处？\n",
      "149. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "150. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "151. English: Answer the following question concisely given the English content: The documents in DEplain-web were aligned with manual and automatic alignment methods. How was the allocation exactly? French: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "152. English: Answer the following question concisely given the English content: How was the CoNLL++ dataset created? French: 根据所给的英文内容，简要回答以下问题： CoNLL++ 数据集是如何创建的？\n",
      "153. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "154. English: Answer the following question concisely given the English content: What are the existing works on this? French: 根据所给的英文内容，简要回答以下问题： 关于这方面的现有研究有哪些？\n",
      "155. English: Answer the following question concisely given the English content: Are multilingual LLMs such as Codex or Bloom sufficient for CLSP? French: 根据所给的英文内容，简要回答以下问题： Codex 或 Bloom 等多语言 LLM 对于 CLSP 来说是否足够？\n",
      "156. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "157. English: Answer the following question concisely given the English content: How does the method deal with the ambiguity of permutations? French: 根据所给的英文内容，简要回答以下问题： 该方法如何处理排列的不确定性？\n",
      "158. English: Answer the following question concisely given the English content: How is the fairness of a downstream NLP model defined? French: 根据所给的英文内容，简要回答以下问题： 如何定义下游 NLP 模型的公平性？\n",
      "159. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "160. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "161. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "162. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "163. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "164. English: Answer the following question concisely given the English content: What does tropicalism indicate in the context of this paper? French: 根据所给的英文内容，简要回答以下问题： 在本文的背景下，热带主义 (tropicalism) 意味着什么？\n",
      "165. English: Answer the following question concisely given the English content: How did the authors create the human-written portrayals of target groups? French: 根据所给的英文内容，简要回答以下问题： 作者是如何创建目标群体的人工描写？\n",
      "166. English: Answer the following question concisely given the English content: What was used to measure context usage in this work? French: 根据所给的英文内容，简要回答以下问题： 本文中使用了什么来衡量语境使用情况？\n",
      "167. English: Answer the following question concisely given the English content: What is the difference between DrBERT and ChuBERT? French: 根据所给的英文内容，简要回答以下问题： DrBERT 和 ChuBERT 有什么区别？\n",
      "168. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "169. English: Answer the following question concisely given the English content: What is iterative transfer learning? French: 根据所给的英文内容，简要回答以下问题： 什么是迭代迁移学习？\n",
      "170. English: Answer the following question concisely given the English content: What is the goal of the dataset? French: 根据所给的英文内容，简要回答以下问题： 数据集的目标是什么？\n",
      "171. English: Answer the following question concisely given the English content: How can an attacker extract model parameters through an EaaS? French: 根据所给的英文内容，简要回答以下问题： 攻击者如何通过 EaaS 来提取模型参数？\n",
      "172. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "173. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "174. English: Answer the following question concisely given the English content: How many annotators were used to create the initial dataset? French: 根据所给的英文内容，简要回答以下问题： 有多少个注释者用于创建初始数据集？\n",
      "175. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "176. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "177. English: Answer the following question concisely given the English content: What is the example that the governor is on the left? French: 根据所给的英文内容，简要回答以下问题： 以左侧为支配词的示例是什么？\n",
      "178. English: Answer the following question concisely given the English content: What are the state-of-the-art models in dialogue systems? French: 根据所给的英文内容，简要回答以下问题： 对话系统中的最先进模型是什么？\n",
      "179. English: Answer the following question concisely given the English content: Why do we need to evaluate the models' acceptability throughout the context window? French: 根据所给的英文内容，简要回答以下问题： 为什么我们需要在整个上下文窗口中评估模型的可接受性？\n",
      "180. English: Answer the following question concisely given the English content: Did training in multilingual fashion cause performance drop compared to monolingual English model? French: 根据所给的英文内容，简要回答以下问题： 与单语英语模型相比，多语言训练是否会导致表现下降？\n",
      "181. English: Answer the following question concisely given the English content: Do the annotators know about the entity in advance? French: 根据所给的英文内容，简要回答以下问题： 注释者是否提前知道该实体？\n",
      "182. English: Answer the following question concisely given the English content: Which MT metrics were used for the evaluation? French: 根据所给的英文内容，简要回答以下问题： 评估使用了哪些 MT（机器翻译）指标？\n",
      "183. English: Answer the following question concisely given the English content: Does the regress in generalization impact specific NER types? French: 根据所给的英文内容，简要回答以下问题： 泛化中的回归是否会影响特定的 NER 类型？\n",
      "184. English: Answer the following question concisely given the English content: Why does positionality in NLP matter? French: 根据所给的英文内容，简要回答以下问题： 为什么 NLP 中的立场很重要？\n",
      "185. English: Answer the following question concisely given the English content: Were the multilingual LLMs like BLOOM fine-tuned with adapters or full fine-tuning? French: 根据所给的英文内容，简要回答以下问题： 像 BLOOM 这样的多语言 LLM 是采用适配器微调还是完整微调？\n",
      "186. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "187. English: Answer the following question concisely given the English content: Which model do they use for transfer learning? French: 根据所给的英文内容，简要回答以下问题： 他们使用哪种模型进行迁移学习？\n",
      "188. English: Answer the following question concisely given the English content: Which are the recent test sets used to assess the PaLM capabilities? French: 根据所给的英文内容，简要回答以下问题： 哪些是最近用于评估 PaLM 能力的测试集？\n",
      "189. English: Answer the following question concisely given the English content: How many recommendations did the authors propose at last? French: 根据所给的英文内容，简要回答以下问题： 作者最终提出了多少条建议？\n",
      "190. English: Answer the following question concisely given the English content: How much is the gain of the proposed method over the strongest baseline? French: 根据所给的英文内容，简要回答以下问题： 与最强的基线相比，提议的方法获得了多少收益？\n",
      "191. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "192. English: Answer the following question concisely given the English content: Can the results and dataset in the paper be used as a benchmark? French: 根据所给的英文内容，简要回答以下问题： 论文中的结果和数据集可以用作基准吗？\n",
      "193. English: Answer the following question concisely given the English content: How many smaller models do they experiment with in the paper? French: 根据所给的英文内容，简要回答以下问题： 他们在论文中进行了多少个较小模型的实验？\n",
      "194. English: Answer the following question concisely given the English content: Which model is used as the base model for investigating multi-model instruction tuning? French: 根据所给的英文内容，简要回答以下问题： 哪个模型被用作研究多模型指令调整的基础模型？\n",
      "195. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "196. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "197. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "198. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "199. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "200. English: Answer the following question concisely given the English content: Which language pairs were analyzed in the paper? French: 根据所给的英文内容，简要回答以下问题： 论文分析了哪些语言对？\n",
      "201. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "202. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "203. English: Answer the following question concisely given the English content: Which models were investigated during the experiments? French: 根据所给的英文内容，简要回答以下问题： 在实验过程中研究了哪些模型？\n",
      "204. English: Answer the following question concisely given the English content: From the 62 diverse tasks used in MultiInstruct, how many tasks are used for training and testing purposes? French: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "205. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "206. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "207. English: Answer the following question concisely given the English content: Which datasets did the authors experiment on? French: 根据所给的英文内容，简要回答以下问题： 作者在实验中使用了哪些数据集？\n",
      "208. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "209. English: Answer the following question concisely given the English content: What is NACHOS? French: 根据所给的英文内容，简要回答以下问题： 什么是 NACHOS？\n",
      "210. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "211. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "212. English: Answer the following question concisely given the English content: How much does the prompting strategy impact the results? French: 根据所给的英文内容，简要回答以下问题： 提示策略对结果有多大影响？\n",
      "213. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "214. English: Answer the following question concisely given the English content: What are the 5 expert-written instructions? French: 根据所给的英文内容，简要回答以下问题： 5 个由专家编写的指令是什么？\n",
      "215. English: Answer the following question concisely given the English content: What do the authors propose to test the models on using information from multiple sources? French: 根据所给的英文内容，简要回答以下问题： 作者建议如何使用来自多种来源的信息来测试模型？\n",
      "216. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "217. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "218. English: Answer the following question concisely given the English content: What are common evaluation methods for dialogue systems? French: 根据所给的英文内容，简要回答以下问题： 对话系统的常用评估方法是什么？\n",
      "219. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "220. English: Answer the following question concisely given the English content: In the example with Servin and Kea, what background knowledge is needed? French: 根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些背景知识？\n",
      "221. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "222. English: Answer the following question concisely given the English content: Is the code available, and if yes, where? French: 根据所给的英文内容，简要回答以下问题： 代码是否公开？如果公开，可在哪里获取？\n",
      "223. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "224. English: Answer the following question concisely given the English content: Are the annotators for NLPositionality balanced in regard to each demographic, i.e., country, gender, etc.? French: 根据所给的英文内容，简要回答以下问题： NLPositionality 的注释者在各个人口统计学特征（即国家/地区、性别等）方面是否均衡？\n",
      "225. English: Answer the following question concisely given the English content: How were sentences perturbed in the acceptable domain? French: 根据所给的英文内容，简要回答以下问题： 如何在可接受的域中扰乱句子？\n",
      "226. English: Answer the following question concisely given the English content: What does it mean to have a dimensional evaluation? French: 根据所给的英文内容，简要回答以下问题： 进行维度评估意味着什么？\n",
      "227. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "228. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "229. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "230. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "231. English: Answer the following question concisely given the English content: In which cases, if any, is the form of the prompting important? French: 根据所给的英文内容，简要回答以下问题： 在哪些情况下（如有），提示的形式很重要？\n",
      "232. English: Answer the following question concisely given the English content: Which dialog models did the authors evaluate? French: 根据所给的英文内容，简要回答以下问题： 作者评估了哪些对话模型？\n",
      "233. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "234. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "235. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "236. English: Answer the following question concisely given the English content: What are the ideal qualities of a good planner? French: 根据所给的英文内容，简要回答以下问题： 优秀规划器的理想品质是什么？\n",
      "237. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "238. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "239. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "240. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "241. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "242. English: Answer the following question concisely given the English content: Which are the most common errors of PaLM? French: 根据所给的英文内容，简要回答以下问题： PaLM 最常见的错误是什么？\n",
      "243. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "244. English: Answer the following question concisely given the English content: What does CFT stand for in this paper? French: 根据所给的英文内容，简要回答以下问题： 在本文中，CFT 代表什么？\n",
      "245. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "246. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "247. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "248. English: Answer the following question concisely given the English content: If the new method has a name, state that name, otherwise state that it does not have a name. French: 根据所给的英文内容，简要回答以下问题： 如果新方法有名称，请注明该名称，如果没有，请注明它没有名称。\n",
      "249. English: Answer the following question concisely given the English content: What was the author's description of the \"marked words\" method? French: 根据所给的英文内容，简要回答以下问题： 作者如何描述“显性词汇”(marked words) 方法？\n",
      "250. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "251. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "252. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "253. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "254. English: Answer the following question concisely given the English content: What is the name of the first mentioned symmetrical dependency structure? Answer the one including the city name French: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "255. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "256. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "257. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "258. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "259. English: Answer the following question concisely given the English content: What datasets can be used to test syntactic phenomena? French: 根据所给的英文内容，简要回答以下问题： 哪些数据集可用于测试句法现象？\n",
      "260. English: Answer the following question concisely given the English content: What are the abbreviations of the five methods for the first research question? French: 根据所给的英文内容，简要回答以下问题： 第一个研究问题的五种方法的缩写是什么？\n",
      "261. English: Answer the following question concisely given the English content: On which tasks is the model evaluated? French: 根据所给的英文内容，简要回答以下问题： 该模型在哪些任务上进行了评估？\n",
      "262. English: Answer the following question concisely given the English content: On which data is CamemBERT initially trained? French: 根据所给的英文内容，简要回答以下问题： CamemBERT 最初是在哪些数据上训练的？\n",
      "263. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "264. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "265. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "266. English: Answer the following question concisely given the English content: Which findings led to the conclusion that the temporal drift is the main cause of performance loss? French: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "267. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "268. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "269. English: Answer the following question concisely given the English content: Why is it necessary to permute the tokens for the output sequence? French: 根据所给的英文内容，简要回答以下问题： 为什么有必要对输出序列中的词元进行排列？\n",
      "270. English: Answer the following question concisely given the English content: Why did the authors recommend that model owners should increase transparency about bias mitigation methods? French: 根据所给的英文内容，简要回答以下问题： 为什么作者建议模型所有者应提高偏见缓解方法的透明度？\n",
      "271. English: Answer the following question concisely given the English content: What are minimal-pair unacceptable inputs? French: 根据所给的英文内容，简要回答以下问题： 什么是最小对不可接受输入？\n",
      "272. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "273. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "274. English: Answer the following question concisely given the English content: What evaluation metrics did the authors use? French: 根据所给的英文内容，简要回答以下问题： 作者使用了哪些评估指标？\n",
      "275. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "276. English: Answer the following question concisely given the English content: Which metric was used for measuring inter-annotator agreement? French: 根据所给的英文内容，简要回答以下问题： 使用了哪个指标来衡量注释者之间的一致性？\n",
      "277. English: Answer the following question concisely given the English content: Which domain was chosen to add completely unrelated sentences to the unacceptable and acceptable queries? French: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "278. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "279. English: Answer the following question concisely given the English content: How does MultiInstruct differ from other benchmarks? French: 根据所给的英文内容，简要回答以下问题： MultiInstruct 与其他基准有何不同？\n",
      "280. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "281. English: Answer the following question concisely given the English content: What is the definition of binary coordination? French: 根据所给的英文内容，简要回答以下问题： 二进制协调的定义是什么？\n",
      "282. English: Answer the following question concisely given the English content: How long, on average, were the prompts used in this study? French: 根据所给的英文内容，简要回答以下问题： 在本研究中，提示语的平均长度是多少？\n",
      "283. English: Answer the following question concisely given the English content: What are the implications of the findings on the smaller T5 model? French: 根据所给的英文内容，简要回答以下问题： 这些发现对较小的 T5 模型有什么影响？\n",
      "284. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "285. English: Answer the following question concisely given the English content: Which learning strategies are investigated in the work? French: 根据所给的英文内容，简要回答以下问题： 论文研究了哪些学习策略？\n",
      "286. English: Answer the following question concisely given the English content: How big is the factor of overfitting due to test reuse specifically? French: 根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？\n",
      "287. English: Answer the following question concisely given the English content: How was the quality of the simplification evaluated? French: 根据所给的英文内容，简要回答以下问题： 如何评估简化质量？\n",
      "288. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "289. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "290. English: Answer the following question concisely given the English content: Do language models have different political biases? French: 根据所给的英文内容，简要回答以下问题： 语言模型是否有不同的政治偏见？\n",
      "291. English: Answer the following question concisely given the English content: What is cognitive dissonance? French: 根据所给的英文内容，简要回答以下问题： 什么是认知失调？\n",
      "292. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "293. English: Answer the following question concisely given the English content: Which language model is the most liberal? French: 根据所给的英文内容，简要回答以下问题： 哪种语言模型最倾向于自由派？\n",
      "294. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "295. English: Answer the following question concisely given the English content: Does cumulative training perform better than iterative when doing active learning? French: 根据所给的英文内容，简要回答以下问题： 在主动学习时，累积训练是否比迭代训练更有效？\n",
      "296. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "297. English: Answer the following question concisely given the English content: Where was the data taken from in the MuDa benchmark? French: 根据所给的英文内容，简要回答以下问题： MuDa 基准中的数据是从哪里获得的？\n",
      "298. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "299. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "300. English: Answer the following question concisely given the English content: What is cross-lingual transfer? French: 根据所给的英文内容，简要回答以下问题： 什么是跨语言转移？\n",
      "301. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "302. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "303. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "304. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "305. English: Answer the following question concisely given the English content: Which latency measures do the authors use? French: 根据所给的英文内容，简要回答以下问题： 作者使用了哪些延迟测量方法？\n",
      "306. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "307. English: Answer the following question concisely given the English content: What are the drawbacks of tree-based methods? French: 根据所给的英文内容，简要回答以下问题： 基于树的方法有哪些缺点？\n",
      "308. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "309. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "310. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "311. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "312. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "313. English: Answer the following question concisely given the English content: What does ABC-Eval stand for? French: 根据所给的英文内容，简要回答以下问题： ABC-Eval 代表什么？\n",
      "314. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n",
      "315. English: Answer the following question concisely given the English content: Until which year is the performance delta between CoNLL-2003 and CoNLL++ higher than 5 percentage points? French: 根据所给的英文内容，简要回答以下问题： 直到哪一年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于 5 个百分点？\n",
      "316. English: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "317. English: Answer the following question concisely given the English content: What is the name of the speaker? French: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "318. English: Answer the following question concisely given the English content: How many authors are involved in the paper? French: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "319. English: Answer the following question concisely given the English content: To which dedicated simulST architecture is the approach compared? French: 根据所给的英文内容，简要回答以下问题： 该方法与哪种专用的 simulST 架构进行了比较？\n",
      "320. English: Summarize the English content in an abstract of approximately 200 words. French: 用400个字左右概括所给的英语内容。\n"
     ]
    }
   ],
   "source": [
    "english = []\n",
    "french = []\n",
    "\n",
    "\n",
    "for row in ds['test']:\n",
    "    if row['prompt_en'] and row['prompt_zh']:\n",
    "        english.append(row['prompt_en'])\n",
    "        french.append(row['prompt_zh'])\n",
    "\n",
    "print(f\"English sentences: {len(english)}\")\n",
    "print(f\"French sentences: {len(french)}\")\n",
    "\n",
    "for i, (eng, fr) in enumerate(zip(english, french), 1):\n",
    "    print(f\"{i:2d}. English: {eng:<30} French: {fr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8253ea9",
   "metadata": {},
   "source": [
    "## Vocabulary and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af57d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    vocab = {'<pad>':0,'<sos>':1,'<eos>':2,\"unk\":3}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "english_vocab = build_vocab(english)\n",
    "french_vocab = build_vocab(french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4cca7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences,vocab,max_len):\n",
    "    tokenized = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = [vocab['<sos>']] + [vocab.get(word,vocab['unk']) for word in sentence.split()] + [vocab['<eos>']]\n",
    "        tokens += [vocab['<pad>']] * (max_len - len(tokens))\n",
    "        tokenized.append(tokens)\n",
    "    return np.array(tokenized)\n",
    "\n",
    "max_len_english = max(len(sentence.split()) for sentence in english) + 2\n",
    "max_len_french = max(len(sentence.split()) for sentence in french) + 2\n",
    "\n",
    "english_data = tokenize(english,english_vocab,max_len_english)\n",
    "french_data = tokenize(french,french_vocab,max_len_french)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deddaa34",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "187a95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,src_data,target_data):\n",
    "        self.src_data = src_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return torch.tensor(self.src_data[idx]),torch.tensor(self.target_data[idx])\n",
    "    \n",
    "\n",
    "dataset = TranslationDataset(english_data,french_data)\n",
    "dataloader = DataLoader(dataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,embed_dim,hidden_dim,num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim,hidden_dim,num_layers,batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs , (hidden,cell) = self.lstm(embedded)\n",
    "        return hidden , cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,embed_dim,hidden_dim,num_layers):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim,embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim,hidden_dim,num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "    def forward(self,x,hidden,cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        outputs , (hidden,cell) = self.lstm(embedded,(hidden,cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions , hidden , cell\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,src,target,teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        target_length  = target.size(1)\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "\n",
    "        outputs = torch.zeros(batch_size,target_length,target_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = target[:,0]\n",
    "\n",
    "        for t in range(1,target_length):\n",
    "            output , hidden , cell = self.decoder(input,hidden,cell)\n",
    "            outputs[:,t,:] = output\n",
    "            top = output.argmax(1)\n",
    "            input = target[:,t] if torch.rand(1).item() < teacher_forcing_ratio else top\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "baa9bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available else 'cpu'\n",
    "print(device)\n",
    "\n",
    "\n",
    "input_dim = len(english_vocab)\n",
    "output_dim = len(french_vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "encoder = Encoder(input_dim,embed_dim,hidden_dim,num_layers)\n",
    "decoder = Decoder(output_dim,embed_dim,hidden_dim,num_layers)\n",
    "model = Seq2Seq(encoder,decoder,device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=french_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3751dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.1313\n",
      "Epoch 2/20, Loss: 1.1027\n",
      "Epoch 3/20, Loss: 1.0816\n",
      "Epoch 4/20, Loss: 1.0966\n",
      "Epoch 5/20, Loss: 1.0467\n",
      "Epoch 6/20, Loss: 1.0120\n",
      "Epoch 7/20, Loss: 1.0161\n",
      "Epoch 8/20, Loss: 0.9946\n",
      "Epoch 9/20, Loss: 0.9706\n",
      "Epoch 10/20, Loss: 0.9272\n",
      "Epoch 11/20, Loss: 0.9267\n",
      "Epoch 12/20, Loss: 0.9034\n",
      "Epoch 13/20, Loss: 0.8697\n",
      "Epoch 14/20, Loss: 0.8413\n",
      "Epoch 15/20, Loss: 0.8340\n",
      "Epoch 16/20, Loss: 0.8126\n",
      "Epoch 17/20, Loss: 0.7943\n",
      "Epoch 18/20, Loss: 0.7667\n",
      "Epoch 19/20, Loss: 0.7473\n",
      "Epoch 20/20, Loss: 0.7352\n"
     ]
    }
   ],
   "source": [
    "def train(model,dataloader,optimizer,criterion,device,num_epoch=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for source , target in dataloader:\n",
    "            source , target = source.to(device) , target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(source,target)\n",
    "\n",
    "            output = output[:,1:].reshape(-1,output.shape[2])\n",
    "            target = target[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epoch}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "train(model,dataloader,optimizer,criterion,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "311f6ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.5053315359323625)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_loss = -np.log(1/len(french_vocab))\n",
    "base_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27a669ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model,sentence,english_vocab,french_vocab,max_len_fr,device):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = [english_vocab['<sos>']]+ [english_vocab.get(word,english_vocab['unk']) for word in sentence.split()] + [english_vocab['<eos>']]\n",
    "    source = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden , cell = model.encoder(source)\n",
    "\n",
    "    target_vocab = {v:k for k, v in french_vocab.items()}\n",
    "    target_indices = [french_vocab['<sos>']]\n",
    "\n",
    "    for _ in range(max_len_fr):\n",
    "        target_tensor = torch.tensor([target_indices[-1]]).to(device)\n",
    "        output , hidden , cell = model.decoder(target_tensor,hidden,cell)\n",
    "        pred = output.argmax(1).item()\n",
    "\n",
    "        target_indices.append(pred)\n",
    "        if pred == french_vocab['<eos>']:\n",
    "            break\n",
    "    \n",
    "    translate_sentence = [target_vocab[i] for i in target_indices[1:-1]]\n",
    "\n",
    "    return \" \".join(translate_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e135253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the main data sources for language models?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 语言模型的主要数据来源是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "2 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "3 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "4 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "5 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which model did they use to obtain the 82%-87% accuracy?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 他们使用哪种模型获得 82%-87% 的准确率？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "6 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "7 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Do CoNLL-2003 taggers still work?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： CoNLL-2003 标注器是否仍然有效？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web\n",
      "\n",
      "\n",
      "8 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the novelty of the proposed human evaluation method?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 提出的人工评估方法有何新颖之处？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "9 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does the success of the existing weakly supervised approach heavily rely on?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 现有弱监督方法的成功在很大程度上依赖于什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "\n",
      "\n",
      "10 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What kind of advances can be done to improve the score?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 可以采取哪些措施来提高分数？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "11 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "12 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "13 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "14 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "15 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which domains are simplified more?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些领域的简化程度更大？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？\n",
      "\n",
      "\n",
      "16 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "17 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the example of the preference for shorter left conjuncts?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 偏好较短左并列词的示例是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "18 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "19 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Can I use the models for my research?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 我可以将这些模型用于我的研究吗？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "20 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: DEplain-web contains documents from the web. Which type of content is inside DEplain-apa?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： DEplain-web 包含来自网络的文档。DEplain-apa 中包含哪种类型的内容？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "21 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which factors lead to good generalization?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些因素有助于良好的泛化？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "22 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "23 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How was the tendency for left conjuncts to be shorter measured?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何衡量左并列词是否更短？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "24 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How were the experiments designed to study the effect of the governor’s position?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何设计实验来研究支配词位置的影响？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "\n",
      "\n",
      "25 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How well does a baseline classifier work training on imbalanced data?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 基线分类器在不平衡数据上的训练效果如何？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "26 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "27 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the characters' names in the example conversation?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 示例对话中的角色名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "28 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: On which discourse phenomena do context-aware MT models improve over context-agnostic ones?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在哪些话语现象上，语境感知 MT 模型比语境无关模型更有优势？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "29 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "30 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "31 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How exactly does the introduced framework quantify the positionality?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 引入的框架如何量化立场？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "32 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "33 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "34 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What was the finding of the previous study where human subjects were given the same persona prompts?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "35 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which sources of data were used in this study?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 此研究使用了哪些数据来源？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "36 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "37 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are some closely related tasks for cognitive dissonance?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 与认知失调密切相关的任务有哪些？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 为什么我们需要在整个上下文窗口中评估模型的可接受性？\n",
      "\n",
      "\n",
      "38 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "39 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "40 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "41 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does the introduced framework differ from the previous works?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 引入的框架与以前的研究有何不同？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "42 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which of the three compared setups overlaps the most with the lexicon of stereotypes?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在三个比较设置中，哪一个与刻板词汇的重叠最多？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？\n",
      "\n",
      "\n",
      "43 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which commercial systems were compared?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 比较了哪些商业系统？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "44 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "45 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Up to how many tokens context length were MPP evaluations performed?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： MPP 评估最多涵盖了多少个词元的上下文长度？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "46 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "47 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which domains did they include in their dataset?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 他们的数据集中包含哪些领域？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web\n",
      "\n",
      "\n",
      "48 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the definition of positionality in general?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 一般来说，positionality（立场）的定义是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "49 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "50 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "51 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does EDAtt adapt an existing offline ST model?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： EDAtt 是否适应了现有的离线 ST 模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "52 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "53 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does the tested model work on the test suite?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 被测模型是否能在测试套件上运行？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "54 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which three variants of KITMUS are there?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： KITMUS 有哪三个变体？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "55 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "56 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "57 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the last research question?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 最后一个研究问题是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "58 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "59 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does the metric sensitivity work?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 指标灵敏度是如何工作的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "60 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "61 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does greater sensitivity indicate improved model performance, or does it suggest the opposite?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 更高的灵敏度是否表示模型性能得到了提高，还是表明相反？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "62 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "63 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "64 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What kind of linguistic context do models receive during pretraining?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在预训练期间，模型会接收什么样的语言上下文？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "65 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many clean validation samples are typically needed for good performance in WSL?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在 WSL 中，通常需要多少个干净的验证样本才能获得良好的表现？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "66 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "67 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "68 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Why is there a need to develop new methods for measuring media biases?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 为什么需要开发新的方法来衡量媒体偏见？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "69 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "70 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "71 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "72 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does the political bias propagation pipeline look like?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 政治偏见传播流程是怎样的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "73 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "74 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does the simplification process differ for DEplain-apa and web?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： DEplain-apa 和网站的简化过程是否有所不同？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "75 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Is Coscript publicly available?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： Coscript 是否公开可用？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "76 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How exactly is the watermark inserted into the text?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 水印是如何插入到文本中的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "77 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "78 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "79 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Can encoder-decoder models such as mt5 improve by training on a mixture of languages?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进吗？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "\n",
      "\n",
      "80 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "81 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is an example of constrained language planning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 受限语言规划的一个示例是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "82 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How do they make sure of the covertness of their method?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 他们如何确保其方法的隐蔽性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "83 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does the work use existing PLMs to build a new one?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 研究如何使用现有的 PLM 来构建新的 PLM？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "84 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which country is GPT-4 the least aligned with?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： GPT-4 与哪个国家/地区的立场最不一致？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "85 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: On which example sentence does the speaker show how the model leverages knowledge learned though the attention mechanism?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "86 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "87 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does the amount of tasks impact the model performance?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 任务的数量如何影响模型的性能？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "88 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Name three treeless baselines that the authors compare their method with.\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 请说出作者用来比较其方法的三个无树基线。\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "89 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: In what relation are the two co-authors with the first author?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 两位合著者与第一作者有什么关系？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 为什么我们需要在整个上下文窗口中评估模型的可接受性？\n",
      "\n",
      "\n",
      "90 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "91 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Who is the first author of PaLM?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： PaLM 的第一作者是谁？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "92 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many problems of SimulST does the speaker mention?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者提到了 SimulST 的几个问题？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "93 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is an effective way to mitigate social and political biases in datasets when training NLP models?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "\n",
      "\n",
      "94 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "95 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How good is the fluency of PaLM?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： PaLM 的流畅度如何？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "96 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the important properties of a watermarking method?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 水印方法的重要属性是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？\n",
      "\n",
      "\n",
      "97 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which are the 14 different languages into which the English TED talks have been translated?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： TED 英语演讲已被翻译成哪 14 种不同的语言？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "98 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many instances are sampled from one dataset for reannotating?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 从一个数据集中抽取多少个实例用于重新注释？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "99 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which distance metrics are used for measuring the difference between benign and backdoor datasets?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些距离度量用于衡量良性和后门数据集之间的差异？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "100 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "101 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How were the multilingual encoder-based models used for this task?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何将基于编码器的多语言模型用于这项任务？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "102 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "103 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "104 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How do the authors decide what moderate-frequency words are?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者如何确定中等频率的单词？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "105 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "106 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What speech segment size does the approach use?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该方法使用的语音片段大小是多少？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "107 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: In the example with Servin and Kea, what entity-specific knowledge is needed?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些特定于实体的知识？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "108 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the most important factor between the example quality and the similarity to the source sentence?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 示例质量和与源句子的相似度相比，哪个因素更为重要？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "109 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "110 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which language models does the paper focus on in the extended experiments?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在扩展实验中，论文侧重于哪些语言模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "111 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does the model use attention scores from a specific layer or combine the scores from several layers?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "112 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the examples of direct inference?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 直接推断的示例有哪些？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "113 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "114 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "115 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "116 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "117 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Was translating the natural language query using a machine translation model before semantic parsing considered as a baseline?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "118 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "119 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "120 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What example did the authors give as a marked group?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者给出的“显性群体”(marked group) 的示例是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "121 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which model architectures do not generalize well?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些模型架构泛化能力较差？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "122 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the names of the testing datasets?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 测试数据集的名称是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "123 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "124 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does the author work with multiple modalities or only text?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者是否采用了多种模态，还是仅使用文本？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "125 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "126 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "127 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "128 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What do the authors claim is an understudied area in NLU?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者认为哪些是 NLU 中研究不足的领域？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "\n",
      "\n",
      "129 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the names of the speakers?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 如何设计实验来研究支配词位置的影响？\n",
      "\n",
      "\n",
      "130 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Did Coscript undergo any quality checks?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： Coscript 是否经过了质量检查？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "131 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the limits of existing resources for on context-dependent translation?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 对于依赖上下文的翻译，现有的资源有哪些局限性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "132 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: To which existing SimulST policies is the approach compared?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该方法与哪些现有的 SimulST 策略进行了比较？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "133 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "134 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "135 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "136 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "137 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Is the dataset publicly available?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 数据集是否公开？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "138 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "139 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "140 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "141 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "142 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "143 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "144 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "145 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What type of tokens does the first step of the method map the input tokens to?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该方法的第一步将输入词元映射到什么类型的词元？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "146 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many scripts are represented in Coscript?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： Coscript 中包含了多少个脚本？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "147 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the best alignment method for DEplain?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： DEplain 的最佳对齐方法是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "148 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the benefit of weakly supervised learning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 弱监督学习有什么好处？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "149 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "150 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "151 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: The documents in DEplain-web were aligned with manual and automatic alignment methods. How was the allocation exactly?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "152 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How was the CoNLL++ dataset created?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： CoNLL++ 数据集是如何创建的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "153 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "154 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the existing works on this?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 关于这方面的现有研究有哪些？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "155 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Are multilingual LLMs such as Codex or Bloom sufficient for CLSP?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： Codex 或 Bloom 等多语言 LLM 对于 CLSP 来说是否足够？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "156 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "157 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does the method deal with the ambiguity of permutations?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该方法如何处理排列的不确定性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "\n",
      "\n",
      "158 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How is the fairness of a downstream NLP model defined?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何定义下游 NLP 模型的公平性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是什么？\n",
      "\n",
      "\n",
      "159 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "160 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "161 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "162 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "163 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "164 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does tropicalism indicate in the context of this paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在本文的背景下，热带主义 (tropicalism) 意味着什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "165 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How did the authors create the human-written portrayals of target groups?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者是如何创建目标群体的人工描写？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "166 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What was used to measure context usage in this work?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 本文中使用了什么来衡量语境使用情况？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "167 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the difference between DrBERT and ChuBERT?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： DrBERT 和 ChuBERT 有什么区别？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "168 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "169 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is iterative transfer learning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 什么是迭代迁移学习？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "170 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the goal of the dataset?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 数据集的目标是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "171 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How can an attacker extract model parameters through an EaaS?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 攻击者如何通过 EaaS 来提取模型参数？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "172 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "173 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "174 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many annotators were used to create the initial dataset?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 有多少个注释者用于创建初始数据集？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "175 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "176 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "177 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the example that the governor is on the left?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 以左侧为支配词的示例是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "\n",
      "\n",
      "178 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the state-of-the-art models in dialogue systems?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 对话系统中的最先进模型是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "\n",
      "\n",
      "179 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Why do we need to evaluate the models' acceptability throughout the context window?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 为什么我们需要在整个上下文窗口中评估模型的可接受性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "180 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Did training in multilingual fashion cause performance drop compared to monolingual English model?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 与单语英语模型相比，多语言训练是否会导致表现下降？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "181 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Do the annotators know about the entity in advance?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 注释者是否提前知道该实体？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "182 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which MT metrics were used for the evaluation?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 评估使用了哪些 MT（机器翻译）指标？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "183 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does the regress in generalization impact specific NER types?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 泛化中的回归是否会影响特定的 NER 类型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "184 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Why does positionality in NLP matter?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 为什么 NLP 中的立场很重要？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 和 CoNLL++ 之间的性能增量才高于 5 个百分点？\n",
      "\n",
      "\n",
      "185 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Were the multilingual LLMs like BLOOM fine-tuned with adapters or full fine-tuning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 像 BLOOM 这样的多语言 LLM 是采用适配器微调还是完整微调？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "186 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "187 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which model do they use for transfer learning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 他们使用哪种模型进行迁移学习？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "188 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which are the recent test sets used to assess the PaLM capabilities?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些是最近用于评估 PaLM 能力的测试集？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "189 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many recommendations did the authors propose at last?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者最终提出了多少条建议？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "190 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How much is the gain of the proposed method over the strongest baseline?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 与最强的基线相比，提议的方法获得了多少收益？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "191 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "192 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Can the results and dataset in the paper be used as a benchmark?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 论文中的结果和数据集可以用作基准吗？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "193 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many smaller models do they experiment with in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 他们在论文中进行了多少个较小模型的实验？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "194 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which model is used as the base model for investigating multi-model instruction tuning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪个模型被用作研究多模型指令调整的基础模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "195 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "196 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "197 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "198 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "199 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "200 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which language pairs were analyzed in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 论文分析了哪些语言对？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 该模型是使用特定层的注意力分数，还是结合多个层的分数？\n",
      "\n",
      "\n",
      "201 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "202 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "203 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which models were investigated during the experiments?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在实验过程中研究了哪些模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "204 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: From the 62 diverse tasks used in MultiInstruct, how many tasks are used for training and testing purposes?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "205 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "206 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "207 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which datasets did the authors experiment on?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者在实验中使用了哪些数据集？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "208 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "209 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is NACHOS?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 什么是 NACHOS？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "210 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "211 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "212 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How much does the prompting strategy impact the results?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 提示策略对结果有多大影响？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "\n",
      "\n",
      "213 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "214 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the 5 expert-written instructions?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 5 个由专家编写的指令是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "215 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What do the authors propose to test the models on using information from multiple sources?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者建议如何使用来自多种来源的信息来测试模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "216 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "217 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "218 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are common evaluation methods for dialogue systems?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 对话系统的常用评估方法是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "\n",
      "\n",
      "219 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "220 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: In the example with Servin and Kea, what background knowledge is needed?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在 Servin 和 Kea 的示例中，需要哪些背景知识？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "221 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "222 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Is the code available, and if yes, where?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 代码是否公开？如果公开，可在哪里获取？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "223 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "224 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Are the annotators for NLPositionality balanced in regard to each demographic, i.e., country, gender, etc.?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： NLPositionality 的注释者在各个人口统计学特征（即国家/地区、性别等）方面是否均衡？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "225 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How were sentences perturbed in the acceptable domain?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何在可接受的域中扰乱句子？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "226 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does it mean to have a dimensional evaluation?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 进行维度评估意味着什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "227 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "228 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "229 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "230 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "231 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: In which cases, if any, is the form of the prompting important?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在哪些情况下（如有），提示的形式很重要？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "\n",
      "\n",
      "232 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which dialog models did the authors evaluate?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者评估了哪些对话模型？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "233 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "234 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "235 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "236 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the ideal qualities of a good planner?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 优秀规划器的理想品质是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "237 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "238 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "239 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "240 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "241 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "242 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which are the most common errors of PaLM?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： PaLM 最常见的错误是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "243 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "244 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does CFT stand for in this paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在本文中，CFT 代表什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "245 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "246 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "247 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "248 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: If the new method has a name, state that name, otherwise state that it does not have a name.\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如果新方法有名称，请注明该名称，如果没有，请注明它没有名称。\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "249 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What was the author's description of the \"marked words\" method?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者如何描述“显性词汇”(marked words) 方法？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "250 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "251 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "252 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "253 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "254 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the first mentioned symmetrical dependency structure? Answer the one including the city name\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "\n",
      "\n",
      "255 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "256 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "257 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "258 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "259 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What datasets can be used to test syntactic phenomena?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些数据集可用于测试句法现象？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "260 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the abbreviations of the five methods for the first research question?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 第一个研究问题的五种方法的缩写是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "261 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: On which tasks is the model evaluated?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该模型在哪些任务上进行了评估？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "262 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: On which data is CamemBERT initially trained?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： CamemBERT 最初是在哪些数据上训练的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "263 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "264 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "265 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "266 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which findings led to the conclusion that the temporal drift is the main cause of performance loss?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "267 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "268 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "269 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Why is it necessary to permute the tokens for the output sequence?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 为什么有必要对输出序列中的词元进行排列？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "\n",
      "\n",
      "270 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Why did the authors recommend that model owners should increase transparency about bias mitigation methods?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 为什么作者建议模型所有者应提高偏见缓解方法的透明度？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在语义解析之前，是否使用机器翻译模型翻译自然语言查询作为基线？\n",
      "\n",
      "\n",
      "271 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are minimal-pair unacceptable inputs?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 什么是最小对不可接受输入？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "272 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "273 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "274 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What evaluation metrics did the authors use?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者使用了哪些评估指标？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "275 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "276 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which metric was used for measuring inter-annotator agreement?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 使用了哪个指标来衡量注释者之间的一致性？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "277 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which domain was chosen to add completely unrelated sentences to the unacceptable and acceptable queries?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "278 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "279 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How does MultiInstruct differ from other benchmarks?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： MultiInstruct 与其他基准有何不同？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "280 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "281 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the definition of binary coordination?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 二进制协调的定义是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "282 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How long, on average, were the prompts used in this study?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在本研究中，提示语的平均长度是多少？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？\n",
      "\n",
      "\n",
      "283 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the implications of the findings on the smaller T5 model?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这些发现对较小的 T5 模型有什么影响？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "284 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "285 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which learning strategies are investigated in the work?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 论文研究了哪些学习策略？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "286 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How big is the factor of overfitting due to test reuse specifically?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 由于测试重复使用而导致的过拟合因素有多大？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "287 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How was the quality of the simplification evaluated?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 如何评估简化质量？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在不可接受和可接受查询中，选择哪个领域来添加完全无关的句子？\n",
      "\n",
      "\n",
      "288 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "289 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "290 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Do language models have different political biases?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 语言模型是否有不同的政治偏见？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "291 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is cognitive dissonance?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 什么是认知失调？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "292 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "293 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which language model is the most liberal?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 哪种语言模型最倾向于自由派？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是什么？\n",
      "\n",
      "\n",
      "294 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "295 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Does cumulative training perform better than iterative when doing active learning?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 在主动学习时，累积训练是否比迭代训练更有效？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "296 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "297 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Where was the data taken from in the MuDa benchmark?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： MuDa 基准中的数据是从哪里获得的？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "298 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "299 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "300 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is cross-lingual transfer?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 什么是跨语言转移？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 第一个提到的对称依存关系结构的名称是什么？请回答包含城市名称的结构\n",
      "\n",
      "\n",
      "301 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "302 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "303 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "304 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "305 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Which latency measures do the authors use?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 作者使用了哪些延迟测量方法？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "306 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "307 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the drawbacks of tree-based methods?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 基于树的方法有哪些缺点？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 哪些发现导致了时间漂移是性能下降的主要原因的结论？\n",
      "\n",
      "\n",
      "308 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "309 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "310 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "311 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "312 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "313 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What does ABC-Eval stand for?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： ABC-Eval 代表什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？\n",
      "\n",
      "\n",
      "314 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n",
      "315 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: Until which year is the performance delta between CoNLL-2003 and CoNLL++ higher than 5 percentage points?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 直到哪一年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于 5 个百分点？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "316 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What are the affiliations of the authors of the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文的作者所属机构是什么？\n",
      "\n",
      "\n",
      "317 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: What is the name of the speaker?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 演讲者的名字是什么？\n",
      "\n",
      "\n",
      "318 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: How many authors are involved in the paper?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 这篇论文有多少位作者？\n",
      "\n",
      "\n",
      "319 ####################################################################################################\n",
      "Original: Answer the following question concisely given the English content: To which dedicated simulST architecture is the approach compared?\n",
      "Ground Truth: 根据所给的英文内容，简要回答以下问题： 该方法与哪种专用的 simulST 架构进行了比较？\n",
      "Prediction: 根据所给的英文内容，简要回答以下问题： 在 MultiInstruct 中使用的 62 个不同任务中，有多少任务用于训练和测试目的？\n",
      "\n",
      "\n",
      "320 ####################################################################################################\n",
      "Original: Summarize the English content in an abstract of approximately 200 words.\n",
      "Ground Truth: 用400个字左右概括所给的英语内容。\n",
      "Prediction: <sos> MPP 和网站的简化过程是否有所不同？\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (eng, fr) in enumerate(zip(english, french), 1):\n",
    "    translation = translate_sentence(model,eng,english_vocab,french_vocab,500,device)\n",
    "    print(i,\"#\"*100)\n",
    "    print('Original:',eng)\n",
    "    print('Ground Truth:',fr)\n",
    "    print('Prediction:',translation)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fde93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

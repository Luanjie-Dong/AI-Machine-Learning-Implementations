{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3597065f",
   "metadata": {},
   "source": [
    "## Objective: Implement activation function from scratch and apply differentiation\n",
    "- reference: https://www.v7labs.com/blog/neural-networks-activation-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeff5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.array([\n",
    "    [1,1,0],\n",
    "    [10,2,-2],\n",
    "    [-100,23,12]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c73a8",
   "metadata": {},
   "source": [
    "### Activation functions:\n",
    "    - ReLU\n",
    "    - Sigmoid\n",
    "    - tanh\n",
    "    - Leaky ReLU\n",
    "    - ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466574fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "#relu derivative : 1 when x >= 0 and 0 when x < 0\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0 , 1 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f82d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid derivative: s(x) * (1 - s(x))\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid_activation(x)\n",
    "    return s * (1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6793593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdc115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_activation(x,alpha=0.01):\n",
    "    return np.where(x > 0 ,x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x,alpha=0.01):\n",
    "    return np.where(x > 0 , 1 , alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002bb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_activation(x,alpha=1.0):\n",
    "    return np.where(x > 0 , x , alpha * (np.exp(x)-1))\n",
    "\n",
    "\n",
    "def elu_derivative(x,alpha=1.0):\n",
    "    return np.where(x > 0 , 1 , elu_activation(x,alpha) + alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec16763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU:\n",
      "Activation: [0 0 0 1 2]\n",
      "Derivative: [0 0 0 1 1]\n",
      "\n",
      "Sigmoid:\n",
      "Activation: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "\n",
      "tanh:\n",
      "Activation: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "Derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n",
      "\n",
      "Leaky ReLU (alpha=0.1):\n",
      "Activation: [-0.2 -0.1  0.   1.   2. ]\n",
      "Derivative: [0.1 0.1 0.1 1.  1. ]\n",
      "\n",
      "ELU (alpha=1.0):\n",
      "Activation: [-0.86466472 -0.63212056  0.          1.          2.        ]\n",
      "Derivative: [0.13533528 0.36787944 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"ReLU:\")\n",
    "print(f\"Activation: {relu_activation(x)}\")\n",
    "print(f\"Derivative: {relu_derivative(x)}\\n\")\n",
    "\n",
    "print(\"Sigmoid:\")\n",
    "print(f\"Activation: {sigmoid_activation(x)}\")\n",
    "print(f\"Derivative: {sigmoid_derivative(x)}\\n\")\n",
    "\n",
    "print(\"tanh:\")\n",
    "print(f\"Activation: {tanh_activation(x)}\")\n",
    "print(f\"Derivative: {tanh_derivative(x)}\\n\")\n",
    "\n",
    "print(\"Leaky ReLU (alpha=0.1):\")\n",
    "print(f\"Activation: {leaky_relu_activation(x, alpha=0.1)}\")\n",
    "print(f\"Derivative: {leaky_relu_derivative(x, alpha=0.1)}\\n\")\n",
    "\n",
    "print(\"ELU (alpha=1.0):\")\n",
    "print(f\"Activation: {elu_activation(x, alpha=1.0)}\")\n",
    "print(f\"Derivative: {elu_derivative(x, alpha=1.0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
